{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Softmax regression: No-hidden layer perceptron (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1-hidden layer percentron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w1 = tf.get_variable('w1', [784, 256], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.1))\n",
    "b1 = tf.get_variable('b1', [256], initializer=tf.zeros_initializer)\n",
    "w2 = tf.get_variable('w2', [256, 10], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.1))\n",
    "b2 = tf.get_variable('b2', [10], initializer=tf.zeros_initializer)\n",
    "\n",
    "h1 = tf.nn.sigmoid(tf.matmul(X, w1) + b1)\n",
    "logits = tf.matmul(h1, w2) + b2\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(epoch_n=20, batch_size=100, n_iter=None, use_training_ph=False):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    N = mnist.train.num_examples\n",
    "    if n_iter is None:\n",
    "        n_iter = N // batch_size\n",
    "    dq = collections.deque(maxlen=5)\n",
    "        \n",
    "    for epoch in range(epoch_n):\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "        for _ in range(n_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X: batch_x, y: batch_y}\n",
    "            if use_training_ph:\n",
    "                feed_dict[training] = True\n",
    "            _, cur_acc, cur_loss = sess.run([train_op, accuracy, loss], feed_dict=feed_dict)\n",
    "            avg_acc += cur_acc\n",
    "            avg_loss += cur_loss\n",
    "\n",
    "        avg_acc /= n_iter\n",
    "        avg_loss /= n_iter\n",
    "\n",
    "        test_acc = 0.\n",
    "        test_loss = 0.\n",
    "        for _ in range(mnist.test.num_examples // 1000):\n",
    "            batch_x, batch_y = mnist.test.next_batch(1000)\n",
    "            feed_dict = {X: batch_x, y: batch_y}\n",
    "            if use_training_ph:\n",
    "                feed_dict[training] = False\n",
    "            cur_acc, cur_loss = sess.run([accuracy, loss], feed_dict=feed_dict)\n",
    "            test_acc += cur_acc\n",
    "            test_loss += cur_loss\n",
    "        test_acc /= (mnist.test.num_examples // 1000)\n",
    "        test_loss /= (mnist.test.num_examples // 1000)\n",
    "        \n",
    "        print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f}\".\n",
    "              format(epoch+1, epoch_n, avg_acc, avg_loss, test_acc, test_loss))\n",
    "        dq.append(test_acc)\n",
    "    \n",
    "    score = np.average(dq)\n",
    "    print(\"average of last 5 test acc: {:.2%}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/20] (train) acc: 80.62%, loss: 0.783 | (test) acc: 88.80%, loss: 0.426\n",
      "[ 2/20] (train) acc: 88.88%, loss: 0.400 | (test) acc: 90.40%, loss: 0.348\n",
      "[ 3/20] (train) acc: 90.13%, loss: 0.346 | (test) acc: 91.12%, loss: 0.312\n",
      "[ 4/20] (train) acc: 90.76%, loss: 0.320 | (test) acc: 91.48%, loss: 0.294\n",
      "[ 5/20] (train) acc: 91.30%, loss: 0.303 | (test) acc: 91.91%, loss: 0.284\n",
      "[ 6/20] (train) acc: 91.66%, loss: 0.290 | (test) acc: 92.23%, loss: 0.272\n",
      "[ 7/20] (train) acc: 91.98%, loss: 0.278 | (test) acc: 92.45%, loss: 0.262\n",
      "[ 8/20] (train) acc: 92.30%, loss: 0.267 | (test) acc: 92.54%, loss: 0.256\n",
      "[ 9/20] (train) acc: 92.70%, loss: 0.257 | (test) acc: 93.06%, loss: 0.247\n",
      "[10/20] (train) acc: 92.93%, loss: 0.248 | (test) acc: 92.84%, loss: 0.241\n",
      "[11/20] (train) acc: 93.22%, loss: 0.240 | (test) acc: 93.34%, loss: 0.231\n",
      "[12/20] (train) acc: 93.47%, loss: 0.231 | (test) acc: 93.46%, loss: 0.230\n",
      "[13/20] (train) acc: 93.64%, loss: 0.224 | (test) acc: 93.83%, loss: 0.217\n",
      "[14/20] (train) acc: 93.92%, loss: 0.216 | (test) acc: 93.89%, loss: 0.211\n",
      "[15/20] (train) acc: 94.09%, loss: 0.209 | (test) acc: 94.20%, loss: 0.204\n",
      "[16/20] (train) acc: 94.30%, loss: 0.202 | (test) acc: 94.37%, loss: 0.198\n",
      "[17/20] (train) acc: 94.50%, loss: 0.195 | (test) acc: 94.39%, loss: 0.192\n",
      "[18/20] (train) acc: 94.66%, loss: 0.189 | (test) acc: 94.74%, loss: 0.187\n",
      "[19/20] (train) acc: 94.87%, loss: 0.183 | (test) acc: 94.64%, loss: 0.181\n",
      "[20/20] (train) acc: 94.98%, loss: 0.177 | (test) acc: 94.61%, loss: 0.179\n",
      "average of last 5 test acc: 94.55%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## tf.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "h1 = slim.fully_connected(X, 256, activation_fn=tf.nn.sigmoid, \n",
    "                          weights_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.1), \n",
    "                          biases_initializer=tf.zeros_initializer())\n",
    "logits = slim.fully_connected(h1, 10, activation_fn=None, \n",
    "                              weights_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.1), \n",
    "                              biases_initializer=tf.zeros_initializer())\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/20] (train) acc: 80.32%, loss: 0.787 | (test) acc: 88.90%, loss: 0.432\n",
      "[ 2/20] (train) acc: 88.85%, loss: 0.401 | (test) acc: 90.55%, loss: 0.345\n",
      "[ 3/20] (train) acc: 90.11%, loss: 0.347 | (test) acc: 90.92%, loss: 0.316\n",
      "[ 4/20] (train) acc: 90.80%, loss: 0.320 | (test) acc: 91.68%, loss: 0.293\n",
      "[ 5/20] (train) acc: 91.32%, loss: 0.302 | (test) acc: 91.79%, loss: 0.281\n",
      "[ 6/20] (train) acc: 91.69%, loss: 0.288 | (test) acc: 92.05%, loss: 0.273\n",
      "[ 7/20] (train) acc: 92.08%, loss: 0.276 | (test) acc: 92.54%, loss: 0.260\n",
      "[ 8/20] (train) acc: 92.40%, loss: 0.265 | (test) acc: 92.73%, loss: 0.254\n",
      "[ 9/20] (train) acc: 92.69%, loss: 0.255 | (test) acc: 93.07%, loss: 0.242\n",
      "[10/20] (train) acc: 92.94%, loss: 0.246 | (test) acc: 93.12%, loss: 0.237\n",
      "[11/20] (train) acc: 93.28%, loss: 0.237 | (test) acc: 93.41%, loss: 0.229\n",
      "[12/20] (train) acc: 93.52%, loss: 0.228 | (test) acc: 93.64%, loss: 0.222\n",
      "[13/20] (train) acc: 93.75%, loss: 0.220 | (test) acc: 93.88%, loss: 0.214\n",
      "[14/20] (train) acc: 94.01%, loss: 0.212 | (test) acc: 94.03%, loss: 0.211\n",
      "[15/20] (train) acc: 94.19%, loss: 0.205 | (test) acc: 94.17%, loss: 0.201\n",
      "[16/20] (train) acc: 94.46%, loss: 0.198 | (test) acc: 94.28%, loss: 0.195\n",
      "[17/20] (train) acc: 94.49%, loss: 0.192 | (test) acc: 94.50%, loss: 0.190\n",
      "[18/20] (train) acc: 94.78%, loss: 0.185 | (test) acc: 94.71%, loss: 0.185\n",
      "[19/20] (train) acc: 94.96%, loss: 0.179 | (test) acc: 94.73%, loss: 0.179\n",
      "[20/20] (train) acc: 95.08%, loss: 0.174 | (test) acc: 94.96%, loss: 0.174\n",
      "average of last 5 test acc: 94.64%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Better MLP - ReLU, xavier_init, adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "h1 = slim.fully_connected(X, 256, activation_fn=tf.nn.relu, weights_initializer=slim.xavier_initializer())\n",
    "logits = slim.fully_connected(h1, 10, activation_fn=None, weights_initializer=slim.xavier_initializer())\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/20] (train) acc: 91.67%, loss: 0.297 | (test) acc: 95.60%, loss: 0.150\n",
      "[ 2/20] (train) acc: 96.31%, loss: 0.129 | (test) acc: 96.57%, loss: 0.109\n",
      "[ 3/20] (train) acc: 97.41%, loss: 0.088 | (test) acc: 97.38%, loss: 0.085\n",
      "[ 4/20] (train) acc: 98.14%, loss: 0.063 | (test) acc: 97.38%, loss: 0.081\n",
      "[ 5/20] (train) acc: 98.55%, loss: 0.049 | (test) acc: 97.45%, loss: 0.079\n",
      "[ 6/20] (train) acc: 98.91%, loss: 0.038 | (test) acc: 97.71%, loss: 0.070\n",
      "[ 7/20] (train) acc: 99.20%, loss: 0.029 | (test) acc: 97.64%, loss: 0.073\n",
      "[ 8/20] (train) acc: 99.36%, loss: 0.023 | (test) acc: 97.86%, loss: 0.065\n",
      "[ 9/20] (train) acc: 99.54%, loss: 0.018 | (test) acc: 97.86%, loss: 0.068\n",
      "[10/20] (train) acc: 99.64%, loss: 0.015 | (test) acc: 97.91%, loss: 0.069\n",
      "[11/20] (train) acc: 99.75%, loss: 0.011 | (test) acc: 98.04%, loss: 0.063\n",
      "[12/20] (train) acc: 99.82%, loss: 0.009 | (test) acc: 98.02%, loss: 0.071\n",
      "[13/20] (train) acc: 99.77%, loss: 0.009 | (test) acc: 98.06%, loss: 0.070\n",
      "[14/20] (train) acc: 99.87%, loss: 0.007 | (test) acc: 98.00%, loss: 0.072\n",
      "[15/20] (train) acc: 99.88%, loss: 0.006 | (test) acc: 97.83%, loss: 0.075\n",
      "[16/20] (train) acc: 99.93%, loss: 0.004 | (test) acc: 97.78%, loss: 0.081\n",
      "[17/20] (train) acc: 99.86%, loss: 0.005 | (test) acc: 97.91%, loss: 0.080\n",
      "[18/20] (train) acc: 99.72%, loss: 0.009 | (test) acc: 97.92%, loss: 0.080\n",
      "[19/20] (train) acc: 99.96%, loss: 0.002 | (test) acc: 98.16%, loss: 0.075\n",
      "[20/20] (train) acc: 100.00%, loss: 0.001 | (test) acc: 98.14%, loss: 0.070\n",
      "average of last 5 test acc: 97.98%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Going Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "h1 = slim.fully_connected(X, 256, activation_fn=tf.nn.relu, weights_initializer=slim.xavier_initializer())\n",
    "h2 = slim.fully_connected(h1, 128, activation_fn=tf.nn.relu, weights_initializer=slim.xavier_initializer())\n",
    "logits = slim.fully_connected(h2, 10, activation_fn=None, weights_initializer=slim.xavier_initializer())\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/20] (train) acc: 92.19%, loss: 0.268 | (test) acc: 96.13%, loss: 0.124\n",
      "[ 2/20] (train) acc: 96.88%, loss: 0.103 | (test) acc: 96.89%, loss: 0.099\n",
      "[ 3/20] (train) acc: 97.93%, loss: 0.067 | (test) acc: 97.53%, loss: 0.076\n",
      "[ 4/20] (train) acc: 98.44%, loss: 0.049 | (test) acc: 97.91%, loss: 0.068\n",
      "[ 5/20] (train) acc: 98.92%, loss: 0.034 | (test) acc: 97.87%, loss: 0.070\n",
      "[ 6/20] (train) acc: 99.16%, loss: 0.027 | (test) acc: 97.83%, loss: 0.072\n",
      "[ 7/20] (train) acc: 99.29%, loss: 0.022 | (test) acc: 97.73%, loss: 0.074\n",
      "[ 8/20] (train) acc: 99.39%, loss: 0.018 | (test) acc: 97.91%, loss: 0.076\n",
      "[ 9/20] (train) acc: 99.44%, loss: 0.016 | (test) acc: 98.17%, loss: 0.072\n",
      "[10/20] (train) acc: 99.51%, loss: 0.015 | (test) acc: 98.03%, loss: 0.078\n",
      "[11/20] (train) acc: 99.62%, loss: 0.012 | (test) acc: 98.05%, loss: 0.076\n",
      "[12/20] (train) acc: 99.57%, loss: 0.013 | (test) acc: 98.19%, loss: 0.072\n",
      "[13/20] (train) acc: 99.62%, loss: 0.011 | (test) acc: 98.11%, loss: 0.076\n",
      "[14/20] (train) acc: 99.77%, loss: 0.007 | (test) acc: 98.02%, loss: 0.087\n",
      "[15/20] (train) acc: 99.67%, loss: 0.011 | (test) acc: 98.12%, loss: 0.089\n",
      "[16/20] (train) acc: 99.68%, loss: 0.010 | (test) acc: 98.19%, loss: 0.083\n",
      "[17/20] (train) acc: 99.76%, loss: 0.007 | (test) acc: 98.24%, loss: 0.086\n",
      "[18/20] (train) acc: 99.76%, loss: 0.008 | (test) acc: 98.14%, loss: 0.094\n",
      "[19/20] (train) acc: 99.68%, loss: 0.010 | (test) acc: 98.05%, loss: 0.099\n",
      "[20/20] (train) acc: 99.75%, loss: 0.008 | (test) acc: 98.34%, loss: 0.079\n",
      "average of last 5 test acc: 98.19%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* No better acc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "h1 = slim.conv2d(x_img, 32, kernel_size=[3,3]) # default relu, xavier\n",
    "p1 = slim.max_pool2d(h1, kernel_size=[2,2])\n",
    "assert p1.shape[1:] == [14, 14, 32]\n",
    "\n",
    "h2 = slim.conv2d(p1, 64, kernel_size=[3,3])\n",
    "p2 = slim.max_pool2d(h2, kernel_size=[2,2])\n",
    "assert p2.shape[1:] == [7, 7, 64]\n",
    "\n",
    "flat = slim.flatten(p2)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/20] (train) acc: 92.70%, loss: 0.250 | (test) acc: 97.81%, loss: 0.071\n",
      "[ 2/20] (train) acc: 97.97%, loss: 0.066 | (test) acc: 98.68%, loss: 0.044\n",
      "[ 3/20] (train) acc: 98.48%, loss: 0.047 | (test) acc: 98.84%, loss: 0.034\n",
      "[ 4/20] (train) acc: 98.82%, loss: 0.038 | (test) acc: 98.85%, loss: 0.034\n",
      "[ 5/20] (train) acc: 98.95%, loss: 0.032 | (test) acc: 98.79%, loss: 0.037\n",
      "[ 6/20] (train) acc: 99.13%, loss: 0.027 | (test) acc: 98.87%, loss: 0.033\n",
      "[ 7/20] (train) acc: 99.26%, loss: 0.022 | (test) acc: 98.86%, loss: 0.033\n",
      "[ 8/20] (train) acc: 99.36%, loss: 0.020 | (test) acc: 98.87%, loss: 0.036\n",
      "[ 9/20] (train) acc: 99.46%, loss: 0.017 | (test) acc: 98.88%, loss: 0.035\n",
      "[10/20] (train) acc: 99.55%, loss: 0.014 | (test) acc: 98.56%, loss: 0.044\n",
      "[11/20] (train) acc: 99.59%, loss: 0.013 | (test) acc: 98.99%, loss: 0.032\n",
      "[12/20] (train) acc: 99.67%, loss: 0.011 | (test) acc: 98.85%, loss: 0.038\n",
      "[13/20] (train) acc: 99.75%, loss: 0.008 | (test) acc: 99.02%, loss: 0.032\n",
      "[14/20] (train) acc: 99.74%, loss: 0.008 | (test) acc: 98.86%, loss: 0.037\n",
      "[15/20] (train) acc: 99.82%, loss: 0.006 | (test) acc: 98.80%, loss: 0.046\n",
      "[16/20] (train) acc: 99.83%, loss: 0.006 | (test) acc: 98.77%, loss: 0.043\n",
      "[17/20] (train) acc: 99.79%, loss: 0.006 | (test) acc: 99.05%, loss: 0.040\n",
      "[18/20] (train) acc: 99.85%, loss: 0.004 | (test) acc: 98.98%, loss: 0.041\n",
      "[19/20] (train) acc: 99.87%, loss: 0.004 | (test) acc: 98.95%, loss: 0.043\n",
      "[20/20] (train) acc: 99.90%, loss: 0.003 | (test) acc: 98.98%, loss: 0.045\n",
      "average of last 5 test acc: 98.95%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deeper ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "h1 = slim.conv2d(x_img, 32, kernel_size=[3,3]) # default relu, xavier\n",
    "p1 = slim.max_pool2d(h1, kernel_size=[2,2])\n",
    "assert p1.shape[1:] == [14, 14, 32]\n",
    "\n",
    "h2 = slim.conv2d(p1, 64, kernel_size=[3,3])\n",
    "p2 = slim.max_pool2d(h2, kernel_size=[2,2])\n",
    "assert p2.shape[1:] == [7, 7, 64]\n",
    "\n",
    "h3 = slim.conv2d(p2, 128, kernel_size=[3,3])\n",
    "p3 = slim.max_pool2d(h3, kernel_size=[2,2], padding='same')\n",
    "assert p3.shape[1:] == [4, 4, 128]\n",
    "\n",
    "flat = slim.flatten(p3)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 93.73%, loss: 0.204 | (test) acc: 98.47%, loss: 0.045\n",
      "[ 2/100] (train) acc: 98.44%, loss: 0.051 | (test) acc: 98.67%, loss: 0.040\n",
      "[ 3/100] (train) acc: 98.86%, loss: 0.036 | (test) acc: 98.83%, loss: 0.035\n",
      "[ 4/100] (train) acc: 99.19%, loss: 0.027 | (test) acc: 98.59%, loss: 0.043\n",
      "[ 5/100] (train) acc: 99.32%, loss: 0.022 | (test) acc: 99.09%, loss: 0.028\n",
      "[ 6/100] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.18%, loss: 0.025\n",
      "[ 7/100] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.01%, loss: 0.028\n",
      "[ 8/100] (train) acc: 99.63%, loss: 0.011 | (test) acc: 99.19%, loss: 0.028\n",
      "[ 9/100] (train) acc: 99.67%, loss: 0.011 | (test) acc: 99.20%, loss: 0.027\n",
      "[10/100] (train) acc: 99.68%, loss: 0.009 | (test) acc: 99.00%, loss: 0.035\n",
      "[11/100] (train) acc: 99.68%, loss: 0.010 | (test) acc: 98.98%, loss: 0.036\n",
      "[12/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.27%, loss: 0.027\n",
      "[13/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.24%, loss: 0.033\n",
      "[14/100] (train) acc: 99.82%, loss: 0.005 | (test) acc: 99.18%, loss: 0.035\n",
      "[15/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.19%, loss: 0.038\n",
      "[16/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.10%, loss: 0.037\n",
      "[17/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.23%, loss: 0.035\n",
      "[18/100] (train) acc: 99.85%, loss: 0.005 | (test) acc: 99.18%, loss: 0.036\n",
      "[19/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.22%, loss: 0.032\n",
      "[20/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.04%, loss: 0.039\n",
      "[21/100] (train) acc: 99.88%, loss: 0.003 | (test) acc: 99.22%, loss: 0.041\n",
      "[22/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.21%, loss: 0.039\n",
      "[23/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.24%, loss: 0.036\n",
      "[24/100] (train) acc: 99.81%, loss: 0.005 | (test) acc: 99.26%, loss: 0.041\n",
      "[25/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.20%, loss: 0.037\n",
      "[26/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.32%, loss: 0.034\n",
      "[27/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.09%, loss: 0.046\n",
      "[28/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 98.95%, loss: 0.050\n",
      "[29/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.16%, loss: 0.044\n",
      "[30/100] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.26%, loss: 0.039\n",
      "[31/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.24%, loss: 0.039\n",
      "[32/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.25%, loss: 0.040\n",
      "[33/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.24%, loss: 0.039\n",
      "[34/100] (train) acc: 99.91%, loss: 0.002 | (test) acc: 99.22%, loss: 0.046\n",
      "[35/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.23%, loss: 0.043\n",
      "[36/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.42%, loss: 0.041\n",
      "[37/100] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.34%, loss: 0.037\n",
      "[38/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.20%, loss: 0.050\n",
      "[39/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.13%, loss: 0.055\n",
      "[40/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.11%, loss: 0.054\n",
      "[41/100] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.39%, loss: 0.039\n",
      "[42/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.11%, loss: 0.059\n",
      "[43/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.26%, loss: 0.046\n",
      "[44/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.34%, loss: 0.040\n",
      "[45/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.32%, loss: 0.045\n",
      "[46/100] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.29%, loss: 0.055\n",
      "[47/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.18%, loss: 0.051\n",
      "[48/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.07%, loss: 0.064\n",
      "[49/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.26%, loss: 0.048\n",
      "[50/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.38%, loss: 0.042\n",
      "[51/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.042\n",
      "[52/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.042\n",
      "[53/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.043\n",
      "[54/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.043\n",
      "[55/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.043\n",
      "[56/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.044\n",
      "[57/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.044\n",
      "[58/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.045\n",
      "[59/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.046\n",
      "[60/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.046\n",
      "[61/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.047\n",
      "[62/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.047\n",
      "[63/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.048\n",
      "[64/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.049\n",
      "[65/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.049\n",
      "[66/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.050\n",
      "[67/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.051\n",
      "[68/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.051\n",
      "[69/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.052\n",
      "[70/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.053\n",
      "[71/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.054\n",
      "[72/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.055\n",
      "[73/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.055\n",
      "[74/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.056\n",
      "[75/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.41%, loss: 0.057\n",
      "[76/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.41%, loss: 0.058\n",
      "[77/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.059\n",
      "[78/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.060\n",
      "[79/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.061\n",
      "[80/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.061\n",
      "[81/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.062\n",
      "[82/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.063\n",
      "[83/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.064\n",
      "[84/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.064\n",
      "[85/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.065\n",
      "[86/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.065\n",
      "[87/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.066\n",
      "[88/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.066\n",
      "[89/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.066\n",
      "[90/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.066\n",
      "[91/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.067\n",
      "[92/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.067\n",
      "[93/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.067\n",
      "[94/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.37%, loss: 0.067\n",
      "[95/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.068\n",
      "[96/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.37%, loss: 0.069\n",
      "[97/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.070\n",
      "[98/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.071\n",
      "[99/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.072\n",
      "[100/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.073\n",
      "average of last 5 test acc: 99.39%\n"
     ]
    }
   ],
   "source": [
    "train(epoch_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## More Deeper ... (VGGNet-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "h1 = slim.conv2d(x_img, 32, kernel_size=[3,3]) # default relu, xavier\n",
    "h1 = slim.conv2d(h1, 32, kernel_size=[3,3])\n",
    "p1 = slim.max_pool2d(h1, kernel_size=[2,2])\n",
    "assert p1.shape[1:] == [14, 14, 32]\n",
    "\n",
    "h2 = slim.conv2d(p1, 64, kernel_size=[3,3])\n",
    "h2 = slim.conv2d(h2, 64, kernel_size=[3,3])\n",
    "p2 = slim.max_pool2d(h2, kernel_size=[2,2])\n",
    "assert p2.shape[1:] == [7, 7, 64]\n",
    "\n",
    "h3 = slim.conv2d(p2, 128, kernel_size=[3,3])\n",
    "h3 = slim.conv2d(h3, 128, kernel_size=[3,3])\n",
    "p3 = slim.max_pool2d(h3, kernel_size=[2,2], padding='same')\n",
    "assert p3.shape[1:] == [4, 4, 128]\n",
    "\n",
    "flat = slim.flatten(p3)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 94.92%, loss: 0.158 | (test) acc: 98.71%, loss: 0.040\n",
      "[ 2/100] (train) acc: 98.70%, loss: 0.041 | (test) acc: 99.19%, loss: 0.024\n",
      "[ 3/100] (train) acc: 99.10%, loss: 0.029 | (test) acc: 99.27%, loss: 0.021\n",
      "[ 4/100] (train) acc: 99.34%, loss: 0.022 | (test) acc: 98.99%, loss: 0.029\n",
      "[ 5/100] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.35%, loss: 0.022\n",
      "[ 6/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 98.93%, loss: 0.032\n",
      "[ 7/100] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.35%, loss: 0.022\n",
      "[ 8/100] (train) acc: 99.61%, loss: 0.012 | (test) acc: 99.35%, loss: 0.024\n",
      "[ 9/100] (train) acc: 99.68%, loss: 0.010 | (test) acc: 99.25%, loss: 0.029\n",
      "[10/100] (train) acc: 99.70%, loss: 0.009 | (test) acc: 99.38%, loss: 0.020\n",
      "[11/100] (train) acc: 99.75%, loss: 0.008 | (test) acc: 99.45%, loss: 0.025\n",
      "[12/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.22%, loss: 0.033\n",
      "[13/100] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.23%, loss: 0.030\n",
      "[14/100] (train) acc: 99.77%, loss: 0.006 | (test) acc: 99.40%, loss: 0.022\n",
      "[15/100] (train) acc: 99.81%, loss: 0.006 | (test) acc: 99.30%, loss: 0.033\n",
      "[16/100] (train) acc: 99.82%, loss: 0.006 | (test) acc: 99.12%, loss: 0.036\n",
      "[17/100] (train) acc: 99.80%, loss: 0.006 | (test) acc: 99.25%, loss: 0.032\n",
      "[18/100] (train) acc: 99.83%, loss: 0.006 | (test) acc: 99.23%, loss: 0.030\n",
      "[19/100] (train) acc: 99.85%, loss: 0.005 | (test) acc: 99.02%, loss: 0.039\n",
      "[20/100] (train) acc: 99.80%, loss: 0.006 | (test) acc: 99.31%, loss: 0.029\n",
      "[21/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.40%, loss: 0.029\n",
      "[22/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.42%, loss: 0.028\n",
      "[23/100] (train) acc: 99.87%, loss: 0.005 | (test) acc: 98.99%, loss: 0.050\n",
      "[24/100] (train) acc: 99.81%, loss: 0.006 | (test) acc: 99.24%, loss: 0.027\n",
      "[25/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.37%, loss: 0.028\n",
      "[26/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.43%, loss: 0.029\n",
      "[27/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.44%, loss: 0.029\n",
      "[28/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.36%, loss: 0.036\n",
      "[29/100] (train) acc: 99.91%, loss: 0.004 | (test) acc: 99.38%, loss: 0.025\n",
      "[30/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.43%, loss: 0.030\n",
      "[31/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.35%, loss: 0.034\n",
      "[32/100] (train) acc: 99.90%, loss: 0.004 | (test) acc: 99.48%, loss: 0.027\n",
      "[33/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.49%, loss: 0.029\n",
      "[34/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.34%, loss: 0.039\n",
      "[35/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.37%, loss: 0.037\n",
      "[36/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.41%, loss: 0.036\n",
      "[37/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.45%, loss: 0.030\n",
      "[38/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.34%, loss: 0.036\n",
      "[39/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.40%, loss: 0.037\n",
      "[40/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.35%, loss: 0.043\n",
      "[41/100] (train) acc: 99.91%, loss: 0.004 | (test) acc: 99.14%, loss: 0.044\n",
      "[42/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.58%, loss: 0.026\n",
      "[43/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.56%, loss: 0.033\n",
      "[44/100] (train) acc: 99.99%, loss: 0.000 | (test) acc: 99.52%, loss: 0.033\n",
      "[45/100] (train) acc: 99.90%, loss: 0.005 | (test) acc: 99.33%, loss: 0.039\n",
      "[46/100] (train) acc: 99.85%, loss: 0.005 | (test) acc: 99.33%, loss: 0.040\n",
      "[47/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.42%, loss: 0.039\n",
      "[48/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.51%, loss: 0.030\n",
      "[49/100] (train) acc: 99.94%, loss: 0.003 | (test) acc: 99.49%, loss: 0.034\n",
      "[50/100] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.32%, loss: 0.044\n",
      "[51/100] (train) acc: 99.86%, loss: 0.005 | (test) acc: 99.33%, loss: 0.044\n",
      "[52/100] (train) acc: 99.90%, loss: 0.004 | (test) acc: 99.38%, loss: 0.040\n",
      "[53/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.28%, loss: 0.047\n",
      "[54/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.37%, loss: 0.041\n",
      "[55/100] (train) acc: 99.88%, loss: 0.005 | (test) acc: 99.37%, loss: 0.034\n",
      "[56/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.39%, loss: 0.038\n",
      "[57/100] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.38%, loss: 0.043\n",
      "[58/100] (train) acc: 99.85%, loss: 0.006 | (test) acc: 99.29%, loss: 0.055\n",
      "[59/100] (train) acc: 99.99%, loss: 0.001 | (test) acc: 99.40%, loss: 0.046\n",
      "[60/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.37%, loss: 0.047\n",
      "[61/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.38%, loss: 0.038\n",
      "[62/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.16%, loss: 0.056\n",
      "[63/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.19%, loss: 0.069\n",
      "[64/100] (train) acc: 99.91%, loss: 0.004 | (test) acc: 99.34%, loss: 0.056\n",
      "[65/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.37%, loss: 0.049\n",
      "[66/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.38%, loss: 0.059\n",
      "[67/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.27%, loss: 0.084\n",
      "[68/100] (train) acc: 99.86%, loss: 0.007 | (test) acc: 99.45%, loss: 0.049\n",
      "[69/100] (train) acc: 99.92%, loss: 0.004 | (test) acc: 99.27%, loss: 0.070\n",
      "[70/100] (train) acc: 99.91%, loss: 0.004 | (test) acc: 99.44%, loss: 0.050\n",
      "[71/100] (train) acc: 99.90%, loss: 0.004 | (test) acc: 99.44%, loss: 0.048\n",
      "[72/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.50%, loss: 0.053\n",
      "[73/100] (train) acc: 99.99%, loss: 0.001 | (test) acc: 99.36%, loss: 0.064\n",
      "[74/100] (train) acc: 99.92%, loss: 0.004 | (test) acc: 99.29%, loss: 0.082\n",
      "[75/100] (train) acc: 99.89%, loss: 0.005 | (test) acc: 99.39%, loss: 0.063\n",
      "[76/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.25%, loss: 0.078\n",
      "[77/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.47%, loss: 0.053\n",
      "[78/100] (train) acc: 99.95%, loss: 0.003 | (test) acc: 99.31%, loss: 0.060\n",
      "[79/100] (train) acc: 99.91%, loss: 0.005 | (test) acc: 99.33%, loss: 0.043\n",
      "[80/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.45%, loss: 0.054\n",
      "[81/100] (train) acc: 99.97%, loss: 0.002 | (test) acc: 99.39%, loss: 0.050\n",
      "[82/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.40%, loss: 0.046\n",
      "[83/100] (train) acc: 99.86%, loss: 0.009 | (test) acc: 99.43%, loss: 0.046\n",
      "[84/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.30%, loss: 0.077\n",
      "[85/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.38%, loss: 0.079\n",
      "[86/100] (train) acc: 99.94%, loss: 0.003 | (test) acc: 99.39%, loss: 0.060\n",
      "[87/100] (train) acc: 99.91%, loss: 0.004 | (test) acc: 99.37%, loss: 0.062\n",
      "[88/100] (train) acc: 99.93%, loss: 0.004 | (test) acc: 99.37%, loss: 0.061\n",
      "[89/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.37%, loss: 0.075\n",
      "[90/100] (train) acc: 99.94%, loss: 0.003 | (test) acc: 99.34%, loss: 0.070\n",
      "[91/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.46%, loss: 0.054\n",
      "[92/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.33%, loss: 0.072\n",
      "[93/100] (train) acc: 99.89%, loss: 0.008 | (test) acc: 99.42%, loss: 0.054\n",
      "[94/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.48%, loss: 0.052\n",
      "[95/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.45%, loss: 0.054\n",
      "[96/100] (train) acc: 99.93%, loss: 0.004 | (test) acc: 99.23%, loss: 0.086\n",
      "[97/100] (train) acc: 99.91%, loss: 0.004 | (test) acc: 99.39%, loss: 0.071\n",
      "[98/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.43%, loss: 0.072\n",
      "[99/100] (train) acc: 99.89%, loss: 0.007 | (test) acc: 99.33%, loss: 0.103\n",
      "[100/100] (train) acc: 99.90%, loss: 0.006 | (test) acc: 99.35%, loss: 0.085\n",
      "average of last 5 test acc: 99.35%\n"
     ]
    }
   ],
   "source": [
    "train(epoch_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "x_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "h1 = slim.conv2d(x_img, 32, kernel_size=[3,3]) # default relu, xavier\n",
    "h1 = slim.conv2d(h1, 32, kernel_size=[3,3])\n",
    "p1 = slim.max_pool2d(h1, kernel_size=[2,2])\n",
    "d1 = slim.dropout(p1, keep_prob=0.7, is_training=training)\n",
    "assert d1.shape[1:] == [14, 14, 32]\n",
    "\n",
    "h2 = slim.conv2d(d1, 64, kernel_size=[3,3])\n",
    "h2 = slim.conv2d(h2, 64, kernel_size=[3,3])\n",
    "p2 = slim.max_pool2d(h2, kernel_size=[2,2])\n",
    "d2 = slim.dropout(p2, keep_prob=0.7, is_training=training)\n",
    "assert d2.shape[1:] == [7, 7, 64]\n",
    "\n",
    "h3 = slim.conv2d(d2, 128, kernel_size=[3,3])\n",
    "h3 = slim.conv2d(h3, 128, kernel_size=[3,3])\n",
    "p3 = slim.max_pool2d(h3, kernel_size=[2,2], padding='same')\n",
    "d3 = slim.dropout(p3, keep_prob=0.7, is_training=training)\n",
    "assert d3.shape[1:] == [4, 4, 128]\n",
    "\n",
    "flat = slim.flatten(d3)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### new `train()` with `training` placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "def train(epoch_n=20):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    batch_size = 100\n",
    "    N = mnist.train.num_examples\n",
    "    n_iter = N // batch_size\n",
    "\n",
    "    for epoch in range(epoch_n):\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "        for _ in range(n_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, cur_acc, cur_loss = sess.run([train_op, accuracy, loss], \n",
    "                                            feed_dict={X: batch_x, y: batch_y, training: True})\n",
    "            avg_acc += cur_acc\n",
    "            avg_loss += cur_loss\n",
    "\n",
    "        avg_acc /= n_iter\n",
    "        avg_loss /= n_iter\n",
    "\n",
    "        test_acc, test_loss = sess.run([accuracy, loss], \n",
    "                                       feed_dict={X: mnist.test.images, y: mnist.test.labels, training: False})\n",
    "        print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f}\".\n",
    "              format(epoch+1, epoch_n, avg_acc, avg_loss, test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 92.76%, loss: 0.224 | (test) acc: 98.65%, loss: 0.040\n",
      "[ 2/100] (train) acc: 98.25%, loss: 0.060 | (test) acc: 99.01%, loss: 0.029\n",
      "[ 3/100] (train) acc: 98.55%, loss: 0.046 | (test) acc: 98.96%, loss: 0.031\n",
      "[ 4/100] (train) acc: 98.82%, loss: 0.037 | (test) acc: 99.32%, loss: 0.022\n",
      "[ 5/100] (train) acc: 99.00%, loss: 0.032 | (test) acc: 99.33%, loss: 0.022\n",
      "[ 6/100] (train) acc: 99.12%, loss: 0.028 | (test) acc: 99.34%, loss: 0.021\n",
      "[ 7/100] (train) acc: 99.22%, loss: 0.024 | (test) acc: 99.32%, loss: 0.023\n",
      "[ 8/100] (train) acc: 99.28%, loss: 0.023 | (test) acc: 99.27%, loss: 0.024\n",
      "[ 9/100] (train) acc: 99.30%, loss: 0.022 | (test) acc: 99.46%, loss: 0.018\n",
      "[10/100] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.50%, loss: 0.017\n",
      "[11/100] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.52%, loss: 0.015\n",
      "[12/100] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.34%, loss: 0.021\n",
      "[13/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.52%, loss: 0.018\n",
      "[14/100] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.32%, loss: 0.018\n",
      "[15/100] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.46%, loss: 0.017\n",
      "[16/100] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.42%, loss: 0.021\n",
      "[17/100] (train) acc: 99.60%, loss: 0.013 | (test) acc: 99.44%, loss: 0.020\n",
      "[18/100] (train) acc: 99.60%, loss: 0.013 | (test) acc: 99.43%, loss: 0.020\n",
      "[19/100] (train) acc: 99.58%, loss: 0.012 | (test) acc: 99.40%, loss: 0.019\n",
      "[20/100] (train) acc: 99.65%, loss: 0.011 | (test) acc: 99.42%, loss: 0.023\n",
      "[21/100] (train) acc: 99.64%, loss: 0.011 | (test) acc: 99.31%, loss: 0.027\n",
      "[22/100] (train) acc: 99.72%, loss: 0.010 | (test) acc: 99.48%, loss: 0.021\n",
      "[23/100] (train) acc: 99.59%, loss: 0.012 | (test) acc: 99.47%, loss: 0.023\n",
      "[24/100] (train) acc: 99.65%, loss: 0.011 | (test) acc: 99.54%, loss: 0.021\n",
      "[25/100] (train) acc: 99.65%, loss: 0.010 | (test) acc: 99.48%, loss: 0.021\n",
      "[26/100] (train) acc: 99.61%, loss: 0.011 | (test) acc: 99.41%, loss: 0.028\n",
      "[27/100] (train) acc: 99.70%, loss: 0.009 | (test) acc: 99.47%, loss: 0.022\n",
      "[28/100] (train) acc: 99.71%, loss: 0.009 | (test) acc: 99.53%, loss: 0.020\n",
      "[29/100] (train) acc: 99.67%, loss: 0.010 | (test) acc: 99.41%, loss: 0.021\n",
      "[30/100] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.47%, loss: 0.026\n",
      "[31/100] (train) acc: 99.70%, loss: 0.009 | (test) acc: 99.37%, loss: 0.025\n",
      "[32/100] (train) acc: 99.76%, loss: 0.008 | (test) acc: 99.38%, loss: 0.032\n",
      "[33/100] (train) acc: 99.69%, loss: 0.010 | (test) acc: 99.40%, loss: 0.027\n",
      "[34/100] (train) acc: 99.64%, loss: 0.011 | (test) acc: 99.51%, loss: 0.022\n",
      "[35/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.48%, loss: 0.030\n",
      "[36/100] (train) acc: 99.72%, loss: 0.009 | (test) acc: 99.48%, loss: 0.027\n",
      "[37/100] (train) acc: 99.73%, loss: 0.008 | (test) acc: 99.45%, loss: 0.029\n",
      "[38/100] (train) acc: 99.70%, loss: 0.010 | (test) acc: 99.42%, loss: 0.029\n",
      "[39/100] (train) acc: 99.79%, loss: 0.007 | (test) acc: 99.52%, loss: 0.029\n",
      "[40/100] (train) acc: 99.76%, loss: 0.008 | (test) acc: 99.46%, loss: 0.027\n",
      "[41/100] (train) acc: 99.80%, loss: 0.007 | (test) acc: 99.38%, loss: 0.036\n",
      "[42/100] (train) acc: 99.70%, loss: 0.010 | (test) acc: 99.47%, loss: 0.030\n",
      "[43/100] (train) acc: 99.73%, loss: 0.009 | (test) acc: 99.50%, loss: 0.023\n",
      "[44/100] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.42%, loss: 0.029\n",
      "[45/100] (train) acc: 99.75%, loss: 0.008 | (test) acc: 99.51%, loss: 0.024\n",
      "[46/100] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.51%, loss: 0.022\n",
      "[47/100] (train) acc: 99.73%, loss: 0.009 | (test) acc: 99.46%, loss: 0.029\n",
      "[48/100] (train) acc: 99.75%, loss: 0.008 | (test) acc: 99.44%, loss: 0.033\n",
      "[49/100] (train) acc: 99.71%, loss: 0.010 | (test) acc: 99.50%, loss: 0.027\n",
      "[50/100] (train) acc: 99.77%, loss: 0.008 | (test) acc: 99.47%, loss: 0.028\n",
      "[51/100] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.47%, loss: 0.035\n",
      "[52/100] (train) acc: 99.72%, loss: 0.009 | (test) acc: 99.50%, loss: 0.028\n",
      "[53/100] (train) acc: 99.75%, loss: 0.008 | (test) acc: 99.55%, loss: 0.027\n",
      "[54/100] (train) acc: 99.78%, loss: 0.007 | (test) acc: 99.55%, loss: 0.030\n",
      "[55/100] (train) acc: 99.79%, loss: 0.008 | (test) acc: 99.37%, loss: 0.040\n",
      "[56/100] (train) acc: 99.74%, loss: 0.010 | (test) acc: 99.56%, loss: 0.022\n",
      "[57/100] (train) acc: 99.80%, loss: 0.007 | (test) acc: 99.53%, loss: 0.026\n",
      "[58/100] (train) acc: 99.77%, loss: 0.007 | (test) acc: 99.47%, loss: 0.026\n",
      "[59/100] (train) acc: 99.76%, loss: 0.008 | (test) acc: 99.50%, loss: 0.029\n",
      "[60/100] (train) acc: 99.78%, loss: 0.008 | (test) acc: 99.26%, loss: 0.040\n",
      "[61/100] (train) acc: 99.75%, loss: 0.009 | (test) acc: 99.40%, loss: 0.035\n",
      "[62/100] (train) acc: 99.72%, loss: 0.009 | (test) acc: 99.41%, loss: 0.037\n",
      "[63/100] (train) acc: 99.80%, loss: 0.007 | (test) acc: 99.44%, loss: 0.036\n",
      "[64/100] (train) acc: 99.78%, loss: 0.008 | (test) acc: 99.52%, loss: 0.029\n",
      "[65/100] (train) acc: 99.85%, loss: 0.006 | (test) acc: 99.45%, loss: 0.033\n",
      "[66/100] (train) acc: 99.79%, loss: 0.007 | (test) acc: 99.43%, loss: 0.033\n",
      "[67/100] (train) acc: 99.74%, loss: 0.009 | (test) acc: 99.52%, loss: 0.030\n",
      "[68/100] (train) acc: 99.77%, loss: 0.009 | (test) acc: 99.41%, loss: 0.033\n",
      "[69/100] (train) acc: 99.75%, loss: 0.009 | (test) acc: 99.49%, loss: 0.029\n",
      "[70/100] (train) acc: 99.73%, loss: 0.010 | (test) acc: 99.57%, loss: 0.027\n",
      "[71/100] (train) acc: 99.80%, loss: 0.006 | (test) acc: 99.49%, loss: 0.032\n",
      "[72/100] (train) acc: 99.78%, loss: 0.008 | (test) acc: 99.49%, loss: 0.030\n",
      "[73/100] (train) acc: 99.77%, loss: 0.008 | (test) acc: 99.45%, loss: 0.035\n",
      "[74/100] (train) acc: 99.74%, loss: 0.010 | (test) acc: 99.46%, loss: 0.032\n",
      "[75/100] (train) acc: 99.81%, loss: 0.007 | (test) acc: 99.57%, loss: 0.030\n",
      "[76/100] (train) acc: 99.78%, loss: 0.008 | (test) acc: 99.51%, loss: 0.034\n",
      "[77/100] (train) acc: 99.77%, loss: 0.009 | (test) acc: 99.39%, loss: 0.032\n",
      "[78/100] (train) acc: 99.76%, loss: 0.009 | (test) acc: 99.42%, loss: 0.031\n",
      "[79/100] (train) acc: 99.80%, loss: 0.007 | (test) acc: 99.41%, loss: 0.034\n",
      "[80/100] (train) acc: 99.77%, loss: 0.009 | (test) acc: 99.41%, loss: 0.035\n",
      "[81/100] (train) acc: 99.79%, loss: 0.008 | (test) acc: 99.49%, loss: 0.038\n",
      "[82/100] (train) acc: 99.77%, loss: 0.009 | (test) acc: 99.50%, loss: 0.031\n",
      "[83/100] (train) acc: 99.77%, loss: 0.008 | (test) acc: 99.54%, loss: 0.031\n",
      "[84/100] (train) acc: 99.83%, loss: 0.005 | (test) acc: 99.52%, loss: 0.048\n",
      "[85/100] (train) acc: 99.74%, loss: 0.010 | (test) acc: 99.44%, loss: 0.039\n",
      "[86/100] (train) acc: 99.78%, loss: 0.008 | (test) acc: 99.33%, loss: 0.043\n",
      "[87/100] (train) acc: 99.78%, loss: 0.007 | (test) acc: 99.42%, loss: 0.034\n",
      "[88/100] (train) acc: 99.78%, loss: 0.008 | (test) acc: 99.58%, loss: 0.028\n",
      "[89/100] (train) acc: 99.75%, loss: 0.008 | (test) acc: 99.48%, loss: 0.036\n",
      "[90/100] (train) acc: 99.73%, loss: 0.010 | (test) acc: 99.47%, loss: 0.036\n",
      "[91/100] (train) acc: 99.77%, loss: 0.008 | (test) acc: 99.50%, loss: 0.031\n",
      "[92/100] (train) acc: 99.80%, loss: 0.007 | (test) acc: 99.47%, loss: 0.046\n",
      "[93/100] (train) acc: 99.74%, loss: 0.012 | (test) acc: 99.49%, loss: 0.036\n",
      "[94/100] (train) acc: 99.78%, loss: 0.008 | (test) acc: 99.53%, loss: 0.032\n",
      "[95/100] (train) acc: 99.79%, loss: 0.007 | (test) acc: 99.47%, loss: 0.045\n",
      "[96/100] (train) acc: 99.76%, loss: 0.010 | (test) acc: 99.44%, loss: 0.045\n",
      "[97/100] (train) acc: 99.79%, loss: 0.008 | (test) acc: 99.45%, loss: 0.035\n",
      "[98/100] (train) acc: 99.77%, loss: 0.008 | (test) acc: 99.52%, loss: 0.041\n",
      "[99/100] (train) acc: 99.79%, loss: 0.008 | (test) acc: 99.49%, loss: 0.038\n",
      "[100/100] (train) acc: 99.82%, loss: 0.007 | (test) acc: 99.46%, loss: 0.047\n",
      "average of last 5 test acc: 99.47%\n"
     ]
    }
   ],
   "source": [
    "train(epoch_n=100, use_training_ph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "x_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "bn_param = {'is_training': training, 'scale': True, 'decay': 0.99}\n",
    "h1 = slim.conv2d(x_img, 32, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "h1 = slim.conv2d(h1, 32, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "p1 = slim.max_pool2d(h1, kernel_size=[2,2])\n",
    "d1 = slim.dropout(p1, keep_prob=0.7, is_training=training)\n",
    "assert d1.shape[1:] == [14, 14, 32]\n",
    "\n",
    "h2 = slim.conv2d(d1, 64, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "h2 = slim.conv2d(h2, 64, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "p2 = slim.max_pool2d(h2, kernel_size=[2,2])\n",
    "d2 = slim.dropout(p2, keep_prob=0.7, is_training=training)\n",
    "assert d2.shape[1:] == [7, 7, 64]\n",
    "\n",
    "h3 = slim.conv2d(d2, 128, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "h3 = slim.conv2d(h3, 128, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "p3 = slim.max_pool2d(h3, kernel_size=[2,2], padding='same')\n",
    "d3 = slim.dropout(p3, keep_prob=0.7, is_training=training)\n",
    "assert d3.shape[1:] == [4, 4, 128]\n",
    "\n",
    "flat = slim.flatten(d3)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# must do this even with slim\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 93.71%, loss: 0.201 | (test) acc: 97.05%, loss: 0.090\n",
      "[ 2/100] (train) acc: 98.14%, loss: 0.059 | (test) acc: 98.47%, loss: 0.046\n",
      "[ 3/100] (train) acc: 98.52%, loss: 0.049 | (test) acc: 98.99%, loss: 0.035\n",
      "[ 4/100] (train) acc: 98.66%, loss: 0.043 | (test) acc: 99.35%, loss: 0.022\n",
      "[ 5/100] (train) acc: 98.88%, loss: 0.035 | (test) acc: 99.24%, loss: 0.024\n",
      "[ 6/100] (train) acc: 98.90%, loss: 0.035 | (test) acc: 99.02%, loss: 0.029\n",
      "[ 7/100] (train) acc: 99.07%, loss: 0.029 | (test) acc: 98.94%, loss: 0.031\n",
      "[ 8/100] (train) acc: 99.11%, loss: 0.027 | (test) acc: 99.29%, loss: 0.020\n",
      "[ 9/100] (train) acc: 99.14%, loss: 0.027 | (test) acc: 99.16%, loss: 0.027\n",
      "[10/100] (train) acc: 99.27%, loss: 0.024 | (test) acc: 99.27%, loss: 0.023\n",
      "[11/100] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.20%, loss: 0.027\n",
      "[12/100] (train) acc: 99.35%, loss: 0.020 | (test) acc: 99.32%, loss: 0.022\n",
      "[13/100] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.15%, loss: 0.028\n",
      "[14/100] (train) acc: 99.47%, loss: 0.018 | (test) acc: 99.37%, loss: 0.022\n",
      "[15/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.40%, loss: 0.018\n",
      "[16/100] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.43%, loss: 0.019\n",
      "[17/100] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.48%, loss: 0.018\n",
      "[18/100] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.37%, loss: 0.021\n",
      "[19/100] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.43%, loss: 0.020\n",
      "[20/100] (train) acc: 99.59%, loss: 0.012 | (test) acc: 99.45%, loss: 0.018\n",
      "[21/100] (train) acc: 99.63%, loss: 0.011 | (test) acc: 99.55%, loss: 0.019\n",
      "[22/100] (train) acc: 99.62%, loss: 0.012 | (test) acc: 99.43%, loss: 0.022\n",
      "[23/100] (train) acc: 99.64%, loss: 0.011 | (test) acc: 99.51%, loss: 0.017\n",
      "[24/100] (train) acc: 99.70%, loss: 0.010 | (test) acc: 99.53%, loss: 0.017\n",
      "[25/100] (train) acc: 99.69%, loss: 0.010 | (test) acc: 99.51%, loss: 0.019\n",
      "[26/100] (train) acc: 99.71%, loss: 0.009 | (test) acc: 99.49%, loss: 0.019\n",
      "[27/100] (train) acc: 99.72%, loss: 0.008 | (test) acc: 99.55%, loss: 0.021\n",
      "[28/100] (train) acc: 99.72%, loss: 0.008 | (test) acc: 99.50%, loss: 0.021\n",
      "[29/100] (train) acc: 99.67%, loss: 0.010 | (test) acc: 99.44%, loss: 0.021\n",
      "[30/100] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.56%, loss: 0.019\n",
      "[31/100] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.47%, loss: 0.020\n",
      "[32/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.54%, loss: 0.019\n",
      "[33/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.48%, loss: 0.022\n",
      "[34/100] (train) acc: 99.80%, loss: 0.006 | (test) acc: 99.55%, loss: 0.018\n",
      "[35/100] (train) acc: 99.79%, loss: 0.006 | (test) acc: 99.42%, loss: 0.020\n",
      "[36/100] (train) acc: 99.83%, loss: 0.005 | (test) acc: 99.57%, loss: 0.018\n",
      "[37/100] (train) acc: 99.77%, loss: 0.007 | (test) acc: 99.41%, loss: 0.024\n",
      "[38/100] (train) acc: 99.79%, loss: 0.006 | (test) acc: 99.46%, loss: 0.023\n",
      "[39/100] (train) acc: 99.80%, loss: 0.005 | (test) acc: 99.44%, loss: 0.023\n",
      "[40/100] (train) acc: 99.78%, loss: 0.006 | (test) acc: 99.44%, loss: 0.022\n",
      "[41/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.59%, loss: 0.021\n",
      "[42/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.68%, loss: 0.017\n",
      "[43/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.45%, loss: 0.022\n",
      "[44/100] (train) acc: 99.82%, loss: 0.006 | (test) acc: 99.55%, loss: 0.017\n",
      "[45/100] (train) acc: 99.83%, loss: 0.004 | (test) acc: 99.61%, loss: 0.018\n",
      "[46/100] (train) acc: 99.83%, loss: 0.005 | (test) acc: 99.63%, loss: 0.016\n",
      "[47/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.54%, loss: 0.023\n",
      "[48/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.46%, loss: 0.019\n",
      "[49/100] (train) acc: 99.83%, loss: 0.004 | (test) acc: 99.51%, loss: 0.021\n",
      "[50/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.59%, loss: 0.018\n",
      "[51/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.54%, loss: 0.023\n",
      "[52/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.58%, loss: 0.021\n",
      "[53/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.61%, loss: 0.020\n",
      "[54/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.55%, loss: 0.020\n",
      "[55/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.52%, loss: 0.018\n",
      "[56/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.62%, loss: 0.020\n",
      "[57/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.55%, loss: 0.019\n",
      "[58/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.52%, loss: 0.021\n",
      "[59/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.57%, loss: 0.020\n",
      "[60/100] (train) acc: 99.83%, loss: 0.004 | (test) acc: 99.51%, loss: 0.028\n",
      "[61/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.64%, loss: 0.021\n",
      "[62/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.47%, loss: 0.022\n",
      "[63/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.46%, loss: 0.026\n",
      "[64/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.51%, loss: 0.024\n",
      "[65/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.59%, loss: 0.022\n",
      "[66/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.59%, loss: 0.021\n",
      "[67/100] (train) acc: 99.88%, loss: 0.003 | (test) acc: 99.54%, loss: 0.022\n",
      "[68/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.64%, loss: 0.018\n",
      "[69/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.59%, loss: 0.020\n",
      "[70/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.56%, loss: 0.024\n",
      "[71/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.55%, loss: 0.026\n",
      "[72/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.47%, loss: 0.028\n",
      "[73/100] (train) acc: 99.91%, loss: 0.002 | (test) acc: 99.57%, loss: 0.022\n",
      "[74/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.48%, loss: 0.025\n",
      "[75/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.41%, loss: 0.028\n",
      "[76/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.58%, loss: 0.022\n",
      "[77/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.51%, loss: 0.028\n",
      "[78/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.51%, loss: 0.027\n",
      "[79/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.49%, loss: 0.030\n",
      "[80/100] (train) acc: 99.89%, loss: 0.002 | (test) acc: 99.63%, loss: 0.019\n",
      "[81/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.52%, loss: 0.030\n",
      "[82/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.56%, loss: 0.026\n",
      "[83/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.61%, loss: 0.025\n",
      "[84/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.58%, loss: 0.025\n",
      "[85/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.61%, loss: 0.022\n",
      "[86/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.50%, loss: 0.024\n",
      "[87/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.60%, loss: 0.019\n",
      "[88/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.65%, loss: 0.020\n",
      "[89/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.59%, loss: 0.025\n",
      "[90/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.55%, loss: 0.025\n",
      "[91/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.59%, loss: 0.023\n",
      "[92/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.53%, loss: 0.025\n",
      "[93/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.63%, loss: 0.020\n",
      "[94/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.54%, loss: 0.026\n",
      "[95/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.53%, loss: 0.021\n",
      "[96/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.58%, loss: 0.024\n",
      "[97/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.59%, loss: 0.022\n",
      "[98/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.58%, loss: 0.020\n",
      "[99/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.55%, loss: 0.023\n",
      "[100/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.55%, loss: 0.028\n",
      "average of last 5 test acc: 99.57%\n"
     ]
    }
   ],
   "source": [
    "train(epoch_n=100, use_training_ph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# More slim by slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "n_filters = 32\n",
    "bn_param = {'is_training': training, 'scale': True, 'decay': 0.99}\n",
    "with slim.arg_scope([slim.conv2d], kernel_size=[3,3],\n",
    "                    normalizer_fn=slim.batch_norm, normalizer_params=bn_param):\n",
    "    for _ in range(3):\n",
    "        net = slim.conv2d(net, n_filters)\n",
    "        net = slim.conv2d(net, n_filters)\n",
    "        net = slim.max_pool2d(net, kernel_size=[2,2], padding='same')\n",
    "        net = slim.dropout(net, keep_prob=0.7, is_training=training)\n",
    "        n_filters *= 2\n",
    "\n",
    "flat = slim.flatten(net)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# must do this even with slim\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 93.34%, loss: 0.219 | (test) acc: 97.34%, loss: 0.081\n",
      "[ 2/100] (train) acc: 98.04%, loss: 0.062 | (test) acc: 98.86%, loss: 0.034\n",
      "[ 3/100] (train) acc: 98.49%, loss: 0.050 | (test) acc: 99.29%, loss: 0.023\n",
      "[ 4/100] (train) acc: 98.66%, loss: 0.043 | (test) acc: 98.93%, loss: 0.035\n",
      "[ 5/100] (train) acc: 98.83%, loss: 0.036 | (test) acc: 98.58%, loss: 0.044\n",
      "[ 6/100] (train) acc: 99.01%, loss: 0.031 | (test) acc: 99.32%, loss: 0.023\n",
      "[ 7/100] (train) acc: 99.03%, loss: 0.029 | (test) acc: 99.37%, loss: 0.021\n",
      "[ 8/100] (train) acc: 99.16%, loss: 0.027 | (test) acc: 99.15%, loss: 0.025\n",
      "[ 9/100] (train) acc: 99.15%, loss: 0.027 | (test) acc: 99.38%, loss: 0.018\n",
      "[10/100] (train) acc: 99.26%, loss: 0.023 | (test) acc: 99.23%, loss: 0.024\n",
      "[11/100] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.28%, loss: 0.025\n",
      "[12/100] (train) acc: 99.34%, loss: 0.020 | (test) acc: 99.51%, loss: 0.017\n",
      "[13/100] (train) acc: 99.37%, loss: 0.021 | (test) acc: 99.28%, loss: 0.024\n",
      "[14/100] (train) acc: 99.47%, loss: 0.018 | (test) acc: 99.47%, loss: 0.021\n",
      "[15/100] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.53%, loss: 0.017\n",
      "[16/100] (train) acc: 99.53%, loss: 0.016 | (test) acc: 99.41%, loss: 0.020\n",
      "[17/100] (train) acc: 99.51%, loss: 0.014 | (test) acc: 98.80%, loss: 0.045\n",
      "[18/100] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.56%, loss: 0.019\n",
      "[19/100] (train) acc: 99.55%, loss: 0.015 | (test) acc: 99.37%, loss: 0.021\n",
      "[20/100] (train) acc: 99.64%, loss: 0.011 | (test) acc: 99.32%, loss: 0.022\n",
      "[21/100] (train) acc: 99.63%, loss: 0.011 | (test) acc: 99.40%, loss: 0.021\n",
      "[22/100] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.41%, loss: 0.020\n",
      "[23/100] (train) acc: 99.64%, loss: 0.011 | (test) acc: 99.47%, loss: 0.022\n",
      "[24/100] (train) acc: 99.64%, loss: 0.010 | (test) acc: 99.48%, loss: 0.020\n",
      "[25/100] (train) acc: 99.63%, loss: 0.010 | (test) acc: 99.51%, loss: 0.021\n",
      "[26/100] (train) acc: 99.70%, loss: 0.009 | (test) acc: 99.48%, loss: 0.019\n",
      "[27/100] (train) acc: 99.71%, loss: 0.009 | (test) acc: 99.39%, loss: 0.022\n",
      "[28/100] (train) acc: 99.73%, loss: 0.009 | (test) acc: 99.46%, loss: 0.020\n",
      "[29/100] (train) acc: 99.77%, loss: 0.006 | (test) acc: 99.55%, loss: 0.018\n",
      "[30/100] (train) acc: 99.68%, loss: 0.009 | (test) acc: 99.51%, loss: 0.018\n",
      "[31/100] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.51%, loss: 0.020\n",
      "[32/100] (train) acc: 99.77%, loss: 0.007 | (test) acc: 99.40%, loss: 0.027\n",
      "[33/100] (train) acc: 99.74%, loss: 0.007 | (test) acc: 99.48%, loss: 0.019\n",
      "[34/100] (train) acc: 99.77%, loss: 0.006 | (test) acc: 99.45%, loss: 0.021\n",
      "[35/100] (train) acc: 99.77%, loss: 0.006 | (test) acc: 99.48%, loss: 0.022\n",
      "[36/100] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.55%, loss: 0.018\n",
      "[37/100] (train) acc: 99.81%, loss: 0.005 | (test) acc: 99.14%, loss: 0.040\n",
      "[38/100] (train) acc: 99.78%, loss: 0.006 | (test) acc: 99.23%, loss: 0.038\n",
      "[39/100] (train) acc: 99.78%, loss: 0.006 | (test) acc: 99.48%, loss: 0.023\n",
      "[40/100] (train) acc: 99.79%, loss: 0.006 | (test) acc: 99.51%, loss: 0.021\n",
      "[41/100] (train) acc: 99.85%, loss: 0.005 | (test) acc: 99.49%, loss: 0.020\n",
      "[42/100] (train) acc: 99.83%, loss: 0.005 | (test) acc: 99.55%, loss: 0.021\n",
      "[43/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.56%, loss: 0.022\n",
      "[44/100] (train) acc: 99.81%, loss: 0.005 | (test) acc: 99.62%, loss: 0.018\n",
      "[45/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.57%, loss: 0.019\n",
      "[46/100] (train) acc: 99.82%, loss: 0.005 | (test) acc: 99.54%, loss: 0.021\n",
      "[47/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.59%, loss: 0.020\n",
      "[48/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.52%, loss: 0.021\n",
      "[49/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.64%, loss: 0.018\n",
      "[50/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.57%, loss: 0.021\n",
      "[51/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.51%, loss: 0.022\n",
      "[52/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.58%, loss: 0.020\n",
      "[53/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.57%, loss: 0.020\n",
      "[54/100] (train) acc: 99.88%, loss: 0.003 | (test) acc: 99.53%, loss: 0.026\n",
      "[55/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.48%, loss: 0.026\n",
      "[56/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.48%, loss: 0.026\n",
      "[57/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.47%, loss: 0.023\n",
      "[58/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.53%, loss: 0.021\n",
      "[59/100] (train) acc: 99.88%, loss: 0.003 | (test) acc: 99.57%, loss: 0.021\n",
      "[60/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.56%, loss: 0.023\n",
      "[61/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.44%, loss: 0.030\n",
      "[62/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.46%, loss: 0.023\n",
      "[63/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.48%, loss: 0.026\n",
      "[64/100] (train) acc: 99.87%, loss: 0.003 | (test) acc: 99.59%, loss: 0.020\n",
      "[65/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.50%, loss: 0.021\n",
      "[66/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.46%, loss: 0.022\n",
      "[67/100] (train) acc: 99.90%, loss: 0.002 | (test) acc: 99.49%, loss: 0.025\n",
      "[68/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.52%, loss: 0.026\n",
      "[69/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.60%, loss: 0.020\n",
      "[70/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.61%, loss: 0.023\n",
      "[71/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.58%, loss: 0.020\n",
      "[72/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.48%, loss: 0.024\n",
      "[73/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.53%, loss: 0.026\n",
      "[74/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.57%, loss: 0.024\n",
      "[75/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.51%, loss: 0.029\n",
      "[76/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.57%, loss: 0.024\n",
      "[77/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.62%, loss: 0.024\n",
      "[78/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.56%, loss: 0.022\n",
      "[79/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.54%, loss: 0.022\n",
      "[80/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.59%, loss: 0.024\n",
      "[81/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.56%, loss: 0.025\n",
      "[82/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.59%, loss: 0.020\n",
      "[83/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.57%, loss: 0.023\n",
      "[84/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.56%, loss: 0.023\n",
      "[85/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.53%, loss: 0.024\n",
      "[86/100] (train) acc: 99.91%, loss: 0.002 | (test) acc: 99.55%, loss: 0.027\n",
      "[87/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.53%, loss: 0.028\n",
      "[88/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.58%, loss: 0.021\n",
      "[89/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.59%, loss: 0.019\n",
      "[90/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.56%, loss: 0.027\n",
      "[91/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.63%, loss: 0.020\n",
      "[92/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.57%, loss: 0.022\n",
      "[93/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.45%, loss: 0.029\n",
      "[94/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.58%, loss: 0.025\n",
      "[95/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.61%, loss: 0.025\n",
      "[96/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.64%, loss: 0.025\n",
      "[97/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.58%, loss: 0.028\n",
      "[98/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.55%, loss: 0.028\n",
      "[99/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.55%, loss: 0.026\n",
      "[100/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.60%, loss: 0.026\n",
      "average of last 5 test acc: 99.58%\n"
     ]
    }
   ],
   "source": [
    "train(epoch_n=100, use_training_ph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Highest acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "n_filters = 64\n",
    "bn_param = {'is_training': training, 'scale': True, 'decay': 0.999}\n",
    "with slim.arg_scope([slim.conv2d], padding='same', normalizer_fn=slim.batch_norm, normalizer_params=bn_param):\n",
    "    for i in range(3):\n",
    "        net = slim.conv2d(net, n_filters, kernel_size=[3,3])\n",
    "        net = slim.dropout(net, keep_prob=0.7, is_training=training)\n",
    "        net = slim.conv2d(net, n_filters, kernel_size=[3,3])\n",
    "        net = slim.dropout(net, keep_prob=0.7, is_training=training)\n",
    "        if i == 2:\n",
    "            net = slim.conv2d(net, n_filters, kernel_size=[1,1])\n",
    "            net = slim.dropout(net, keep_prob=0.7, is_training=training)\n",
    "        \n",
    "        n_filters *= 2\n",
    "        # 5x5 stride conv\n",
    "        net = slim.conv2d(net, n_filters, kernel_size=[5,5], stride=2)\n",
    "        net = slim.dropout(net, keep_prob=0.7, is_training=training)\n",
    "\n",
    "flat = slim.flatten(net)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# must do this even with slim\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/200] (train) acc: 89.56%, loss: 0.364 | (test) acc: 51.01%, loss: 3.299\n",
      "[ 2/200] (train) acc: 97.69%, loss: 0.082 | (test) acc: 51.34%, loss: 4.660\n",
      "[ 3/200] (train) acc: 98.04%, loss: 0.070 | (test) acc: 58.96%, loss: 2.740\n",
      "[ 4/200] (train) acc: 98.42%, loss: 0.056 | (test) acc: 9.86%, loss: 18.193\n",
      "[ 5/200] (train) acc: 98.51%, loss: 0.052 | (test) acc: 53.71%, loss: 2.925\n",
      "[ 6/200] (train) acc: 98.67%, loss: 0.046 | (test) acc: 86.61%, loss: 0.618\n",
      "[ 7/200] (train) acc: 98.92%, loss: 0.036 | (test) acc: 98.44%, loss: 0.073\n",
      "[ 8/200] (train) acc: 98.97%, loss: 0.035 | (test) acc: 98.91%, loss: 0.057\n",
      "[ 9/200] (train) acc: 99.17%, loss: 0.029 | (test) acc: 99.08%, loss: 0.042\n",
      "[10/200] (train) acc: 99.13%, loss: 0.029 | (test) acc: 99.09%, loss: 0.054\n",
      "[11/200] (train) acc: 99.15%, loss: 0.028 | (test) acc: 99.18%, loss: 0.046\n",
      "[12/200] (train) acc: 99.26%, loss: 0.023 | (test) acc: 99.28%, loss: 0.035\n",
      "[13/200] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.16%, loss: 0.038\n",
      "[14/200] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.31%, loss: 0.035\n",
      "[15/200] (train) acc: 99.45%, loss: 0.018 | (test) acc: 99.43%, loss: 0.029\n",
      "[16/200] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.33%, loss: 0.029\n",
      "[17/200] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.20%, loss: 0.035\n",
      "[18/200] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.39%, loss: 0.035\n",
      "[19/200] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.22%, loss: 0.034\n",
      "[20/200] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.06%, loss: 0.059\n",
      "[21/200] (train) acc: 99.61%, loss: 0.013 | (test) acc: 99.43%, loss: 0.028\n",
      "[22/200] (train) acc: 99.60%, loss: 0.012 | (test) acc: 99.32%, loss: 0.030\n",
      "[23/200] (train) acc: 99.62%, loss: 0.012 | (test) acc: 99.43%, loss: 0.025\n",
      "[24/200] (train) acc: 99.69%, loss: 0.010 | (test) acc: 99.48%, loss: 0.025\n",
      "[25/200] (train) acc: 99.63%, loss: 0.011 | (test) acc: 99.24%, loss: 0.046\n",
      "[26/200] (train) acc: 99.65%, loss: 0.011 | (test) acc: 99.08%, loss: 0.046\n",
      "[27/200] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.44%, loss: 0.027\n",
      "[28/200] (train) acc: 99.72%, loss: 0.009 | (test) acc: 99.46%, loss: 0.025\n",
      "[29/200] (train) acc: 99.67%, loss: 0.010 | (test) acc: 99.59%, loss: 0.023\n",
      "[30/200] (train) acc: 99.73%, loss: 0.008 | (test) acc: 99.48%, loss: 0.024\n",
      "[31/200] (train) acc: 99.77%, loss: 0.007 | (test) acc: 99.58%, loss: 0.022\n",
      "[32/200] (train) acc: 99.68%, loss: 0.010 | (test) acc: 99.40%, loss: 0.034\n",
      "[33/200] (train) acc: 99.82%, loss: 0.007 | (test) acc: 99.37%, loss: 0.031\n",
      "[34/200] (train) acc: 99.78%, loss: 0.007 | (test) acc: 99.41%, loss: 0.029\n",
      "[35/200] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.39%, loss: 0.030\n",
      "[36/200] (train) acc: 99.79%, loss: 0.007 | (test) acc: 99.57%, loss: 0.025\n",
      "[37/200] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.50%, loss: 0.023\n",
      "[38/200] (train) acc: 99.82%, loss: 0.005 | (test) acc: 99.49%, loss: 0.026\n",
      "[39/200] (train) acc: 99.79%, loss: 0.007 | (test) acc: 99.47%, loss: 0.028\n",
      "[40/200] (train) acc: 99.80%, loss: 0.006 | (test) acc: 99.41%, loss: 0.031\n",
      "[41/200] (train) acc: 99.81%, loss: 0.006 | (test) acc: 99.56%, loss: 0.021\n",
      "[42/200] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.43%, loss: 0.025\n",
      "[43/200] (train) acc: 99.82%, loss: 0.005 | (test) acc: 99.56%, loss: 0.025\n",
      "[44/200] (train) acc: 99.82%, loss: 0.005 | (test) acc: 99.53%, loss: 0.028\n",
      "[45/200] (train) acc: 99.83%, loss: 0.005 | (test) acc: 99.38%, loss: 0.029\n",
      "[46/200] (train) acc: 99.83%, loss: 0.006 | (test) acc: 99.45%, loss: 0.027\n",
      "[47/200] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.59%, loss: 0.022\n",
      "[48/200] (train) acc: 99.83%, loss: 0.005 | (test) acc: 99.55%, loss: 0.027\n",
      "[49/200] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.53%, loss: 0.027\n",
      "[50/200] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.44%, loss: 0.034\n",
      "[51/200] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.50%, loss: 0.032\n",
      "[52/200] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.52%, loss: 0.031\n",
      "[53/200] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.54%, loss: 0.032\n",
      "[54/200] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.45%, loss: 0.026\n",
      "[55/200] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.52%, loss: 0.027\n",
      "[56/200] (train) acc: 99.91%, loss: 0.002 | (test) acc: 99.39%, loss: 0.030\n",
      "[57/200] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.43%, loss: 0.036\n",
      "[58/200] (train) acc: 99.86%, loss: 0.005 | (test) acc: 99.35%, loss: 0.038\n",
      "[59/200] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.58%, loss: 0.027\n",
      "[60/200] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.39%, loss: 0.041\n",
      "[61/200] (train) acc: 99.90%, loss: 0.002 | (test) acc: 99.51%, loss: 0.029\n",
      "[62/200] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.59%, loss: 0.027\n",
      "[63/200] (train) acc: 99.88%, loss: 0.003 | (test) acc: 99.60%, loss: 0.026\n",
      "[64/200] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.56%, loss: 0.030\n",
      "[65/200] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.48%, loss: 0.031\n",
      "[66/200] (train) acc: 99.88%, loss: 0.003 | (test) acc: 99.50%, loss: 0.031\n",
      "[67/200] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.48%, loss: 0.032\n",
      "[68/200] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.57%, loss: 0.027\n",
      "[69/200] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.54%, loss: 0.028\n",
      "[70/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.55%, loss: 0.028\n",
      "[71/200] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.51%, loss: 0.035\n",
      "[72/200] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.54%, loss: 0.031\n",
      "[73/200] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.50%, loss: 0.029\n",
      "[74/200] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.52%, loss: 0.029\n",
      "[75/200] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.54%, loss: 0.026\n",
      "[76/200] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.57%, loss: 0.027\n",
      "[77/200] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.61%, loss: 0.027\n",
      "[78/200] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.61%, loss: 0.026\n",
      "[79/200] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.56%, loss: 0.028\n",
      "[80/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.52%, loss: 0.031\n",
      "[81/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.47%, loss: 0.035\n",
      "[82/200] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.51%, loss: 0.029\n",
      "[83/200] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.52%, loss: 0.034\n",
      "[84/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.47%, loss: 0.034\n",
      "[85/200] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.56%, loss: 0.034\n",
      "[86/200] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.56%, loss: 0.032\n",
      "[87/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.53%, loss: 0.035\n",
      "[88/200] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.56%, loss: 0.025\n",
      "[89/200] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.50%, loss: 0.039\n",
      "[90/200] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.58%, loss: 0.030\n",
      "[91/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.55%, loss: 0.030\n",
      "[92/200] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.55%, loss: 0.032\n",
      "[93/200] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.33%, loss: 0.037\n",
      "[94/200] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.56%, loss: 0.026\n",
      "[95/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.62%, loss: 0.027\n",
      "[96/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.50%, loss: 0.031\n",
      "[97/200] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.59%, loss: 0.027\n",
      "[98/200] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.55%, loss: 0.031\n",
      "[99/200] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.61%, loss: 0.031\n",
      "[100/200] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.50%, loss: 0.032\n",
      "[101/200] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.50%, loss: 0.032\n",
      "[102/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.53%, loss: 0.036\n",
      "[103/200] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.53%, loss: 0.031\n",
      "[104/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.36%, loss: 0.037\n",
      "[105/200] (train) acc: 99.91%, loss: 0.002 | (test) acc: 99.54%, loss: 0.031\n",
      "[106/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.55%, loss: 0.033\n",
      "[107/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.53%, loss: 0.031\n",
      "[108/200] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.48%, loss: 0.037\n",
      "[109/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.61%, loss: 0.031\n",
      "[110/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.56%, loss: 0.029\n",
      "[111/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.54%, loss: 0.037\n",
      "[112/200] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.47%, loss: 0.037\n",
      "[113/200] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.53%, loss: 0.040\n",
      "[114/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.50%, loss: 0.039\n",
      "[115/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.55%, loss: 0.037\n",
      "[116/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.37%, loss: 0.037\n",
      "[117/200] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.49%, loss: 0.037\n",
      "[118/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.54%, loss: 0.034\n",
      "[119/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.54%, loss: 0.039\n",
      "[120/200] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.60%, loss: 0.037\n",
      "[121/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.54%, loss: 0.037\n",
      "[122/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.63%, loss: 0.033\n",
      "[123/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.61%, loss: 0.030\n",
      "[124/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.56%, loss: 0.034\n",
      "[125/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.57%, loss: 0.030\n",
      "[126/200] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.56%, loss: 0.034\n",
      "[127/200] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.46%, loss: 0.037\n",
      "[128/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.58%, loss: 0.037\n",
      "[129/200] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.56%, loss: 0.031\n",
      "[130/200] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.60%, loss: 0.039\n",
      "[131/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.60%, loss: 0.032\n",
      "[132/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.45%, loss: 0.036\n",
      "[133/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.57%, loss: 0.034\n",
      "[134/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.61%, loss: 0.030\n",
      "[135/200] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.55%, loss: 0.032\n",
      "[136/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.44%, loss: 0.044\n",
      "[137/200] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.43%, loss: 0.035\n",
      "[138/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.45%, loss: 0.043\n",
      "[139/200] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.52%, loss: 0.039\n",
      "[140/200] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.50%, loss: 0.039\n",
      "[141/200] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.44%, loss: 0.036\n",
      "[142/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.50%, loss: 0.039\n",
      "[143/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.49%, loss: 0.037\n",
      "[144/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.56%, loss: 0.036\n",
      "[145/200] (train) acc: 99.98%, loss: 0.001 | (test) acc: 99.43%, loss: 0.042\n",
      "[146/200] (train) acc: 99.98%, loss: 0.001 | (test) acc: 99.40%, loss: 0.051\n",
      "[147/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.56%, loss: 0.035\n",
      "[148/200] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.51%, loss: 0.042\n",
      "[149/200] (train) acc: 99.91%, loss: 0.002 | (test) acc: 99.53%, loss: 0.038\n",
      "[150/200] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.55%, loss: 0.037\n",
      "[151/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.53%, loss: 0.042\n",
      "[152/200] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.55%, loss: 0.034\n",
      "[153/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.57%, loss: 0.035\n",
      "[154/200] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.48%, loss: 0.037\n",
      "[155/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.56%, loss: 0.040\n",
      "[156/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.57%, loss: 0.033\n",
      "[157/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.53%, loss: 0.043\n",
      "[158/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.55%, loss: 0.041\n",
      "[159/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.56%, loss: 0.032\n",
      "[160/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.56%, loss: 0.032\n",
      "[161/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.62%, loss: 0.039\n",
      "[162/200] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.62%, loss: 0.032\n",
      "[163/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.52%, loss: 0.040\n",
      "[164/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.56%, loss: 0.035\n",
      "[165/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.62%, loss: 0.025\n",
      "[166/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.53%, loss: 0.035\n",
      "[167/200] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.56%, loss: 0.031\n",
      "[168/200] (train) acc: 99.98%, loss: 0.001 | (test) acc: 99.66%, loss: 0.026\n",
      "[169/200] (train) acc: 99.98%, loss: 0.001 | (test) acc: 99.64%, loss: 0.029\n",
      "[170/200] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.61%, loss: 0.032\n",
      "[171/200] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.60%, loss: 0.030\n",
      "[172/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.58%, loss: 0.032\n",
      "[173/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.62%, loss: 0.035\n",
      "[174/200] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.49%, loss: 0.039\n",
      "[175/200] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.57%, loss: 0.033\n",
      "[176/200] (train) acc: 99.98%, loss: 0.001 | (test) acc: 99.58%, loss: 0.037\n",
      "[177/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.61%, loss: 0.036\n",
      "[178/200] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.52%, loss: 0.040\n",
      "[179/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.63%, loss: 0.032\n",
      "[180/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.60%, loss: 0.031\n",
      "[181/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.57%, loss: 0.039\n",
      "[182/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.58%, loss: 0.032\n",
      "[183/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.63%, loss: 0.035\n",
      "[184/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.59%, loss: 0.040\n",
      "[185/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.55%, loss: 0.043\n",
      "[186/200] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.57%, loss: 0.043\n",
      "[187/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.65%, loss: 0.037\n",
      "[188/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.46%, loss: 0.049\n",
      "[189/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.60%, loss: 0.029\n",
      "[190/200] (train) acc: 99.98%, loss: 0.001 | (test) acc: 99.48%, loss: 0.035\n",
      "[191/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.64%, loss: 0.030\n",
      "[192/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.68%, loss: 0.029\n",
      "[193/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.50%, loss: 0.044\n",
      "[194/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.57%, loss: 0.040\n",
      "[195/200] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.46%, loss: 0.044\n",
      "[196/200] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.51%, loss: 0.041\n",
      "[197/200] (train) acc: 99.98%, loss: 0.001 | (test) acc: 99.56%, loss: 0.034\n",
      "[198/200] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.56%, loss: 0.035\n",
      "[199/200] (train) acc: 99.99%, loss: 0.001 | (test) acc: 99.59%, loss: 0.035\n",
      "[200/200] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.44%, loss: 0.046\n",
      "average of last 5 test acc: 99.53%\n"
     ]
    }
   ],
   "source": [
    "train(epoch_n=200, use_training_ph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Currently, our best model is not `highest acc`, but `BN`.\n",
    "* Comparison with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AffineGenerator():\n",
    "    def __init__(self, mnist):\n",
    "        from keras.preprocessing.image import ImageDataGenerator\n",
    "        \n",
    "        self.mnist = mnist\n",
    "        self.datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1)\n",
    "        self.train_x = np.reshape(self.mnist.train.images, [-1, 28, 28, 1])\n",
    "        self.train_y = self.mnist.train.labels\n",
    "\n",
    "    def generate(self, batch_size=64):\n",
    "        cnt = 0\n",
    "        batch_n = self.train_x.shape[0] // batch_size\n",
    "        for x, y in self.datagen.flow(self.train_x, self.train_y, batch_size=batch_size):\n",
    "            ret_x = x.reshape(-1, 784)\n",
    "            yield ret_x, y\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt == batch_n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "def train_da(epoch_n=20, batch_size=100, use_training_ph=False, datagen=AffineGenerator(mnist)):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    N = mnist.train.num_examples\n",
    "    dq = collections.deque(maxlen=5)\n",
    "\n",
    "    for epoch in range(epoch_n):\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "\n",
    "        n_iter = 0\n",
    "        for batch_x, batch_y in datagen.generate(batch_size=batch_size):\n",
    "            feed_dict = {X: batch_x, y: batch_y}\n",
    "            if use_training_ph:\n",
    "                feed_dict[training] = True\n",
    "            _, cur_acc, cur_loss = sess.run([train_op, accuracy, loss], feed_dict=feed_dict)\n",
    "            avg_acc += cur_acc\n",
    "            avg_loss += cur_loss\n",
    "            n_iter += 1\n",
    "\n",
    "        avg_acc /= n_iter\n",
    "        avg_loss /= n_iter\n",
    "\n",
    "        test_acc = 0.\n",
    "        test_loss = 0.\n",
    "        for _ in range(mnist.test.num_examples // 1000):\n",
    "            batch_x, batch_y = mnist.test.next_batch(1000)\n",
    "            feed_dict = {X: batch_x, y: batch_y}\n",
    "            if use_training_ph:\n",
    "                feed_dict[training] = False\n",
    "            cur_acc, cur_loss = sess.run([accuracy, loss], feed_dict=feed_dict)\n",
    "            test_acc += cur_acc\n",
    "            test_loss += cur_loss\n",
    "        test_acc /= (mnist.test.num_examples // 1000)\n",
    "        test_loss /= (mnist.test.num_examples // 1000)\n",
    "\n",
    "        print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f}\".\n",
    "              format(epoch+1, epoch_n, avg_acc, avg_loss, test_acc, test_loss))\n",
    "        dq.append(test_acc)\n",
    "\n",
    "    score = np.average(dq)\n",
    "    print(\"average of last 5 test acc: {:.2%}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# same as BN model\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "n_filters = 32\n",
    "bn_param = {'is_training': training, 'scale': True, 'decay': 0.99}\n",
    "with slim.arg_scope([slim.conv2d], kernel_size=[3,3],\n",
    "                    normalizer_fn=slim.batch_norm, normalizer_params=bn_param):\n",
    "    for _ in range(3):\n",
    "        net = slim.conv2d(net, n_filters)\n",
    "        net = slim.conv2d(net, n_filters)\n",
    "        net = slim.max_pool2d(net, kernel_size=[2,2], padding='same')\n",
    "        net = slim.dropout(net, keep_prob=0.7, is_training=training)\n",
    "        n_filters *= 2\n",
    "\n",
    "flat = slim.flatten(net)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# must do this even with slim\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 88.60%, loss: 0.359 | (test) acc: 97.81%, loss: 0.066\n",
      "[ 2/100] (train) acc: 96.40%, loss: 0.115 | (test) acc: 98.95%, loss: 0.031\n",
      "[ 3/100] (train) acc: 97.18%, loss: 0.091 | (test) acc: 99.15%, loss: 0.026\n",
      "[ 4/100] (train) acc: 97.73%, loss: 0.075 | (test) acc: 99.40%, loss: 0.018\n",
      "[ 5/100] (train) acc: 97.89%, loss: 0.068 | (test) acc: 99.23%, loss: 0.024\n",
      "[ 6/100] (train) acc: 97.98%, loss: 0.066 | (test) acc: 99.36%, loss: 0.017\n",
      "[ 7/100] (train) acc: 98.29%, loss: 0.055 | (test) acc: 99.39%, loss: 0.018\n",
      "[ 8/100] (train) acc: 98.37%, loss: 0.051 | (test) acc: 99.42%, loss: 0.020\n",
      "[ 9/100] (train) acc: 98.38%, loss: 0.052 | (test) acc: 99.32%, loss: 0.022\n",
      "[10/100] (train) acc: 98.57%, loss: 0.046 | (test) acc: 99.33%, loss: 0.022\n",
      "[11/100] (train) acc: 98.57%, loss: 0.047 | (test) acc: 99.53%, loss: 0.014\n",
      "[12/100] (train) acc: 98.61%, loss: 0.044 | (test) acc: 99.56%, loss: 0.015\n",
      "[13/100] (train) acc: 98.64%, loss: 0.044 | (test) acc: 99.37%, loss: 0.018\n",
      "[14/100] (train) acc: 98.69%, loss: 0.041 | (test) acc: 99.46%, loss: 0.020\n",
      "[15/100] (train) acc: 98.83%, loss: 0.038 | (test) acc: 99.57%, loss: 0.013\n",
      "[16/100] (train) acc: 98.87%, loss: 0.036 | (test) acc: 99.33%, loss: 0.020\n",
      "[17/100] (train) acc: 98.84%, loss: 0.036 | (test) acc: 99.51%, loss: 0.015\n",
      "[18/100] (train) acc: 98.90%, loss: 0.035 | (test) acc: 99.67%, loss: 0.010\n",
      "[19/100] (train) acc: 98.89%, loss: 0.034 | (test) acc: 99.31%, loss: 0.020\n",
      "[20/100] (train) acc: 98.93%, loss: 0.034 | (test) acc: 99.51%, loss: 0.016\n",
      "[21/100] (train) acc: 99.02%, loss: 0.031 | (test) acc: 99.60%, loss: 0.014\n",
      "[22/100] (train) acc: 99.05%, loss: 0.031 | (test) acc: 99.55%, loss: 0.013\n",
      "[23/100] (train) acc: 98.99%, loss: 0.031 | (test) acc: 99.59%, loss: 0.013\n",
      "[24/100] (train) acc: 99.09%, loss: 0.029 | (test) acc: 99.63%, loss: 0.011\n",
      "[25/100] (train) acc: 99.10%, loss: 0.029 | (test) acc: 99.54%, loss: 0.015\n",
      "[26/100] (train) acc: 99.14%, loss: 0.028 | (test) acc: 99.60%, loss: 0.013\n",
      "[27/100] (train) acc: 99.14%, loss: 0.028 | (test) acc: 99.64%, loss: 0.013\n",
      "[28/100] (train) acc: 99.11%, loss: 0.027 | (test) acc: 99.57%, loss: 0.015\n",
      "[29/100] (train) acc: 99.17%, loss: 0.026 | (test) acc: 99.53%, loss: 0.013\n",
      "[30/100] (train) acc: 99.17%, loss: 0.026 | (test) acc: 99.44%, loss: 0.018\n",
      "[31/100] (train) acc: 99.20%, loss: 0.025 | (test) acc: 99.65%, loss: 0.011\n",
      "[32/100] (train) acc: 99.23%, loss: 0.024 | (test) acc: 99.61%, loss: 0.012\n",
      "[33/100] (train) acc: 99.23%, loss: 0.024 | (test) acc: 99.61%, loss: 0.012\n",
      "[34/100] (train) acc: 99.27%, loss: 0.024 | (test) acc: 99.26%, loss: 0.021\n",
      "[35/100] (train) acc: 99.24%, loss: 0.024 | (test) acc: 99.52%, loss: 0.014\n",
      "[36/100] (train) acc: 99.20%, loss: 0.026 | (test) acc: 99.61%, loss: 0.012\n",
      "[37/100] (train) acc: 99.27%, loss: 0.023 | (test) acc: 99.66%, loss: 0.012\n",
      "[38/100] (train) acc: 99.33%, loss: 0.022 | (test) acc: 99.63%, loss: 0.012\n",
      "[39/100] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.64%, loss: 0.015\n",
      "[40/100] (train) acc: 99.34%, loss: 0.021 | (test) acc: 99.68%, loss: 0.010\n",
      "[41/100] (train) acc: 99.31%, loss: 0.022 | (test) acc: 99.68%, loss: 0.012\n",
      "[42/100] (train) acc: 99.34%, loss: 0.022 | (test) acc: 99.52%, loss: 0.014\n",
      "[43/100] (train) acc: 99.32%, loss: 0.021 | (test) acc: 99.64%, loss: 0.012\n",
      "[44/100] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.64%, loss: 0.011\n",
      "[45/100] (train) acc: 99.39%, loss: 0.020 | (test) acc: 99.66%, loss: 0.011\n",
      "[46/100] (train) acc: 99.34%, loss: 0.020 | (test) acc: 99.61%, loss: 0.013\n",
      "[47/100] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.55%, loss: 0.013\n",
      "[48/100] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.59%, loss: 0.012\n",
      "[49/100] (train) acc: 99.36%, loss: 0.019 | (test) acc: 99.58%, loss: 0.015\n",
      "[50/100] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.64%, loss: 0.013\n",
      "[51/100] (train) acc: 99.31%, loss: 0.021 | (test) acc: 99.58%, loss: 0.015\n",
      "[52/100] (train) acc: 99.38%, loss: 0.020 | (test) acc: 99.65%, loss: 0.012\n",
      "[53/100] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.60%, loss: 0.011\n",
      "[54/100] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.66%, loss: 0.012\n",
      "[55/100] (train) acc: 99.38%, loss: 0.018 | (test) acc: 99.64%, loss: 0.011\n",
      "[56/100] (train) acc: 99.41%, loss: 0.019 | (test) acc: 99.65%, loss: 0.011\n",
      "[57/100] (train) acc: 99.49%, loss: 0.017 | (test) acc: 99.65%, loss: 0.012\n",
      "[58/100] (train) acc: 99.41%, loss: 0.017 | (test) acc: 99.64%, loss: 0.010\n",
      "[59/100] (train) acc: 99.41%, loss: 0.019 | (test) acc: 99.65%, loss: 0.013\n",
      "[60/100] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.60%, loss: 0.012\n",
      "[61/100] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.73%, loss: 0.010\n",
      "[62/100] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.64%, loss: 0.012\n",
      "[63/100] (train) acc: 99.45%, loss: 0.018 | (test) acc: 99.67%, loss: 0.009\n",
      "[64/100] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.69%, loss: 0.009\n",
      "[65/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.68%, loss: 0.011\n",
      "[66/100] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.71%, loss: 0.011\n",
      "[67/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.69%, loss: 0.010\n",
      "[68/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.65%, loss: 0.010\n",
      "[69/100] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.68%, loss: 0.011\n",
      "[70/100] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.65%, loss: 0.011\n",
      "[71/100] (train) acc: 99.44%, loss: 0.016 | (test) acc: 99.69%, loss: 0.010\n",
      "[72/100] (train) acc: 99.48%, loss: 0.015 | (test) acc: 99.65%, loss: 0.012\n",
      "[73/100] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.67%, loss: 0.011\n",
      "[74/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.63%, loss: 0.011\n",
      "[75/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.70%, loss: 0.010\n",
      "[76/100] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.75%, loss: 0.009\n",
      "[77/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.67%, loss: 0.011\n",
      "[78/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.62%, loss: 0.012\n",
      "[79/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.72%, loss: 0.009\n",
      "[80/100] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.74%, loss: 0.010\n",
      "[81/100] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.62%, loss: 0.012\n",
      "[82/100] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.71%, loss: 0.009\n",
      "[83/100] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.61%, loss: 0.012\n",
      "[84/100] (train) acc: 99.48%, loss: 0.015 | (test) acc: 99.66%, loss: 0.010\n",
      "[85/100] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.65%, loss: 0.011\n",
      "[86/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.65%, loss: 0.012\n",
      "[87/100] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.63%, loss: 0.012\n",
      "[88/100] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.69%, loss: 0.011\n",
      "[89/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.66%, loss: 0.012\n",
      "[90/100] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.69%, loss: 0.010\n",
      "[91/100] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.69%, loss: 0.010\n",
      "[92/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.71%, loss: 0.009\n",
      "[93/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.73%, loss: 0.010\n",
      "[94/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.66%, loss: 0.011\n",
      "[95/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.72%, loss: 0.009\n",
      "[96/100] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.69%, loss: 0.010\n",
      "[97/100] (train) acc: 99.51%, loss: 0.014 | (test) acc: 99.64%, loss: 0.012\n",
      "[98/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.67%, loss: 0.010\n",
      "[99/100] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.64%, loss: 0.010\n",
      "[100/100] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.64%, loss: 0.009\n",
      "average of last 5 test acc: 99.66%\n"
     ]
    }
   ],
   "source": [
    "train_da(epoch_n=100, use_training_ph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def one_hot(dense, n_class=10):\n",
    "    N = dense.shape[0]\n",
    "    ret = np.zeros([N, n_class])\n",
    "    ret[np.arange(N), dense] = 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BestModel(object):\n",
    "    def __init__(self, lr):\n",
    "        X = tf.placeholder(tf.float32, [None, 784])\n",
    "        y = tf.placeholder(tf.float32, [None, 10])\n",
    "        training = tf.placeholder(tf.bool)\n",
    "\n",
    "        net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "        n_filters = 32\n",
    "        bn_param = {'is_training': training, 'scale': True, 'decay': 0.99}\n",
    "        with slim.arg_scope([slim.conv2d], kernel_size=[3,3],\n",
    "                            normalizer_fn=slim.batch_norm, normalizer_params=bn_param):\n",
    "            for _ in range(3):\n",
    "                net = slim.conv2d(net, n_filters)\n",
    "                net = slim.conv2d(net, n_filters)\n",
    "                net = slim.max_pool2d(net, kernel_size=[2,2], padding='same')\n",
    "                net = slim.dropout(net, keep_prob=0.7, is_training=training)\n",
    "                n_filters *= 2\n",
    "\n",
    "        flat = slim.flatten(net)\n",
    "        logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "        prob = tf.nn.softmax(logits)\n",
    "\n",
    "        # add predict ops for majority voting ensemble\n",
    "        predict = tf.argmax(logits, axis=1)\n",
    "        correct = tf.equal(predict, tf.argmax(y, axis=1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # must do this even with slim\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "        \n",
    "        # for interaction\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.training = training\n",
    "        self.predict = predict\n",
    "        self.accuracy = accuracy\n",
    "        self.loss = loss\n",
    "        self.train_op = train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model0 ... {'lr': 0.0003, 'batch_size': 50, 'epoch_n': 80}\n",
      "[ 1/80] (train) acc: 85.35%, loss: 0.453 | (test) acc: 98.43%, loss: 0.046\n",
      "[ 2/80] (train) acc: 95.77%, loss: 0.136 | (test) acc: 98.97%, loss: 0.032\n",
      "[ 3/80] (train) acc: 96.84%, loss: 0.104 | (test) acc: 99.24%, loss: 0.024\n",
      "[ 4/80] (train) acc: 97.27%, loss: 0.088 | (test) acc: 99.13%, loss: 0.027\n",
      "[ 5/80] (train) acc: 97.65%, loss: 0.074 | (test) acc: 99.31%, loss: 0.021\n",
      "[ 6/80] (train) acc: 97.83%, loss: 0.070 | (test) acc: 99.42%, loss: 0.018\n",
      "[ 7/80] (train) acc: 98.03%, loss: 0.062 | (test) acc: 99.27%, loss: 0.022\n",
      "[ 8/80] (train) acc: 98.20%, loss: 0.057 | (test) acc: 99.36%, loss: 0.022\n",
      "[ 9/80] (train) acc: 98.31%, loss: 0.056 | (test) acc: 99.45%, loss: 0.018\n",
      "[10/80] (train) acc: 98.37%, loss: 0.052 | (test) acc: 99.42%, loss: 0.017\n",
      "[11/80] (train) acc: 98.51%, loss: 0.048 | (test) acc: 99.50%, loss: 0.016\n",
      "[12/80] (train) acc: 98.54%, loss: 0.047 | (test) acc: 99.42%, loss: 0.018\n",
      "[13/80] (train) acc: 98.64%, loss: 0.045 | (test) acc: 99.45%, loss: 0.016\n",
      "[14/80] (train) acc: 98.67%, loss: 0.042 | (test) acc: 99.31%, loss: 0.021\n",
      "[15/80] (train) acc: 98.71%, loss: 0.041 | (test) acc: 99.51%, loss: 0.015\n",
      "[16/80] (train) acc: 98.76%, loss: 0.039 | (test) acc: 99.42%, loss: 0.017\n",
      "[17/80] (train) acc: 98.74%, loss: 0.040 | (test) acc: 99.39%, loss: 0.018\n",
      "[18/80] (train) acc: 98.87%, loss: 0.037 | (test) acc: 99.53%, loss: 0.013\n",
      "[19/80] (train) acc: 98.93%, loss: 0.034 | (test) acc: 99.61%, loss: 0.014\n",
      "[20/80] (train) acc: 98.86%, loss: 0.036 | (test) acc: 99.41%, loss: 0.017\n",
      "[21/80] (train) acc: 98.90%, loss: 0.036 | (test) acc: 99.58%, loss: 0.014\n",
      "[22/80] (train) acc: 99.00%, loss: 0.031 | (test) acc: 99.57%, loss: 0.012\n",
      "[23/80] (train) acc: 98.89%, loss: 0.035 | (test) acc: 99.57%, loss: 0.014\n",
      "[24/80] (train) acc: 98.88%, loss: 0.035 | (test) acc: 99.56%, loss: 0.014\n",
      "[25/80] (train) acc: 99.01%, loss: 0.031 | (test) acc: 99.60%, loss: 0.011\n",
      "[26/80] (train) acc: 99.07%, loss: 0.030 | (test) acc: 99.65%, loss: 0.012\n",
      "[27/80] (train) acc: 99.06%, loss: 0.030 | (test) acc: 99.52%, loss: 0.014\n",
      "[28/80] (train) acc: 99.03%, loss: 0.030 | (test) acc: 99.58%, loss: 0.015\n",
      "[29/80] (train) acc: 99.07%, loss: 0.029 | (test) acc: 99.56%, loss: 0.013\n",
      "[30/80] (train) acc: 99.10%, loss: 0.030 | (test) acc: 99.48%, loss: 0.018\n",
      "[31/80] (train) acc: 99.14%, loss: 0.027 | (test) acc: 99.65%, loss: 0.009\n",
      "[32/80] (train) acc: 99.07%, loss: 0.029 | (test) acc: 99.59%, loss: 0.013\n",
      "[33/80] (train) acc: 99.19%, loss: 0.025 | (test) acc: 99.63%, loss: 0.010\n",
      "[34/80] (train) acc: 99.18%, loss: 0.026 | (test) acc: 99.49%, loss: 0.014\n",
      "[35/80] (train) acc: 99.23%, loss: 0.024 | (test) acc: 99.50%, loss: 0.013\n",
      "[36/80] (train) acc: 99.23%, loss: 0.025 | (test) acc: 99.58%, loss: 0.014\n",
      "[37/80] (train) acc: 99.15%, loss: 0.027 | (test) acc: 99.49%, loss: 0.014\n",
      "[38/80] (train) acc: 99.23%, loss: 0.025 | (test) acc: 99.48%, loss: 0.016\n",
      "[39/80] (train) acc: 99.19%, loss: 0.025 | (test) acc: 99.57%, loss: 0.013\n",
      "[40/80] (train) acc: 99.21%, loss: 0.024 | (test) acc: 99.65%, loss: 0.012\n",
      "[41/80] (train) acc: 99.25%, loss: 0.024 | (test) acc: 99.62%, loss: 0.011\n",
      "[42/80] (train) acc: 99.21%, loss: 0.025 | (test) acc: 99.69%, loss: 0.011\n",
      "[43/80] (train) acc: 99.22%, loss: 0.025 | (test) acc: 99.68%, loss: 0.009\n",
      "[44/80] (train) acc: 99.25%, loss: 0.023 | (test) acc: 99.67%, loss: 0.011\n",
      "[45/80] (train) acc: 99.29%, loss: 0.021 | (test) acc: 99.54%, loss: 0.013\n",
      "[46/80] (train) acc: 99.28%, loss: 0.021 | (test) acc: 99.65%, loss: 0.011\n",
      "[47/80] (train) acc: 99.35%, loss: 0.021 | (test) acc: 99.57%, loss: 0.014\n",
      "[48/80] (train) acc: 99.30%, loss: 0.022 | (test) acc: 99.66%, loss: 0.011\n",
      "[49/80] (train) acc: 99.31%, loss: 0.022 | (test) acc: 99.63%, loss: 0.012\n",
      "[50/80] (train) acc: 99.28%, loss: 0.023 | (test) acc: 99.58%, loss: 0.013\n",
      "[51/80] (train) acc: 99.33%, loss: 0.022 | (test) acc: 99.61%, loss: 0.011\n",
      "[52/80] (train) acc: 99.34%, loss: 0.021 | (test) acc: 99.63%, loss: 0.011\n",
      "[53/80] (train) acc: 99.39%, loss: 0.019 | (test) acc: 99.63%, loss: 0.011\n",
      "[54/80] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.66%, loss: 0.010\n",
      "[55/80] (train) acc: 99.33%, loss: 0.020 | (test) acc: 99.64%, loss: 0.011\n",
      "[56/80] (train) acc: 99.31%, loss: 0.021 | (test) acc: 99.65%, loss: 0.010\n",
      "[57/80] (train) acc: 99.27%, loss: 0.022 | (test) acc: 99.64%, loss: 0.011\n",
      "[58/80] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.57%, loss: 0.012\n",
      "[59/80] (train) acc: 99.35%, loss: 0.019 | (test) acc: 99.66%, loss: 0.010\n",
      "[60/80] (train) acc: 99.36%, loss: 0.019 | (test) acc: 99.62%, loss: 0.012\n",
      "[61/80] (train) acc: 99.42%, loss: 0.018 | (test) acc: 99.65%, loss: 0.011\n",
      "[62/80] (train) acc: 99.41%, loss: 0.019 | (test) acc: 99.70%, loss: 0.011\n",
      "[63/80] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.70%, loss: 0.009\n",
      "[64/80] (train) acc: 99.40%, loss: 0.018 | (test) acc: 99.67%, loss: 0.011\n",
      "[65/80] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.64%, loss: 0.012\n",
      "[66/80] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.65%, loss: 0.011\n",
      "[67/80] (train) acc: 99.42%, loss: 0.017 | (test) acc: 99.59%, loss: 0.012\n",
      "[68/80] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.62%, loss: 0.011\n",
      "[69/80] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.60%, loss: 0.011\n",
      "[70/80] (train) acc: 99.40%, loss: 0.018 | (test) acc: 99.61%, loss: 0.011\n",
      "[71/80] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.67%, loss: 0.011\n",
      "[72/80] (train) acc: 99.43%, loss: 0.017 | (test) acc: 99.58%, loss: 0.013\n",
      "[73/80] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.59%, loss: 0.012\n",
      "[74/80] (train) acc: 99.43%, loss: 0.017 | (test) acc: 99.62%, loss: 0.012\n",
      "[75/80] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.57%, loss: 0.012\n",
      "[76/80] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.65%, loss: 0.010\n",
      "[77/80] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.55%, loss: 0.011\n",
      "[78/80] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.67%, loss: 0.011\n",
      "[79/80] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.60%, loss: 0.013\n",
      "[80/80] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.62%, loss: 0.011\n",
      "\n",
      "Train model1 ... {'lr': 0.0007, 'batch_size': 100, 'epoch_n': 100}\n",
      "[ 1/100] (train) acc: 87.51%, loss: 0.388 | (test) acc: 94.70%, loss: 0.160\n",
      "[ 2/100] (train) acc: 96.23%, loss: 0.117 | (test) acc: 98.96%, loss: 0.033\n",
      "[ 3/100] (train) acc: 97.19%, loss: 0.091 | (test) acc: 99.21%, loss: 0.022\n",
      "[ 4/100] (train) acc: 97.56%, loss: 0.078 | (test) acc: 99.31%, loss: 0.021\n",
      "[ 5/100] (train) acc: 97.84%, loss: 0.071 | (test) acc: 99.36%, loss: 0.018\n",
      "[ 6/100] (train) acc: 98.09%, loss: 0.063 | (test) acc: 99.38%, loss: 0.018\n",
      "[ 7/100] (train) acc: 98.16%, loss: 0.060 | (test) acc: 99.35%, loss: 0.018\n",
      "[ 8/100] (train) acc: 98.20%, loss: 0.056 | (test) acc: 99.36%, loss: 0.019\n",
      "[ 9/100] (train) acc: 98.37%, loss: 0.053 | (test) acc: 99.30%, loss: 0.021\n",
      "[10/100] (train) acc: 98.43%, loss: 0.049 | (test) acc: 99.48%, loss: 0.016\n",
      "[11/100] (train) acc: 98.47%, loss: 0.048 | (test) acc: 99.46%, loss: 0.017\n",
      "[12/100] (train) acc: 98.63%, loss: 0.043 | (test) acc: 99.40%, loss: 0.018\n",
      "[13/100] (train) acc: 98.55%, loss: 0.045 | (test) acc: 99.54%, loss: 0.017\n",
      "[14/100] (train) acc: 98.73%, loss: 0.041 | (test) acc: 99.44%, loss: 0.016\n",
      "[15/100] (train) acc: 98.69%, loss: 0.042 | (test) acc: 99.09%, loss: 0.030\n",
      "[16/100] (train) acc: 98.83%, loss: 0.038 | (test) acc: 99.48%, loss: 0.015\n",
      "[17/100] (train) acc: 98.81%, loss: 0.037 | (test) acc: 99.49%, loss: 0.015\n",
      "[18/100] (train) acc: 98.90%, loss: 0.035 | (test) acc: 99.52%, loss: 0.014\n",
      "[19/100] (train) acc: 98.88%, loss: 0.035 | (test) acc: 99.54%, loss: 0.014\n",
      "[20/100] (train) acc: 98.91%, loss: 0.034 | (test) acc: 99.53%, loss: 0.013\n",
      "[21/100] (train) acc: 98.94%, loss: 0.034 | (test) acc: 99.49%, loss: 0.017\n",
      "[22/100] (train) acc: 98.96%, loss: 0.033 | (test) acc: 99.54%, loss: 0.012\n",
      "[23/100] (train) acc: 99.06%, loss: 0.030 | (test) acc: 99.51%, loss: 0.017\n",
      "[24/100] (train) acc: 99.01%, loss: 0.031 | (test) acc: 99.50%, loss: 0.016\n",
      "[25/100] (train) acc: 99.10%, loss: 0.028 | (test) acc: 99.50%, loss: 0.016\n",
      "[26/100] (train) acc: 99.06%, loss: 0.030 | (test) acc: 99.48%, loss: 0.015\n",
      "[27/100] (train) acc: 99.09%, loss: 0.027 | (test) acc: 99.50%, loss: 0.014\n",
      "[28/100] (train) acc: 99.10%, loss: 0.028 | (test) acc: 99.37%, loss: 0.019\n",
      "[29/100] (train) acc: 99.12%, loss: 0.028 | (test) acc: 99.60%, loss: 0.012\n",
      "[30/100] (train) acc: 99.28%, loss: 0.024 | (test) acc: 99.47%, loss: 0.016\n",
      "[31/100] (train) acc: 99.19%, loss: 0.025 | (test) acc: 99.59%, loss: 0.013\n",
      "[32/100] (train) acc: 99.20%, loss: 0.025 | (test) acc: 99.65%, loss: 0.011\n",
      "[33/100] (train) acc: 99.14%, loss: 0.027 | (test) acc: 99.57%, loss: 0.012\n",
      "[34/100] (train) acc: 99.18%, loss: 0.025 | (test) acc: 99.61%, loss: 0.012\n",
      "[35/100] (train) acc: 99.22%, loss: 0.024 | (test) acc: 99.57%, loss: 0.012\n",
      "[36/100] (train) acc: 99.22%, loss: 0.025 | (test) acc: 99.60%, loss: 0.011\n",
      "[37/100] (train) acc: 99.24%, loss: 0.023 | (test) acc: 99.61%, loss: 0.011\n",
      "[38/100] (train) acc: 99.21%, loss: 0.024 | (test) acc: 99.38%, loss: 0.020\n",
      "[39/100] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.57%, loss: 0.012\n",
      "[40/100] (train) acc: 99.31%, loss: 0.022 | (test) acc: 99.72%, loss: 0.010\n",
      "[41/100] (train) acc: 99.23%, loss: 0.023 | (test) acc: 99.60%, loss: 0.012\n",
      "[42/100] (train) acc: 99.31%, loss: 0.023 | (test) acc: 99.66%, loss: 0.011\n",
      "[43/100] (train) acc: 99.32%, loss: 0.022 | (test) acc: 99.66%, loss: 0.010\n",
      "[44/100] (train) acc: 99.26%, loss: 0.023 | (test) acc: 99.69%, loss: 0.011\n",
      "[45/100] (train) acc: 99.34%, loss: 0.021 | (test) acc: 99.62%, loss: 0.012\n",
      "[46/100] (train) acc: 99.34%, loss: 0.021 | (test) acc: 99.62%, loss: 0.011\n",
      "[47/100] (train) acc: 99.42%, loss: 0.018 | (test) acc: 99.49%, loss: 0.016\n",
      "[48/100] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.53%, loss: 0.015\n",
      "[49/100] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.61%, loss: 0.010\n",
      "[50/100] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.56%, loss: 0.012\n",
      "[51/100] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.60%, loss: 0.012\n",
      "[52/100] (train) acc: 99.35%, loss: 0.020 | (test) acc: 99.55%, loss: 0.012\n",
      "[53/100] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.57%, loss: 0.012\n",
      "[54/100] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.70%, loss: 0.010\n",
      "[55/100] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.70%, loss: 0.010\n",
      "[56/100] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.70%, loss: 0.010\n",
      "[57/100] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.68%, loss: 0.010\n",
      "[58/100] (train) acc: 99.44%, loss: 0.019 | (test) acc: 99.58%, loss: 0.014\n",
      "[59/100] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.63%, loss: 0.010\n",
      "[60/100] (train) acc: 99.38%, loss: 0.018 | (test) acc: 99.66%, loss: 0.011\n",
      "[61/100] (train) acc: 99.37%, loss: 0.018 | (test) acc: 99.65%, loss: 0.012\n",
      "[62/100] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.64%, loss: 0.011\n",
      "[63/100] (train) acc: 99.42%, loss: 0.018 | (test) acc: 99.64%, loss: 0.009\n",
      "[64/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.64%, loss: 0.011\n",
      "[65/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.69%, loss: 0.011\n",
      "[66/100] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.73%, loss: 0.009\n",
      "[67/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.62%, loss: 0.011\n",
      "[68/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.55%, loss: 0.014\n",
      "[69/100] (train) acc: 99.39%, loss: 0.018 | (test) acc: 99.69%, loss: 0.010\n",
      "[70/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.64%, loss: 0.012\n",
      "[71/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.59%, loss: 0.011\n",
      "[72/100] (train) acc: 99.51%, loss: 0.016 | (test) acc: 99.66%, loss: 0.010\n",
      "[73/100] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.60%, loss: 0.011\n",
      "[74/100] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.67%, loss: 0.012\n",
      "[75/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.67%, loss: 0.012\n",
      "[76/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.68%, loss: 0.010\n",
      "[77/100] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.70%, loss: 0.010\n",
      "[78/100] (train) acc: 99.55%, loss: 0.015 | (test) acc: 99.63%, loss: 0.010\n",
      "[79/100] (train) acc: 99.48%, loss: 0.015 | (test) acc: 99.73%, loss: 0.009\n",
      "[80/100] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.62%, loss: 0.012\n",
      "[81/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.66%, loss: 0.010\n",
      "[82/100] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.64%, loss: 0.011\n",
      "[83/100] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.67%, loss: 0.011\n",
      "[84/100] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.67%, loss: 0.011\n",
      "[85/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.69%, loss: 0.010\n",
      "[86/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.74%, loss: 0.009\n",
      "[87/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.69%, loss: 0.009\n",
      "[88/100] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.65%, loss: 0.012\n",
      "[89/100] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.60%, loss: 0.012\n",
      "[90/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.62%, loss: 0.011\n",
      "[91/100] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.69%, loss: 0.011\n",
      "[92/100] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.62%, loss: 0.012\n",
      "[93/100] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.71%, loss: 0.011\n",
      "[94/100] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.68%, loss: 0.011\n",
      "[95/100] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.72%, loss: 0.009\n",
      "[96/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.70%, loss: 0.010\n",
      "[97/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.73%, loss: 0.009\n",
      "[98/100] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.69%, loss: 0.012\n",
      "[99/100] (train) acc: 99.62%, loss: 0.012 | (test) acc: 99.75%, loss: 0.008\n",
      "[100/100] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.68%, loss: 0.010\n",
      "\n",
      "Train model2 ... {'lr': 0.001, 'batch_size': 100, 'epoch_n': 100}\n",
      "[ 1/100] (train) acc: 88.07%, loss: 0.384 | (test) acc: 97.63%, loss: 0.069\n",
      "[ 2/100] (train) acc: 96.48%, loss: 0.112 | (test) acc: 98.94%, loss: 0.034\n",
      "[ 3/100] (train) acc: 97.19%, loss: 0.089 | (test) acc: 99.31%, loss: 0.020\n",
      "[ 4/100] (train) acc: 97.72%, loss: 0.074 | (test) acc: 99.12%, loss: 0.027\n",
      "[ 5/100] (train) acc: 97.89%, loss: 0.068 | (test) acc: 99.25%, loss: 0.025\n",
      "[ 6/100] (train) acc: 98.09%, loss: 0.062 | (test) acc: 99.17%, loss: 0.024\n",
      "[ 7/100] (train) acc: 98.26%, loss: 0.057 | (test) acc: 99.39%, loss: 0.018\n",
      "[ 8/100] (train) acc: 98.37%, loss: 0.054 | (test) acc: 99.47%, loss: 0.016\n",
      "[ 9/100] (train) acc: 98.44%, loss: 0.050 | (test) acc: 99.55%, loss: 0.015\n",
      "[10/100] (train) acc: 98.54%, loss: 0.046 | (test) acc: 99.33%, loss: 0.020\n",
      "[11/100] (train) acc: 98.62%, loss: 0.045 | (test) acc: 99.22%, loss: 0.022\n",
      "[12/100] (train) acc: 98.61%, loss: 0.044 | (test) acc: 99.18%, loss: 0.026\n",
      "[13/100] (train) acc: 98.67%, loss: 0.042 | (test) acc: 99.56%, loss: 0.013\n",
      "[14/100] (train) acc: 98.67%, loss: 0.041 | (test) acc: 99.40%, loss: 0.020\n",
      "[15/100] (train) acc: 98.82%, loss: 0.039 | (test) acc: 99.37%, loss: 0.018\n",
      "[16/100] (train) acc: 98.79%, loss: 0.038 | (test) acc: 99.51%, loss: 0.016\n",
      "[17/100] (train) acc: 98.87%, loss: 0.036 | (test) acc: 99.35%, loss: 0.019\n",
      "[18/100] (train) acc: 98.95%, loss: 0.033 | (test) acc: 99.63%, loss: 0.011\n",
      "[19/100] (train) acc: 98.92%, loss: 0.035 | (test) acc: 99.46%, loss: 0.018\n",
      "[20/100] (train) acc: 98.93%, loss: 0.033 | (test) acc: 99.61%, loss: 0.013\n",
      "[21/100] (train) acc: 99.02%, loss: 0.031 | (test) acc: 99.56%, loss: 0.012\n",
      "[22/100] (train) acc: 99.01%, loss: 0.032 | (test) acc: 99.58%, loss: 0.012\n",
      "[23/100] (train) acc: 99.11%, loss: 0.029 | (test) acc: 99.59%, loss: 0.013\n",
      "[24/100] (train) acc: 99.08%, loss: 0.029 | (test) acc: 99.57%, loss: 0.012\n",
      "[25/100] (train) acc: 99.13%, loss: 0.029 | (test) acc: 99.48%, loss: 0.017\n",
      "[26/100] (train) acc: 99.12%, loss: 0.028 | (test) acc: 99.59%, loss: 0.013\n",
      "[27/100] (train) acc: 99.08%, loss: 0.029 | (test) acc: 99.52%, loss: 0.014\n",
      "[28/100] (train) acc: 99.16%, loss: 0.027 | (test) acc: 99.47%, loss: 0.013\n",
      "[29/100] (train) acc: 99.19%, loss: 0.025 | (test) acc: 99.61%, loss: 0.012\n",
      "[30/100] (train) acc: 99.14%, loss: 0.026 | (test) acc: 99.59%, loss: 0.013\n",
      "[31/100] (train) acc: 99.12%, loss: 0.027 | (test) acc: 99.57%, loss: 0.012\n",
      "[32/100] (train) acc: 99.22%, loss: 0.023 | (test) acc: 99.59%, loss: 0.011\n",
      "[33/100] (train) acc: 99.27%, loss: 0.024 | (test) acc: 99.63%, loss: 0.012\n",
      "[34/100] (train) acc: 99.21%, loss: 0.026 | (test) acc: 99.60%, loss: 0.012\n",
      "[35/100] (train) acc: 99.27%, loss: 0.023 | (test) acc: 99.63%, loss: 0.010\n",
      "[36/100] (train) acc: 99.23%, loss: 0.024 | (test) acc: 99.63%, loss: 0.012\n",
      "[37/100] (train) acc: 99.24%, loss: 0.023 | (test) acc: 99.52%, loss: 0.013\n",
      "[38/100] (train) acc: 99.27%, loss: 0.023 | (test) acc: 99.64%, loss: 0.011\n",
      "[39/100] (train) acc: 99.27%, loss: 0.023 | (test) acc: 99.63%, loss: 0.011\n",
      "[40/100] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.59%, loss: 0.013\n",
      "[41/100] (train) acc: 99.27%, loss: 0.022 | (test) acc: 99.61%, loss: 0.011\n",
      "[42/100] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.56%, loss: 0.014\n",
      "[43/100] (train) acc: 99.27%, loss: 0.022 | (test) acc: 99.56%, loss: 0.013\n",
      "[44/100] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.58%, loss: 0.011\n",
      "[45/100] (train) acc: 99.33%, loss: 0.020 | (test) acc: 99.46%, loss: 0.014\n",
      "[46/100] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.61%, loss: 0.012\n",
      "[47/100] (train) acc: 99.39%, loss: 0.019 | (test) acc: 99.62%, loss: 0.011\n",
      "[48/100] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.62%, loss: 0.013\n",
      "[49/100] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.59%, loss: 0.011\n",
      "[50/100] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.64%, loss: 0.011\n",
      "[51/100] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.60%, loss: 0.015\n",
      "[52/100] (train) acc: 99.39%, loss: 0.019 | (test) acc: 99.59%, loss: 0.011\n",
      "[53/100] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.66%, loss: 0.010\n",
      "[54/100] (train) acc: 99.42%, loss: 0.017 | (test) acc: 99.63%, loss: 0.011\n",
      "[55/100] (train) acc: 99.42%, loss: 0.018 | (test) acc: 99.65%, loss: 0.012\n",
      "[56/100] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.55%, loss: 0.012\n",
      "[57/100] (train) acc: 99.40%, loss: 0.018 | (test) acc: 99.56%, loss: 0.014\n",
      "[58/100] (train) acc: 99.45%, loss: 0.018 | (test) acc: 99.52%, loss: 0.015\n",
      "[59/100] (train) acc: 99.45%, loss: 0.018 | (test) acc: 99.66%, loss: 0.010\n",
      "[60/100] (train) acc: 99.44%, loss: 0.019 | (test) acc: 99.66%, loss: 0.012\n",
      "[61/100] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.65%, loss: 0.010\n",
      "[62/100] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.66%, loss: 0.011\n",
      "[63/100] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.52%, loss: 0.014\n",
      "[64/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.68%, loss: 0.011\n",
      "[65/100] (train) acc: 99.43%, loss: 0.017 | (test) acc: 99.58%, loss: 0.012\n",
      "[66/100] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.62%, loss: 0.011\n",
      "[67/100] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.61%, loss: 0.011\n",
      "[68/100] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.72%, loss: 0.011\n",
      "[69/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.64%, loss: 0.011\n",
      "[70/100] (train) acc: 99.43%, loss: 0.017 | (test) acc: 99.59%, loss: 0.012\n",
      "[71/100] (train) acc: 99.51%, loss: 0.016 | (test) acc: 99.63%, loss: 0.010\n",
      "[72/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.65%, loss: 0.012\n",
      "[73/100] (train) acc: 99.40%, loss: 0.018 | (test) acc: 99.65%, loss: 0.012\n",
      "[74/100] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.69%, loss: 0.010\n",
      "[75/100] (train) acc: 99.52%, loss: 0.016 | (test) acc: 99.65%, loss: 0.011\n",
      "[76/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.59%, loss: 0.012\n",
      "[77/100] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.65%, loss: 0.011\n",
      "[78/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.66%, loss: 0.009\n",
      "[79/100] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.63%, loss: 0.011\n",
      "[80/100] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.64%, loss: 0.012\n",
      "[81/100] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.60%, loss: 0.011\n",
      "[82/100] (train) acc: 99.55%, loss: 0.015 | (test) acc: 99.66%, loss: 0.012\n",
      "[83/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.57%, loss: 0.013\n",
      "[84/100] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.58%, loss: 0.016\n",
      "[85/100] (train) acc: 99.52%, loss: 0.014 | (test) acc: 99.65%, loss: 0.012\n",
      "[86/100] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.67%, loss: 0.011\n",
      "[87/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.63%, loss: 0.011\n",
      "[88/100] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.59%, loss: 0.014\n",
      "[89/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.68%, loss: 0.009\n",
      "[90/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.71%, loss: 0.010\n",
      "[91/100] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.66%, loss: 0.009\n",
      "[92/100] (train) acc: 99.53%, loss: 0.013 | (test) acc: 99.63%, loss: 0.012\n",
      "[93/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.61%, loss: 0.010\n",
      "[94/100] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.61%, loss: 0.010\n",
      "[95/100] (train) acc: 99.52%, loss: 0.014 | (test) acc: 99.63%, loss: 0.010\n",
      "[96/100] (train) acc: 99.54%, loss: 0.013 | (test) acc: 99.67%, loss: 0.010\n",
      "[97/100] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.66%, loss: 0.010\n",
      "[98/100] (train) acc: 99.58%, loss: 0.014 | (test) acc: 99.62%, loss: 0.011\n",
      "[99/100] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.61%, loss: 0.013\n",
      "[100/100] (train) acc: 99.58%, loss: 0.012 | (test) acc: 99.71%, loss: 0.008\n",
      "\n",
      "Train model3 ... {'lr': 0.002, 'batch_size': 200, 'epoch_n': 120}\n",
      "[ 1/120] (train) acc: 87.26%, loss: 0.428 | (test) acc: 66.62%, loss: 0.898\n",
      "[ 2/120] (train) acc: 96.54%, loss: 0.112 | (test) acc: 98.57%, loss: 0.046\n",
      "[ 3/120] (train) acc: 97.39%, loss: 0.083 | (test) acc: 98.65%, loss: 0.041\n",
      "[ 4/120] (train) acc: 97.65%, loss: 0.075 | (test) acc: 99.15%, loss: 0.027\n",
      "[ 5/120] (train) acc: 98.01%, loss: 0.065 | (test) acc: 99.21%, loss: 0.021\n",
      "[ 6/120] (train) acc: 98.17%, loss: 0.060 | (test) acc: 99.31%, loss: 0.022\n",
      "[ 7/120] (train) acc: 98.28%, loss: 0.056 | (test) acc: 99.34%, loss: 0.020\n",
      "[ 8/120] (train) acc: 98.42%, loss: 0.051 | (test) acc: 99.40%, loss: 0.017\n",
      "[ 9/120] (train) acc: 98.49%, loss: 0.050 | (test) acc: 99.49%, loss: 0.016\n",
      "[10/120] (train) acc: 98.61%, loss: 0.046 | (test) acc: 99.52%, loss: 0.014\n",
      "[11/120] (train) acc: 98.49%, loss: 0.047 | (test) acc: 99.36%, loss: 0.020\n",
      "[12/120] (train) acc: 98.72%, loss: 0.042 | (test) acc: 99.42%, loss: 0.015\n",
      "[13/120] (train) acc: 98.77%, loss: 0.038 | (test) acc: 99.42%, loss: 0.019\n",
      "[14/120] (train) acc: 98.76%, loss: 0.041 | (test) acc: 99.47%, loss: 0.019\n",
      "[15/120] (train) acc: 98.86%, loss: 0.038 | (test) acc: 99.49%, loss: 0.014\n",
      "[16/120] (train) acc: 98.88%, loss: 0.035 | (test) acc: 99.59%, loss: 0.014\n",
      "[17/120] (train) acc: 98.94%, loss: 0.034 | (test) acc: 99.17%, loss: 0.026\n",
      "[18/120] (train) acc: 98.98%, loss: 0.033 | (test) acc: 99.43%, loss: 0.015\n",
      "[19/120] (train) acc: 99.03%, loss: 0.032 | (test) acc: 99.47%, loss: 0.017\n",
      "[20/120] (train) acc: 98.93%, loss: 0.033 | (test) acc: 99.56%, loss: 0.013\n",
      "[21/120] (train) acc: 99.02%, loss: 0.031 | (test) acc: 99.49%, loss: 0.017\n",
      "[22/120] (train) acc: 98.97%, loss: 0.032 | (test) acc: 99.43%, loss: 0.018\n",
      "[23/120] (train) acc: 99.09%, loss: 0.029 | (test) acc: 99.50%, loss: 0.018\n",
      "[24/120] (train) acc: 99.07%, loss: 0.029 | (test) acc: 99.55%, loss: 0.015\n",
      "[25/120] (train) acc: 99.06%, loss: 0.030 | (test) acc: 99.58%, loss: 0.013\n",
      "[26/120] (train) acc: 99.07%, loss: 0.030 | (test) acc: 99.45%, loss: 0.018\n",
      "[27/120] (train) acc: 99.14%, loss: 0.027 | (test) acc: 99.55%, loss: 0.015\n",
      "[28/120] (train) acc: 99.18%, loss: 0.027 | (test) acc: 99.48%, loss: 0.015\n",
      "[29/120] (train) acc: 99.17%, loss: 0.026 | (test) acc: 99.44%, loss: 0.016\n",
      "[30/120] (train) acc: 99.07%, loss: 0.029 | (test) acc: 99.50%, loss: 0.017\n",
      "[31/120] (train) acc: 99.17%, loss: 0.027 | (test) acc: 99.44%, loss: 0.017\n",
      "[32/120] (train) acc: 99.23%, loss: 0.025 | (test) acc: 99.58%, loss: 0.014\n",
      "[33/120] (train) acc: 99.18%, loss: 0.026 | (test) acc: 99.59%, loss: 0.014\n",
      "[34/120] (train) acc: 99.17%, loss: 0.025 | (test) acc: 99.55%, loss: 0.014\n",
      "[35/120] (train) acc: 99.24%, loss: 0.025 | (test) acc: 99.53%, loss: 0.013\n",
      "[36/120] (train) acc: 99.25%, loss: 0.023 | (test) acc: 99.60%, loss: 0.012\n",
      "[37/120] (train) acc: 99.22%, loss: 0.023 | (test) acc: 99.56%, loss: 0.014\n",
      "[38/120] (train) acc: 99.30%, loss: 0.023 | (test) acc: 99.38%, loss: 0.018\n",
      "[39/120] (train) acc: 99.26%, loss: 0.023 | (test) acc: 99.58%, loss: 0.014\n",
      "[40/120] (train) acc: 99.26%, loss: 0.025 | (test) acc: 99.63%, loss: 0.013\n",
      "[41/120] (train) acc: 99.31%, loss: 0.022 | (test) acc: 99.64%, loss: 0.010\n",
      "[42/120] (train) acc: 99.30%, loss: 0.020 | (test) acc: 99.58%, loss: 0.012\n",
      "[43/120] (train) acc: 99.30%, loss: 0.022 | (test) acc: 99.59%, loss: 0.012\n",
      "[44/120] (train) acc: 99.35%, loss: 0.021 | (test) acc: 99.65%, loss: 0.011\n",
      "[45/120] (train) acc: 99.31%, loss: 0.022 | (test) acc: 99.44%, loss: 0.017\n",
      "[46/120] (train) acc: 99.32%, loss: 0.021 | (test) acc: 99.49%, loss: 0.016\n",
      "[47/120] (train) acc: 99.29%, loss: 0.021 | (test) acc: 99.61%, loss: 0.012\n",
      "[48/120] (train) acc: 99.29%, loss: 0.023 | (test) acc: 99.59%, loss: 0.015\n",
      "[49/120] (train) acc: 99.42%, loss: 0.019 | (test) acc: 99.59%, loss: 0.011\n",
      "[50/120] (train) acc: 99.28%, loss: 0.022 | (test) acc: 99.58%, loss: 0.014\n",
      "[51/120] (train) acc: 99.35%, loss: 0.020 | (test) acc: 99.53%, loss: 0.013\n",
      "[52/120] (train) acc: 99.34%, loss: 0.020 | (test) acc: 99.58%, loss: 0.012\n",
      "[53/120] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.60%, loss: 0.012\n",
      "[54/120] (train) acc: 99.42%, loss: 0.019 | (test) acc: 99.47%, loss: 0.020\n",
      "[55/120] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.56%, loss: 0.012\n",
      "[56/120] (train) acc: 99.41%, loss: 0.019 | (test) acc: 99.60%, loss: 0.012\n",
      "[57/120] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.53%, loss: 0.016\n",
      "[58/120] (train) acc: 99.40%, loss: 0.021 | (test) acc: 99.57%, loss: 0.013\n",
      "[59/120] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.57%, loss: 0.011\n",
      "[60/120] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.60%, loss: 0.014\n",
      "[61/120] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.58%, loss: 0.013\n",
      "[62/120] (train) acc: 99.41%, loss: 0.019 | (test) acc: 99.62%, loss: 0.012\n",
      "[63/120] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.57%, loss: 0.013\n",
      "[64/120] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.50%, loss: 0.015\n",
      "[65/120] (train) acc: 99.48%, loss: 0.018 | (test) acc: 99.57%, loss: 0.014\n",
      "[66/120] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.55%, loss: 0.016\n",
      "[67/120] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.53%, loss: 0.015\n",
      "[68/120] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.58%, loss: 0.014\n",
      "[69/120] (train) acc: 99.49%, loss: 0.017 | (test) acc: 99.68%, loss: 0.010\n",
      "[70/120] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.58%, loss: 0.013\n",
      "[71/120] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.59%, loss: 0.012\n",
      "[72/120] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.65%, loss: 0.010\n",
      "[73/120] (train) acc: 99.51%, loss: 0.016 | (test) acc: 99.63%, loss: 0.011\n",
      "[74/120] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.59%, loss: 0.011\n",
      "[75/120] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.61%, loss: 0.012\n",
      "[76/120] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.58%, loss: 0.013\n",
      "[77/120] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.63%, loss: 0.011\n",
      "[78/120] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.67%, loss: 0.010\n",
      "[79/120] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.60%, loss: 0.010\n",
      "[80/120] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.69%, loss: 0.010\n",
      "[81/120] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.63%, loss: 0.009\n",
      "[82/120] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.65%, loss: 0.010\n",
      "[83/120] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.68%, loss: 0.009\n",
      "[84/120] (train) acc: 99.55%, loss: 0.016 | (test) acc: 99.64%, loss: 0.009\n",
      "[85/120] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.66%, loss: 0.011\n",
      "[86/120] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.60%, loss: 0.011\n",
      "[87/120] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.56%, loss: 0.012\n",
      "[88/120] (train) acc: 99.52%, loss: 0.014 | (test) acc: 99.58%, loss: 0.012\n",
      "[89/120] (train) acc: 99.48%, loss: 0.015 | (test) acc: 99.55%, loss: 0.013\n",
      "[90/120] (train) acc: 99.52%, loss: 0.016 | (test) acc: 99.56%, loss: 0.013\n",
      "[91/120] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.66%, loss: 0.011\n",
      "[92/120] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.62%, loss: 0.013\n",
      "[93/120] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.67%, loss: 0.010\n",
      "[94/120] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.67%, loss: 0.011\n",
      "[95/120] (train) acc: 99.53%, loss: 0.013 | (test) acc: 99.64%, loss: 0.010\n",
      "[96/120] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.68%, loss: 0.011\n",
      "[97/120] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.64%, loss: 0.011\n",
      "[98/120] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.52%, loss: 0.014\n",
      "[99/120] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.62%, loss: 0.011\n",
      "[100/120] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.66%, loss: 0.011\n",
      "[101/120] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.63%, loss: 0.010\n",
      "[102/120] (train) acc: 99.58%, loss: 0.014 | (test) acc: 99.68%, loss: 0.011\n",
      "[103/120] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.64%, loss: 0.014\n",
      "[104/120] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.68%, loss: 0.010\n",
      "[105/120] (train) acc: 99.55%, loss: 0.013 | (test) acc: 99.62%, loss: 0.011\n",
      "[106/120] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.68%, loss: 0.012\n",
      "[107/120] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.58%, loss: 0.012\n",
      "[108/120] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.65%, loss: 0.011\n",
      "[109/120] (train) acc: 99.55%, loss: 0.013 | (test) acc: 99.55%, loss: 0.013\n",
      "[110/120] (train) acc: 99.61%, loss: 0.013 | (test) acc: 99.64%, loss: 0.011\n",
      "[111/120] (train) acc: 99.62%, loss: 0.012 | (test) acc: 99.68%, loss: 0.010\n",
      "[112/120] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.60%, loss: 0.013\n",
      "[113/120] (train) acc: 99.61%, loss: 0.013 | (test) acc: 99.64%, loss: 0.011\n",
      "[114/120] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.65%, loss: 0.012\n",
      "[115/120] (train) acc: 99.60%, loss: 0.012 | (test) acc: 99.67%, loss: 0.011\n",
      "[116/120] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.66%, loss: 0.010\n",
      "[117/120] (train) acc: 99.61%, loss: 0.012 | (test) acc: 99.67%, loss: 0.011\n",
      "[118/120] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.67%, loss: 0.012\n",
      "[119/120] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.56%, loss: 0.013\n",
      "[120/120] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.63%, loss: 0.012\n",
      "\n",
      "Train model4 ... {'lr': 0.003, 'batch_size': 300, 'epoch_n': 150}\n",
      "[ 1/150] (train) acc: 76.56%, loss: 0.844 | (test) acc: 11.35%, loss: 8.766\n",
      "[ 2/150] (train) acc: 95.88%, loss: 0.131 | (test) acc: 45.88%, loss: 1.741\n",
      "[ 3/150] (train) acc: 96.99%, loss: 0.095 | (test) acc: 95.87%, loss: 0.126\n",
      "[ 4/150] (train) acc: 97.48%, loss: 0.079 | (test) acc: 98.93%, loss: 0.032\n",
      "[ 5/150] (train) acc: 97.73%, loss: 0.073 | (test) acc: 99.30%, loss: 0.021\n",
      "[ 6/150] (train) acc: 98.13%, loss: 0.062 | (test) acc: 99.45%, loss: 0.016\n",
      "[ 7/150] (train) acc: 98.21%, loss: 0.057 | (test) acc: 99.46%, loss: 0.017\n",
      "[ 8/150] (train) acc: 98.37%, loss: 0.052 | (test) acc: 99.19%, loss: 0.026\n",
      "[ 9/150] (train) acc: 98.46%, loss: 0.052 | (test) acc: 99.35%, loss: 0.021\n",
      "[10/150] (train) acc: 98.46%, loss: 0.049 | (test) acc: 99.29%, loss: 0.022\n",
      "[11/150] (train) acc: 98.69%, loss: 0.042 | (test) acc: 99.20%, loss: 0.025\n",
      "[12/150] (train) acc: 98.55%, loss: 0.046 | (test) acc: 99.19%, loss: 0.025\n",
      "[13/150] (train) acc: 98.64%, loss: 0.044 | (test) acc: 99.34%, loss: 0.019\n",
      "[14/150] (train) acc: 98.69%, loss: 0.042 | (test) acc: 99.45%, loss: 0.019\n",
      "[15/150] (train) acc: 98.73%, loss: 0.039 | (test) acc: 99.36%, loss: 0.019\n",
      "[16/150] (train) acc: 98.74%, loss: 0.041 | (test) acc: 99.51%, loss: 0.016\n",
      "[17/150] (train) acc: 98.92%, loss: 0.035 | (test) acc: 99.59%, loss: 0.013\n",
      "[18/150] (train) acc: 98.96%, loss: 0.035 | (test) acc: 99.60%, loss: 0.012\n",
      "[19/150] (train) acc: 98.91%, loss: 0.035 | (test) acc: 99.26%, loss: 0.027\n",
      "[20/150] (train) acc: 98.87%, loss: 0.036 | (test) acc: 99.35%, loss: 0.018\n",
      "[21/150] (train) acc: 99.01%, loss: 0.033 | (test) acc: 99.47%, loss: 0.017\n",
      "[22/150] (train) acc: 99.07%, loss: 0.031 | (test) acc: 99.46%, loss: 0.016\n",
      "[23/150] (train) acc: 98.95%, loss: 0.033 | (test) acc: 99.52%, loss: 0.014\n",
      "[24/150] (train) acc: 99.03%, loss: 0.031 | (test) acc: 99.39%, loss: 0.016\n",
      "[25/150] (train) acc: 99.03%, loss: 0.032 | (test) acc: 99.50%, loss: 0.016\n",
      "[26/150] (train) acc: 98.97%, loss: 0.032 | (test) acc: 99.42%, loss: 0.020\n",
      "[27/150] (train) acc: 99.12%, loss: 0.029 | (test) acc: 99.24%, loss: 0.026\n",
      "[28/150] (train) acc: 99.06%, loss: 0.029 | (test) acc: 99.39%, loss: 0.018\n",
      "[29/150] (train) acc: 99.09%, loss: 0.029 | (test) acc: 99.57%, loss: 0.014\n",
      "[30/150] (train) acc: 99.18%, loss: 0.027 | (test) acc: 99.56%, loss: 0.013\n",
      "[31/150] (train) acc: 99.06%, loss: 0.030 | (test) acc: 99.53%, loss: 0.014\n",
      "[32/150] (train) acc: 99.17%, loss: 0.026 | (test) acc: 99.44%, loss: 0.017\n",
      "[33/150] (train) acc: 99.15%, loss: 0.028 | (test) acc: 99.47%, loss: 0.017\n",
      "[34/150] (train) acc: 99.12%, loss: 0.027 | (test) acc: 99.56%, loss: 0.015\n",
      "[35/150] (train) acc: 99.17%, loss: 0.027 | (test) acc: 99.59%, loss: 0.012\n",
      "[36/150] (train) acc: 99.17%, loss: 0.026 | (test) acc: 99.52%, loss: 0.016\n",
      "[37/150] (train) acc: 99.12%, loss: 0.027 | (test) acc: 99.47%, loss: 0.017\n",
      "[38/150] (train) acc: 99.17%, loss: 0.026 | (test) acc: 99.62%, loss: 0.011\n",
      "[39/150] (train) acc: 99.19%, loss: 0.026 | (test) acc: 99.57%, loss: 0.012\n",
      "[40/150] (train) acc: 99.27%, loss: 0.023 | (test) acc: 99.54%, loss: 0.016\n",
      "[41/150] (train) acc: 99.21%, loss: 0.025 | (test) acc: 99.46%, loss: 0.015\n",
      "[42/150] (train) acc: 99.22%, loss: 0.025 | (test) acc: 99.60%, loss: 0.013\n",
      "[43/150] (train) acc: 99.27%, loss: 0.024 | (test) acc: 99.49%, loss: 0.016\n",
      "[44/150] (train) acc: 99.25%, loss: 0.023 | (test) acc: 99.54%, loss: 0.013\n",
      "[45/150] (train) acc: 99.22%, loss: 0.023 | (test) acc: 99.54%, loss: 0.014\n",
      "[46/150] (train) acc: 99.30%, loss: 0.022 | (test) acc: 99.60%, loss: 0.011\n",
      "[47/150] (train) acc: 99.26%, loss: 0.023 | (test) acc: 99.62%, loss: 0.013\n",
      "[48/150] (train) acc: 99.30%, loss: 0.023 | (test) acc: 99.62%, loss: 0.013\n",
      "[49/150] (train) acc: 99.24%, loss: 0.023 | (test) acc: 99.52%, loss: 0.017\n",
      "[50/150] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.68%, loss: 0.011\n",
      "[51/150] (train) acc: 99.28%, loss: 0.021 | (test) acc: 99.54%, loss: 0.016\n",
      "[52/150] (train) acc: 99.32%, loss: 0.021 | (test) acc: 99.59%, loss: 0.014\n",
      "[53/150] (train) acc: 99.29%, loss: 0.021 | (test) acc: 99.54%, loss: 0.013\n",
      "[54/150] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.61%, loss: 0.014\n",
      "[55/150] (train) acc: 99.35%, loss: 0.020 | (test) acc: 99.57%, loss: 0.013\n",
      "[56/150] (train) acc: 99.32%, loss: 0.021 | (test) acc: 99.61%, loss: 0.012\n",
      "[57/150] (train) acc: 99.35%, loss: 0.021 | (test) acc: 99.69%, loss: 0.013\n",
      "[58/150] (train) acc: 99.32%, loss: 0.022 | (test) acc: 99.51%, loss: 0.014\n",
      "[59/150] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.58%, loss: 0.014\n",
      "[60/150] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.53%, loss: 0.015\n",
      "[61/150] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.57%, loss: 0.014\n",
      "[62/150] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.54%, loss: 0.015\n",
      "[63/150] (train) acc: 99.43%, loss: 0.019 | (test) acc: 99.53%, loss: 0.016\n",
      "[64/150] (train) acc: 99.38%, loss: 0.020 | (test) acc: 99.65%, loss: 0.013\n",
      "[65/150] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.45%, loss: 0.018\n",
      "[66/150] (train) acc: 99.39%, loss: 0.019 | (test) acc: 99.54%, loss: 0.014\n",
      "[67/150] (train) acc: 99.38%, loss: 0.020 | (test) acc: 99.62%, loss: 0.012\n",
      "[68/150] (train) acc: 99.42%, loss: 0.018 | (test) acc: 99.62%, loss: 0.012\n",
      "[69/150] (train) acc: 99.42%, loss: 0.018 | (test) acc: 99.57%, loss: 0.013\n",
      "[70/150] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.61%, loss: 0.011\n",
      "[71/150] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.60%, loss: 0.012\n",
      "[72/150] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.58%, loss: 0.012\n",
      "[73/150] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.59%, loss: 0.015\n",
      "[74/150] (train) acc: 99.42%, loss: 0.018 | (test) acc: 99.64%, loss: 0.012\n",
      "[75/150] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.62%, loss: 0.013\n",
      "[76/150] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.61%, loss: 0.012\n",
      "[77/150] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.60%, loss: 0.013\n",
      "[78/150] (train) acc: 99.43%, loss: 0.019 | (test) acc: 99.67%, loss: 0.010\n",
      "[79/150] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.56%, loss: 0.014\n",
      "[80/150] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.61%, loss: 0.012\n",
      "[81/150] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.60%, loss: 0.011\n",
      "[82/150] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.58%, loss: 0.013\n",
      "[83/150] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.60%, loss: 0.011\n",
      "[84/150] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.67%, loss: 0.011\n",
      "[85/150] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.61%, loss: 0.012\n",
      "[86/150] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.68%, loss: 0.012\n",
      "[87/150] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.68%, loss: 0.013\n",
      "[88/150] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.52%, loss: 0.013\n",
      "[89/150] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.63%, loss: 0.013\n",
      "[90/150] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.63%, loss: 0.012\n",
      "[91/150] (train) acc: 99.48%, loss: 0.017 | (test) acc: 99.62%, loss: 0.011\n",
      "[92/150] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.64%, loss: 0.011\n",
      "[93/150] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.67%, loss: 0.014\n",
      "[94/150] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.63%, loss: 0.012\n",
      "[95/150] (train) acc: 99.53%, loss: 0.016 | (test) acc: 99.58%, loss: 0.013\n",
      "[96/150] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.62%, loss: 0.011\n",
      "[97/150] (train) acc: 99.47%, loss: 0.015 | (test) acc: 99.61%, loss: 0.012\n",
      "[98/150] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.68%, loss: 0.011\n",
      "[99/150] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.66%, loss: 0.011\n",
      "[100/150] (train) acc: 99.51%, loss: 0.014 | (test) acc: 99.63%, loss: 0.012\n",
      "[101/150] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.60%, loss: 0.013\n",
      "[102/150] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.71%, loss: 0.009\n",
      "[103/150] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.68%, loss: 0.012\n",
      "[104/150] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.64%, loss: 0.013\n",
      "[105/150] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.67%, loss: 0.010\n",
      "[106/150] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.60%, loss: 0.012\n",
      "[107/150] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.56%, loss: 0.012\n",
      "[108/150] (train) acc: 99.52%, loss: 0.014 | (test) acc: 99.67%, loss: 0.010\n",
      "[109/150] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.64%, loss: 0.011\n",
      "[110/150] (train) acc: 99.52%, loss: 0.016 | (test) acc: 99.65%, loss: 0.011\n",
      "[111/150] (train) acc: 99.54%, loss: 0.013 | (test) acc: 99.67%, loss: 0.011\n",
      "[112/150] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.66%, loss: 0.011\n",
      "[113/150] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.68%, loss: 0.012\n",
      "[114/150] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.60%, loss: 0.012\n",
      "[115/150] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.60%, loss: 0.014\n",
      "[116/150] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.62%, loss: 0.014\n",
      "[117/150] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.64%, loss: 0.012\n",
      "[118/150] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.59%, loss: 0.012\n",
      "[119/150] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.60%, loss: 0.013\n",
      "[120/150] (train) acc: 99.53%, loss: 0.016 | (test) acc: 99.69%, loss: 0.011\n",
      "[121/150] (train) acc: 99.61%, loss: 0.013 | (test) acc: 99.65%, loss: 0.013\n",
      "[122/150] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.65%, loss: 0.011\n",
      "[123/150] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.61%, loss: 0.011\n",
      "[124/150] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.66%, loss: 0.011\n",
      "[125/150] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.68%, loss: 0.012\n",
      "[126/150] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.51%, loss: 0.016\n",
      "[127/150] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.66%, loss: 0.011\n",
      "[128/150] (train) acc: 99.61%, loss: 0.013 | (test) acc: 99.71%, loss: 0.010\n",
      "[129/150] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.66%, loss: 0.009\n",
      "[130/150] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.61%, loss: 0.013\n",
      "[131/150] (train) acc: 99.59%, loss: 0.012 | (test) acc: 99.60%, loss: 0.014\n",
      "[132/150] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.69%, loss: 0.010\n",
      "[133/150] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.72%, loss: 0.009\n",
      "[134/150] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.64%, loss: 0.011\n",
      "[135/150] (train) acc: 99.58%, loss: 0.014 | (test) acc: 99.71%, loss: 0.010\n",
      "[136/150] (train) acc: 99.60%, loss: 0.013 | (test) acc: 99.66%, loss: 0.012\n",
      "[137/150] (train) acc: 99.61%, loss: 0.012 | (test) acc: 99.64%, loss: 0.011\n",
      "[138/150] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.64%, loss: 0.012\n",
      "[139/150] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.64%, loss: 0.012\n",
      "[140/150] (train) acc: 99.62%, loss: 0.012 | (test) acc: 99.70%, loss: 0.010\n",
      "[141/150] (train) acc: 99.61%, loss: 0.012 | (test) acc: 99.68%, loss: 0.010\n",
      "[142/150] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.69%, loss: 0.011\n",
      "[143/150] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.66%, loss: 0.010\n",
      "[144/150] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.64%, loss: 0.012\n",
      "[145/150] (train) acc: 99.59%, loss: 0.012 | (test) acc: 99.61%, loss: 0.016\n",
      "[146/150] (train) acc: 99.59%, loss: 0.012 | (test) acc: 99.71%, loss: 0.011\n",
      "[147/150] (train) acc: 99.62%, loss: 0.013 | (test) acc: 99.67%, loss: 0.010\n",
      "[148/150] (train) acc: 99.61%, loss: 0.013 | (test) acc: 99.70%, loss: 0.011\n",
      "[149/150] (train) acc: 99.62%, loss: 0.012 | (test) acc: 99.69%, loss: 0.011\n",
      "[150/150] (train) acc: 99.61%, loss: 0.013 | (test) acc: 99.58%, loss: 0.013\n",
      "\n",
      "last test accs for each model\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-666ead03fea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last test accs for each model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model{}: {:.2%}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mensemble_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_vote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "N = mnist.train.num_examples\n",
    "dq = collections.deque()\n",
    "datagen=AffineGenerator(mnist)\n",
    "\n",
    "lrs = [0.0003, 0.0007, 0.001, 0.002, 0.003]\n",
    "configs = [\n",
    "    {'lr': 0.0003, 'batch_size':  50, 'epoch_n':  80},\n",
    "    {'lr': 0.0007, 'batch_size': 100, 'epoch_n': 100},\n",
    "    {'lr':  0.001, 'batch_size': 100, 'epoch_n': 100},\n",
    "    {'lr':  0.002, 'batch_size': 200, 'epoch_n': 120},\n",
    "    {'lr':  0.003, 'batch_size': 300, 'epoch_n': 150}\n",
    "]\n",
    "\n",
    "pred_vote = np.zeros([mnist.test.num_examples, 10])\n",
    "for i, cfg in enumerate(configs):\n",
    "    tf.reset_default_graph()\n",
    "    model = BestModel(cfg['lr'])\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Train model{} ... {}\".format(i, cfg))\n",
    "        \n",
    "        for epoch in range(cfg['epoch_n']):\n",
    "            avg_loss = 0.\n",
    "            avg_acc = 0.\n",
    "\n",
    "            n_iter = 0\n",
    "            for batch_x, batch_y in datagen.generate(batch_size=cfg['batch_size']):\n",
    "                feed_dict = {model.X: batch_x, model.y: batch_y, model.training: True}\n",
    "                _, cur_acc, cur_loss = sess.run([model.train_op, model.accuracy, model.loss], feed_dict=feed_dict)\n",
    "                avg_acc += cur_acc\n",
    "                avg_loss += cur_loss\n",
    "                n_iter += 1\n",
    "\n",
    "            avg_acc /= n_iter\n",
    "            avg_loss /= n_iter\n",
    "\n",
    "            feed_dict = {model.X: mnist.test.images, model.y: mnist.test.labels, model.training: False}\n",
    "            test_acc, test_loss = sess.run([model.accuracy, model.loss], feed_dict=feed_dict)\n",
    "            \n",
    "            print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f}\".\n",
    "                  format(epoch+1, cfg['epoch_n'], avg_acc, avg_loss, test_acc, test_loss))\n",
    "    \n",
    "        # use last model for ensemble\n",
    "        feed_dict = {model.X: mnist.test.images, model.y: mnist.test.labels, model.training: False}\n",
    "        test_acc, test_pred = sess.run([model.accuracy, model.predict], feed_dict=feed_dict)\n",
    "        dq.append(test_acc)\n",
    "        pred_vote += one_hot(test_pred)\n",
    "        print\n",
    "\n",
    "print(\"last test accs for each model\")\n",
    "for i, acc in enumerate(dq):\n",
    "    print(\"model{}: {:.2%}\".format(i, acc))\n",
    "\n",
    "ensemble_acc = np.average(np.argmax(mnist.test.labels, axis=1) == np.argmax(pred_vote, axis=1))\n",
    "print(\"ensemble: {:.2%}\".format(ensemble_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last test accs for each model\n",
      "model0: 99.62%\n",
      "model1: 99.68%\n",
      "model2: 99.71%\n",
      "model3: 99.63%\n",
      "model4: 99.58%\n",
      "ensemble: 99.71%\n"
     ]
    }
   ],
   "source": [
    "print(\"last test accs for each model\")\n",
    "for i, acc in enumerate(dq):\n",
    "    print(\"model{}: {:.2%}\".format(i, acc))\n",
    "\n",
    "ensemble_acc = np.average(np.argmax(mnist.test.labels, axis=1) == np.argmax(pred_vote, axis=1))\n",
    "print(\"ensemble: {:.2%}\".format(ensemble_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very-Advanced: Snap-shot ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - tf 1.1",
   "language": "python",
   "name": "python2-tf1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
