{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "* 행이 걸리는 경우가 있음\n",
    "    * tfrecords 를 안만들어도 될 때 writer_io 로 열고 그냥 넘어갔음\n",
    "    * 이렇게 되면 데이터가 다 지워짐\n",
    "    * 이상태에서 실행하면 행이 걸림!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import urllib\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url, dest=None):\n",
    "    if not dest:\n",
    "        dest = 'data/' + url.split('/')[-1]\n",
    "    urllib.urlretrieve(url, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Flower example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/flower_photos\"):\n",
    "    print(\"Download flower dataset..\")\n",
    "    download_file(url)\n",
    "    print(\"Extracting dataset..\")\n",
    "    tarfile.open(\"data/flower_photos.tgz\", \"r:gz\").extractall(path=\"data/\")\n",
    "#     os.remove(\"data/flower_photos.tgz\") # 굳이..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bytes_features(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def _int64_features(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert dataset to TFRecord format..\n",
      "3306 364\n"
     ]
    }
   ],
   "source": [
    "print(\"Convert dataset to TFRecord format..\")\n",
    "\n",
    "# 귀찮으니 valid 는 빼자\n",
    "train_ratio = 0.9\n",
    "\n",
    "tfrecords_train_fn = \"data/flower_photos_train.tfrecords\"\n",
    "tfrecords_test_fn = \"data/flower_photos_test.tfrecords\"\n",
    "\n",
    "num_train = 0\n",
    "num_test = 0\n",
    "\n",
    "remake_tfrecords = False\n",
    "\n",
    "if not (tf.gfile.Exists(tfrecords_train_fn) and tf.gfile.Exists(tfrecords_test_fn) and remake_tfrecords == False):\n",
    "    train_writer = tf.python_io.TFRecordWriter(tfrecords_train_fn)\n",
    "    test_writer = tf.python_io.TFRecordWriter(tfrecords_test_fn)\n",
    "    \n",
    "    for i, label in enumerate(LABELS):\n",
    "        dir_name = os.path.join(\"data/flower_photos\", label)\n",
    "        paths = glob.glob(dir_name + \"/*.jpg\")\n",
    "        num_examples = len(paths)\n",
    "        for j, path in enumerate(paths):\n",
    "            im = scipy.misc.imread(path)\n",
    "            im = scipy.misc.imresize(im, [64, 64])\n",
    "\n",
    "            im_raw = im.tostring()\n",
    "            features = {\n",
    "                \"shape\": _int64_features(im.shape),\n",
    "                \"image\": _bytes_features([im_raw]),\n",
    "                \"label\": _int64_features([i])\n",
    "            }\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "            is_train = j < (num_examples * train_ratio)\n",
    "\n",
    "            if is_train:\n",
    "                num_train += 1\n",
    "                train_writer.write(example.SerializeToString())\n",
    "            else:\n",
    "                num_test += 1\n",
    "                test_writer.write(example.SerializeToString())\n",
    "\n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "else:\n",
    "    num_train = 3306\n",
    "    num_test = 364\n",
    "    \n",
    "# how to get num_examples from tfrecords file?\n",
    "print num_train, num_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(tfrecords_fn, batch_size=100, shuffle=False):\n",
    "    with tf.variable_scope(\"get_batch\"):\n",
    "        # make input pipeline\n",
    "        filename_queue = tf.train.string_input_producer([tfrecords_fn])\n",
    "        reader = tf.TFRecordReader()\n",
    "        key, records = reader.read(filename_queue)\n",
    "\n",
    "        features = tf.parse_single_example(\n",
    "            records,\n",
    "            features={\n",
    "        #         \"height\": tf.FixedLenFeature([], tf.int64),\n",
    "        #         \"width\": tf.FixedLenFeature([], tf.int64),\n",
    "                \"shape\": tf.FixedLenFeature([3], tf.int64),\n",
    "                \"image\": tf.FixedLenFeature([], tf.string),\n",
    "                \"label\": tf.FixedLenFeature([], tf.int64)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        image = tf.decode_raw(features[\"image\"], tf.uint8)\n",
    "        shape = tf.cast(features[\"shape\"], tf.int32)\n",
    "        label = tf.cast(features[\"label\"], tf.int32)\n",
    "\n",
    "        image = tf.reshape(image, [64, 64, 3])\n",
    "        resized_image = tf.image.resize_images(images=image, size=[64, 64])\n",
    "        resized_image = tf.cast(resized_image, tf.float32)\n",
    "        resized_image = resized_image / 127.5 - 1.0 \n",
    "#         resized_image = tf.image.per_image_standardization(resized_image)\n",
    "        # 사실 이 normalization 을 굳이 여기서 해 줄 필요는 없을 것 같기는 함\n",
    "        # 다만 TF docs 에는 fn_queue - reader - decoder - preprocessing 으로 되어 있으니 그 위치가 여기는 맞음\n",
    "\n",
    "        one_hot_label = tf.one_hot(label, depth=5)\n",
    "\n",
    "        # 여기 들어오는 resized_image 는 fixed_size 이어야 함 (same size)\n",
    "        # 생각해보니 하나의 텐서로 mini-batch 가 구성되어야 하니 당연한 것 같기는 하네.\n",
    "        # Q. FCN 같은건 그럼 어떻게 구현하지?\n",
    "        min_after_dequeue = batch_size * 10 # recommended from cs20si \n",
    "        capacity = min_after_dequeue + batch_size * 3 # recommended from tf official docs\n",
    "        params = {\n",
    "            'tensors': [resized_image, one_hot_label],\n",
    "            'batch_size': batch_size,\n",
    "            'capacity': capacity,\n",
    "            'num_threads': 1,\n",
    "            'allow_smaller_final_batch': True\n",
    "        }\n",
    "#         'min_after_dequeue': min_after_dequeue,\n",
    "        \n",
    "        if shuffle:\n",
    "            params['min_after_dequeue'] = min_after_dequeue\n",
    "            images, labels = tf.train.shuffle_batch(**params)\n",
    "        else:\n",
    "            images, labels = tf.train.batch(**params)\n",
    "        \n",
    "#             images, labels = tf.train.shuffle_batch(\n",
    "#                 [resized_image, one_hot_label],\n",
    "#                 batch_size=batch_size,\n",
    "#                 capacity=capacity,\n",
    "#                 num_threads=1,\n",
    "#                 min_after_dequeue=min_after_dequeue,\n",
    "#                 allow_smaller_final_batch=True)\n",
    "\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph 에 어차피 박아넣을거라면 training 이 placeholder 일 필요가 없음\n",
    "def build_nets(tfrecords_fn, training, batch_size=100, batch_shuffle=True):\n",
    "    with tf.variable_scope(\"build_nets\"):\n",
    "        X, y = get_batch(tfrecords_fn, batch_size=batch_size, shuffle=batch_shuffle)\n",
    "\n",
    "        net = X\n",
    "        n_filters = 64\n",
    "        bn_param = {'is_training': training, 'scale': True, 'decay': 0.99}\n",
    "        with slim.arg_scope([slim.conv2d], kernel_size=[3,3],\n",
    "                            normalizer_fn=slim.batch_norm, normalizer_params=bn_param):\n",
    "            for _ in range(4):\n",
    "                net = slim.conv2d(net, n_filters)\n",
    "                net = slim.conv2d(net, n_filters)\n",
    "                net = slim.max_pool2d(net, kernel_size=[2,2], padding='same')\n",
    "                n_filters *= 2\n",
    "\n",
    "        # l0: [64,64,3]\n",
    "        # l1: [32, 32, 64]\n",
    "        # l2: [16, 16, 128]\n",
    "        # l3: [8, 8, 256]\n",
    "        # l4: [4, 4, 512]\n",
    "\n",
    "        flat = slim.flatten(net)\n",
    "        fc = slim.fully_connected(flat, 1024, normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "        logits = slim.fully_connected(fc, 5, activation_fn=None)\n",
    "        with tf.variable_scope('softmax'):\n",
    "            prob = tf.nn.softmax(logits)\n",
    "\n",
    "        with tf.variable_scope('accuracy'):\n",
    "            correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        with tf.variable_scope('loss'):\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # must do this even with slim\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name.replace(\":\", \"_\"), var)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        return accuracy, loss, train_op, summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# batch_size 를 여기서 정하는 게 이상한가...\n",
    "accuracy, loss, train_op, summary_op = build_nets(tfrecords_train_fn, training=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.617188 1.14451\n",
      "60 0.585938 1.26716\n",
      "120 0.5 1.5387\n",
      "180 0.539062 1.20299\n",
      "240 0.617188 1.00528\n",
      "300 0.601562 0.898494\n",
      "360 0.601562 1.04775\n",
      "420 0.742188 0.836873\n",
      "480 0.726562 0.710021\n",
      "540 0.648438 0.83435\n",
      "600 0.789062 0.55791\n",
      "660 0.726562 0.754714\n",
      "720 0.828125 0.516672\n",
      "780 0.789062 0.503422\n",
      "840 0.84375 0.394094\n",
      "900 0.875 0.319184\n",
      "960 0.960938 0.186435\n",
      "1020 0.953125 0.175627\n",
      "1080 0.929688 0.186613\n",
      "1140 0.953125 0.190176\n",
      "1200 0.9375 0.136219\n",
      "1260 0.992188 0.0327191\n",
      "1320 0.96875 0.0799625\n",
      "1380 0.992188 0.0310558\n",
      "1440 0.992188 0.0161073\n",
      "1500 0.992188 0.0147804\n",
      "1560 0.992188 0.021153\n",
      "1620 1.0 0.00017167\n",
      "1680 1.0 0.000622992\n",
      "1740 1.0 0.000266943\n",
      "1799 1.0 0.000608409\n"
     ]
    }
   ],
   "source": [
    "# sess = tf.Session()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    # 에러의 이유를 알아낸 것 같다.\n",
    "    # https://stackoverflow.com/questions/37632102/tensorflow-trouble-re-opening-queues-after-restoring-a-session\n",
    "    # input-pipeline 을 사용한 경우에는 이렇게 그냥 다 저장하면 안되고 var_list 를 만들어서 저장해줘야함.\n",
    "    # => 그게문제가 아니었음 -_-\n",
    "    saver = tf.train.Saver(max_to_keep=100) # None 으로 하면 안 되네\n",
    "    summary_writer = tf.summary.FileWriter(logdir='./summary/train', graph=sess.graph, flush_secs=10)\n",
    "    \n",
    "    n_iter = 1800\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        _, cur_summary, cur_acc, cur_loss = sess.run([train_op, summary_op, accuracy, loss])\n",
    "        summary_writer.add_summary(cur_summary, global_step=i)\n",
    "        \n",
    "        if i % 60 == 0 or i == n_iter-1: # 30 번이 1에퐄\n",
    "            print i, cur_acc, cur_loss\n",
    "            saver.save(sess, 'checkpoints/flower', global_step=i)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build evaluation graph\n",
    "tf.reset_default_graph()\n",
    "accuracy, loss, train_op, summary_op = build_nets(tfrecords_test_fn, training=False, \n",
    "                                                  batch_size=num_test, batch_shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== checkpoints ===\n",
      "model_checkpoint_path: \"checkpoints/flower-1799\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-0\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-60\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-120\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-180\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-240\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-300\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-360\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-420\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-480\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-540\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-600\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-660\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-720\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-780\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-840\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-900\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-960\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1020\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1080\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1140\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1200\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1260\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1320\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1380\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1440\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1500\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1560\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1620\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1680\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1740\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1799\"\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-0\n",
      "0 0.173077 1.69978\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-60\n",
      "60 0.244505 10.9348\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-120\n",
      "120 0.304945 2.71323\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-180\n",
      "180 0.244505 6.90922\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-240\n",
      "240 0.266484 4.87386\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-300\n",
      "300 0.291209 3.97834\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-360\n",
      "360 0.505495 1.58187\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-420\n",
      "420 0.634615 0.9416\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-480\n",
      "480 0.587912 1.09895\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-540\n",
      "540 0.571429 1.35559\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-600\n",
      "600 0.598901 1.38224\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-660\n",
      "660 0.667583 0.868731\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-720\n",
      "720 0.67033 0.796161\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-780\n",
      "780 0.692308 0.879712\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-840\n",
      "840 0.64011 1.07481\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-900\n",
      "900 0.717033 0.946638\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-960\n",
      "960 0.681319 1.06754\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1020\n",
      "1020 0.706044 1.00364\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1080\n",
      "1080 0.708791 0.799581\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1140\n",
      "1140 0.675824 1.08808\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1200\n",
      "1200 0.758242 0.971912\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1260\n",
      "1260 0.692308 0.978027\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1320\n",
      "1320 0.747253 1.02035\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1380\n",
      "1380 0.686813 1.67361\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1440\n",
      "1440 0.728022 1.18755\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1500\n",
      "1500 0.728022 1.58113\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1560\n",
      "1560 0.75 1.20319\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1620\n",
      "1620 0.769231 0.892534\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1680\n",
      "1680 0.78022 0.898475\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1740\n",
      "1740 0.777473 0.935548\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1799\n",
      "1799 0.78022 0.902253\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver() \n",
    "    ckpt = tf.train.get_checkpoint_state(\"checkpoints/\")\n",
    "    print \"=== checkpoints ===\"\n",
    "    print ckpt\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    summary_writer = tf.summary.FileWriter(logdir='./summary/test', graph=sess.graph, flush_secs=10)\n",
    "    \n",
    "    for v in ckpt.all_model_checkpoint_paths:\n",
    "        saver.restore(sess, v)\n",
    "        global_step = v.split('/')[-1].split('-')[-1]\n",
    "        \n",
    "        n_iter = int(np.ceil(num_test/800.))\n",
    "\n",
    "        cur_summary, cur_acc, cur_loss = sess.run([summary_op, accuracy, loss])\n",
    "        summary_writer.add_summary(cur_summary, global_step=global_step)\n",
    "\n",
    "        print global_step, cur_acc, cur_loss\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - tf.latest",
   "language": "python",
   "name": "python2-tf-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
