{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze\n",
    "\n",
    "Objective: Enc -> Dec -> Enc 를 탄다고 했을 때, 두번째 enc 만 freeze 해서 gradient 가 안 가게 하려면?  \n",
    "크게 3가지 방법이 있음:\n",
    "\n",
    "1. Cloning: forward by copied enc\n",
    "2. Manual grad: calc grad using `torch.autograd.grad` function instead of `backward()`\n",
    "3. Inference-only freezing: Freeze enc in forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as tv_utils\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad():\n",
    "    enc.zero_grad()\n",
    "    dec.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(r):\n",
    "    return r.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_stats(model):\n",
    "    grads = []\n",
    "    for p in model.parameters():\n",
    "        grads.append(p.grad.flatten())\n",
    "    \n",
    "    g = torch.cat(grads)\n",
    "    mean = g.mean()\n",
    "    std = g.std()\n",
    "\n",
    "    return mean.item(), std.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(x):\n",
    "    \"\"\"Do not freeze second encoder\"\"\"\n",
    "    x = enc(x)\n",
    "    x = dec(x)\n",
    "    x = enc(x)\n",
    "    \n",
    "    loss = compute_loss(x)\n",
    "    loss.backward()\n",
    "\n",
    "    return grad_stats(enc), grad_stats(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloning(x):\n",
    "    \"\"\"Clone the encoder and freeze it\"\"\"\n",
    "    x = enc(x)\n",
    "    x = dec(x)\n",
    "    x = freeze_enc(x)\n",
    "    \n",
    "    loss = compute_loss(x)\n",
    "    loss.backward()\n",
    "\n",
    "    return grad_stats(enc), grad_stats(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freezing(x):\n",
    "    \"\"\"Inference-only freezing\"\"\"\n",
    "    x = enc(x)\n",
    "    x = dec(x)\n",
    "    \n",
    "    for p in enc.parameters():\n",
    "        p.requires_grad_(False)\n",
    "        \n",
    "    x = enc(x)\n",
    "    \n",
    "    for p in enc.parameters():\n",
    "        p.requires_grad_(True)\n",
    "    \n",
    "    loss = compute_loss(x)\n",
    "    loss.backward()\n",
    "    \n",
    "    return grad_stats(enc), grad_stats(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def two_step_backward(params1, loss, params2, feat, retain_graph=False):\n",
    "#     \"\"\"\n",
    "#     module := params1 + params2\n",
    "#     loss.backward(params1)  # exclude params2\n",
    "#     feat.backward(params2, feat.grad)  # feat has grads from step 1\n",
    "#     \"\"\"\n",
    "#     params1 = list(params1)\n",
    "#     params2 = list(params2)\n",
    "    \n",
    "#     grads1 = torch.autograd.grad(loss, params1 + [feat], retain_graph=retain_graph)\n",
    "#     grads1, feat_grad = grads1[:-1], grads1[-1]\n",
    "#     grads2 = torch.autograd.grad(feat, params2, feat_grad, retain_graph=retain_graph)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for p, g in zip(params1 + params2, grads1 + grads2):\n",
    "#             p.grad = g.detach()\n",
    "\n",
    "def two_step_backward(params1, feat, params2, loss, **kwargs):\n",
    "    \"\"\"torch.autograd.grad 를 이용해서 decoder 와 first encoder grad 직접 계산하여 넣어줌.\n",
    "    \n",
    "    Args:\n",
    "        params1: encoder params\n",
    "        feat: encoded features\n",
    "        params2: decoder params\n",
    "        loss: loss\n",
    "        kwargs: kwargs for backward() function: `retain_graph` and `create_graph`\n",
    "    \"\"\"\n",
    "    params1 = list(params1)\n",
    "    params2 = list(params2)\n",
    "    \n",
    "    grads2 = torch.autograd.grad(loss, [feat] + params2, **kwargs)\n",
    "    feat_grad, grads2 = grads2[0], grads2[1:]\n",
    "    grads1 = torch.autograd.grad(feat, params1, feat_grad, **kwargs)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for p, g in zip(params1 + params2, grads1 + grads2):\n",
    "            p.grad = g.detach()\n",
    "\n",
    "def grading(x):\n",
    "    feat = enc(x)\n",
    "    fake = dec(feat)\n",
    "    r = enc(fake)\n",
    "    \n",
    "    loss = compute_loss(r)\n",
    "    \n",
    "    two_step_backward(enc.parameters(), feat, dec.parameters(), loss)\n",
    "    \n",
    "    return grad_stats(enc), grad_stats(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder()\n",
    "dec = Decoder()\n",
    "\n",
    "# freeze enc for cloning method\n",
    "freeze_enc = copy.deepcopy(enc)\n",
    "for p in freeze_enc.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "x = torch.rand(B, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1.2913068532943726, 1.9068480730056763),\n",
       " (0.006920290645211935, 1.1694526672363281))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_grad()\n",
    "baseline(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.15245112776756287, 0.48006945848464966),\n",
       " (0.006920290645211935, 1.1694526672363281))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_grad()\n",
    "cloning(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.15245112776756287, 0.48006945848464966),\n",
       " (0.006920290645211935, 1.1694526672363281))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_grad()\n",
    "freezing(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.15245112776756287, 0.48006945848464966),\n",
       " (0.006920290645211935, 1.1694526672363281))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_grad()\n",
    "grading(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
