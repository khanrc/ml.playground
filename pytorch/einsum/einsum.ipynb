{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einsum\n",
    "\n",
    "Einstein summation. pytorch, numpy, tensorflow 모두 지원. pytorch 에서는 조금 부족하다는 얘기도 있던데 확인.\n",
    "\n",
    "Reference:\n",
    "\n",
    "- https://ita9naiwa.github.io/numeric%20calculation/2018/11/10/Einsum.html (KR)\n",
    "- https://rockt.github.io/2018/04/30/einsum (EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication:\n",
    "$$ \\text{Result}=\\text{einsum}(Bik,Bkj\\rightarrow Bij)$$\n",
    "\n",
    "- 기본적으론 위와 같은 형식. 즉, 겹치는 index 에 대해 연산이 이루어진다.\n",
    "\n",
    "Outer product:\n",
    "$$ \\text{Result}=\\text{einsum}(Bik,Bjk\\rightarrow Bijk)$$\n",
    "\n",
    "- 차원 수가 늘어나는 경우: outer product.\n",
    "- 위는 i,j 가 outer product 로 결합되어 ij 가 되고 Bk 는 유지된 경우.\n",
    "\n",
    "Hadamard product & Summation (=> Dot product):\n",
    "$$ \\text{Result}=\\text{einsum}(Bik,Bjk\\rightarrow Bij)$$\n",
    "\n",
    "- k 가 겹치니까 k 에 대해서 hadamard product 연산이 들어가고, k 가 없어지므로 summation 이 들어간 것.\n",
    "- 사실 matrix multiplication 과 hadamard product 가 완전히 달라 보이지만 einsum 의 관점에서 보면 그게 그거.\n",
    "  - matrix multiplication 도 그냥 서로다른 축에 대해 hadamard product 가 수행되고 summation 이 수행된것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "아래 내용을 보고 올 것\n",
    "\n",
    "```py\n",
    "torch.einsum('xhyk,bvk,bqk->bhvq', (A, B, C))\n",
    "```\n",
    "\n",
    "- LHS 에는 있지만 RHS 는 없는 xyk 는 summation 이 되어 사라짐. 다만 xy 는 제일 앞에 애만 있고, k 는 셋다 있으니까 k는 hadamard product 이후 summation.\n",
    "- 나머지 bhvq 는 남겨짐.\n",
    "    - b 가 있는 BC 서로 hadamard product 를 하고, A는 없으므로 outer product.\n",
    "    - 나머지 hvq 는 서로 하나씩만 있으니 서로 다 outer product.\n",
    "    \n",
    "- 정리해보면, 매트릭스는 각 차원이 벡터이고, 따라서 매트릭스 연산은 벡터 연산으로 분해할 수 있음.\n",
    "- 벡터 연산은 두 종류로, hadamard product 와 outer product 로 나눌 수 있다. (dot product 는 그냥 hadamard product + summation)\n",
    "    - hadamard product + summation => dot product / matrix multiplication\n",
    "        - `[A] * [A] = [A]` (hadamard product)\n",
    "        - `sum([A] * [A]) = []` (dot product)\n",
    "        - matrix multiplication 은 이 벡터 연산을 어떤 축으로 할 것인가에 대한 것일 뿐임.\n",
    "    - 연산을 할때 서로 사이즈가 다르면, outer product 가 가능. 이렇게 되면 위에서는 차원이 보존되거나 작아졌는데 여기서는 오히려 증가한다. \n",
    "        - `[A] * [B] = [A, B]` (outer product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# b=3, h=2, v=5, q=6.\n",
    "# x = y = 1, k=4.\n",
    "A = torch.rand(1, 2, 1, 4)\n",
    "B = torch.rand(3, 5, 4)\n",
    "C = torch.rand(3, 6, 4)\n",
    "r = torch.einsum('xhyk,bvk,bqk->bhvq', A, B, C)\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(3):\n",
    "    for h in range(2):\n",
    "        for v in range(5):\n",
    "            for q in range(6):\n",
    "                assert r[b, h, v, q] == (A[:, h, :, :] * B[b, v, :] * C[b, q, :]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  14,   38,   62,   86,  110],\n",
       "         [  38,  126,  214,  302,  390],\n",
       "         [  62,  214,  366,  518,  670]],\n",
       "\n",
       "        [[1166, 1382, 1598, 1814, 2030],\n",
       "         [1510, 1790, 2070, 2350, 2630],\n",
       "         [1854, 2198, 2542, 2886, 3230]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(2*3*4).reshape(2,3,4)\n",
    "B = torch.arange(2*5*4).reshape(2,5,4)\n",
    "torch.einsum(\"bik,bjk->bij\", A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_einsum(cmd, *args, print_args=True):\n",
    "    if print_args:\n",
    "        print(\"args ({}):\".format(len(args)))\n",
    "        for v in args:\n",
    "            print(v)\n",
    "        print(\"\")\n",
    "    r = torch.einsum(cmd, *args)\n",
    "    print(\"{}: {}\".format(cmd, r.shape))\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args (1):\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "ij->ji: torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Transpose\n",
    "A = torch.arange(6).reshape(2, 3)\n",
    "print_einsum(\"ij->ji\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args (1):\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "ij->: torch.Size([])\n",
      "tensor(15)\n",
      "ij->j: torch.Size([3])\n",
      "tensor([3, 5, 7])\n",
      "ij->i: torch.Size([2])\n",
      "tensor([ 3, 12])\n"
     ]
    }
   ],
   "source": [
    "# sum, column sum, row sum\n",
    "A = torch.arange(6).reshape(2, 3)\n",
    "print_einsum(\"ij->\", A)\n",
    "print_einsum(\"ij->j\", A, print_args=False)\n",
    "print_einsum(\"ij->i\", A, print_args=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args (2):\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([0, 1, 2])\n",
      "\n",
      "ij,j->i: torch.Size([2])\n",
      "tensor([ 5, 14])\n"
     ]
    }
   ],
   "source": [
    "# matrix-vector multiplication\n",
    "A = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(3)\n",
    "print_einsum(\"ij,j->i\", A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args (2):\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "\n",
      "ik,kj->ij: torch.Size([2, 5])\n",
      "tensor([[ 25,  28,  31,  34,  37],\n",
      "        [ 70,  82,  94, 106, 118]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 25,  28,  31,  34,  37],\n",
       "        [ 70,  82,  94, 106, 118]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix-matrix multiplication\n",
    "B = torch.arange(15).reshape(3, 5)\n",
    "print_einsum(\"ik,kj->ij\", A, B)\n",
    "A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args (2):\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11],\n",
      "         [12, 13],\n",
      "         [14, 15]]])\n",
      "\n",
      "ijv,ivk->ijk: torch.Size([2, 3, 2])\n",
      "tensor([[[  28,   34],\n",
      "         [  76,   98],\n",
      "         [ 124,  162]],\n",
      "\n",
      "        [[ 604,  658],\n",
      "         [ 780,  850],\n",
      "         [ 956, 1042]]])\n",
      "tensor([[[  28,   34],\n",
      "         [  76,   98],\n",
      "         [ 124,  162]],\n",
      "\n",
      "        [[ 604,  658],\n",
      "         [ 780,  850],\n",
      "         [ 956, 1042]]])\n",
      "tensor([[[  28,   34],\n",
      "         [  76,   98],\n",
      "         [ 124,  162]],\n",
      "\n",
      "        [[ 604,  658],\n",
      "         [ 780,  850],\n",
      "         [ 956, 1042]]])\n"
     ]
    }
   ],
   "source": [
    "# batch matrix-matrix multiplication\n",
    "A = torch.arange(2*3*4).reshape(2,3,4)\n",
    "B = torch.arange(2*4*2).reshape(2,4,2)\n",
    "print_einsum(\"ijv,ivk->ijk\", A, B)\n",
    "print(A @ B)\n",
    "print(torch.bmm(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args (2):\n",
      "tensor([0, 1, 2])\n",
      "tensor([3, 4, 5])\n",
      "\n",
      "i,i->: torch.Size([])\n",
      "tensor(14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector-vector dot product\n",
    "a = torch.arange(3)\n",
    "b = torch.arange(3, 6)\n",
    "print_einsum(\"i,i->\", a, b)\n",
    "(a*b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args (2):\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "\n",
      "ij,ij->: torch.Size([])\n",
      "tensor(145)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(145)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix-matrix dot product\n",
    "A = torch.arange(6).reshape(2,3)\n",
    "B = torch.arange(6, 12).reshape(2,3)\n",
    "print_einsum(\"ij,ij->\", A, B)\n",
    "(A*B).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadamard product\n",
    "\n",
    "Element-wise product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args (2):\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "\n",
      "ij,ij->ij: torch.Size([2, 3])\n",
      "tensor([[ 0,  7, 16],\n",
      "        [27, 40, 55]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  7, 16],\n",
       "        [27, 40, 55]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(2,3)\n",
    "B = torch.arange(6, 12).reshape(2,3)\n",
    "print_einsum(\"ij,ij->ij\", A, B)\n",
    "A*B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args (2):\n",
      "tensor([0, 1, 2])\n",
      "tensor([3, 4, 5, 6])\n",
      "\n",
      "i,j->ij: torch.Size([3, 4])\n",
      "tensor([[ 0,  0,  0,  0],\n",
      "        [ 3,  4,  5,  6],\n",
      "        [ 6,  8, 10, 12]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0],\n",
       "        [ 3,  4,  5,  6],\n",
       "        [ 6,  8, 10, 12]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3)\n",
    "b = torch.arange(3, 7)\n",
    "print_einsum(\"i,j->ij\", a, b)\n",
    "a.view(-1, 1) * b.view(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilinear transformation\n",
    "\n",
    "- 3개도 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "b = torch.randn(5,3,7)\n",
    "c = torch.randn(2,7)\n",
    "torch.einsum('ik,jkl,il->ij', [a, b, c]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
