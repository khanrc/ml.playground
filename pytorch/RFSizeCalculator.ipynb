{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receptive field size calculator\n",
    "\n",
    "RF 계산하는게 귀찮아서 계산기를 만듦.\n",
    "\n",
    "Method:\n",
    "\n",
    "1. Forward with `x.requires_grad = True`\n",
    "2. RF 를 구하고 싶은 featuremap (or output) 을 선택\n",
    "3. Backward from the center element of chosen featuremap\n",
    "4. Calc RF using nonzero grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rf(net, C_in, size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        net\n",
    "        C_in\n",
    "        size: input size of x\n",
    "    \"\"\"\n",
    "    # 1. Forward with x.requires_grad=True\n",
    "    x = torch.rand(1, C_in, size, size, requires_grad=True)\n",
    "    r = net(x)\n",
    "    # Skip step 2 by assuming final output as a target featuremap\n",
    "    # 3. Backward from center element\n",
    "    r[0, 0, r.size(2)//2, r.size(3)//2].backward()\n",
    "    # 4. Calc RF using nonzero grads\n",
    "    spatial_max_points = x.grad[0, 0].nonzero().max(0).values\n",
    "    spatial_min_points = x.grad[0, 0].nonzero().min(0).values\n",
    "    rf_size = spatial_max_points - spatial_min_points + 1\n",
    "    \n",
    "    # sanity check\n",
    "    rf = x[\n",
    "        0, 0, \n",
    "        spatial_min_points[0]:spatial_min_points[0]+rf_size[0], \n",
    "        spatial_min_points[1]:spatial_min_points[1]+rf_size[1]\n",
    "    ]\n",
    "    n_zero = (rf == 0.).sum()\n",
    "    assert n_zero == 0, \"RF has {} zero element\".format(n_zero)\n",
    "    \n",
    "    return rf_size.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code blocks\n",
    "\n",
    "code blocks from MaHFG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatcher(dispatch_fn):\n",
    "    def decorated(key, *args):\n",
    "        if callable(key):\n",
    "            return key\n",
    "\n",
    "        if key is None:\n",
    "            key = 'none'\n",
    "\n",
    "        return dispatch_fn(key, *args)\n",
    "    return decorated\n",
    "\n",
    "\n",
    "@dispatcher\n",
    "def norm_dispatch(norm):\n",
    "    return {\n",
    "        'none': nn.Identity,\n",
    "        'in': partial(nn.InstanceNorm2d, affine=False),  # false as default\n",
    "        'bn': nn.BatchNorm2d,\n",
    "    }[norm.lower()]\n",
    "\n",
    "\n",
    "@dispatcher\n",
    "def w_norm_dispatch(w_norm):\n",
    "    # NOTE Unlike other dispatcher, w_norm is function, not class.\n",
    "    return {\n",
    "        'none': lambda x: x\n",
    "    }[w_norm.lower()]\n",
    "\n",
    "\n",
    "@dispatcher\n",
    "def activ_dispatch(activ, norm=None):\n",
    "    return {\n",
    "        \"none\": nn.Identity,\n",
    "        \"relu\": nn.ReLU,\n",
    "        \"lrelu\": partial(nn.LeakyReLU, negative_slope=0.2)\n",
    "    }[activ.lower()]\n",
    "\n",
    "\n",
    "@dispatcher\n",
    "def pad_dispatch(pad_type):\n",
    "    return {\n",
    "        \"zero\": nn.ZeroPad2d,\n",
    "        \"replicate\": nn.ReplicationPad2d,\n",
    "        \"reflect\": nn.ReflectionPad2d\n",
    "    }[pad_type.lower()]\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    \"\"\" pre-active linear block \"\"\"\n",
    "    def __init__(self, C_in, C_out, norm='none', activ='relu', bias=True, w_norm='none',\n",
    "                 dropout=0.):\n",
    "        super().__init__()\n",
    "        activ = activ_dispatch(activ, norm)\n",
    "        if norm.lower() == 'bn':\n",
    "            norm = nn.BatchNorm1d\n",
    "        elif norm.lower() == 'frn':\n",
    "            norm = FilterResponseNorm1d\n",
    "        elif norm.lower() == 'none':\n",
    "            norm = nn.Identity\n",
    "        else:\n",
    "            raise ValueError(f\"LinearBlock supports BN only (but {norm} is given)\")\n",
    "        w_norm = w_norm_dispatch(w_norm)\n",
    "        self.norm = norm(C_in)\n",
    "        self.activ = activ()\n",
    "        if dropout > 0.:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear = w_norm(nn.Linear(C_in, C_out, bias))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.activ(x)\n",
    "        if hasattr(self, 'dropout'):\n",
    "            x = self.dropout(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\" pre-active conv block \"\"\"\n",
    "    def __init__(self, C_in, C_out, kernel_size=3, stride=1, padding=1, norm='none',\n",
    "                 activ='relu', bias=True, upsample=False, downsample=False, w_norm='none',\n",
    "                 pad_type='zero', dropout=0., size=None):\n",
    "        # 1x1 conv assertion\n",
    "        if kernel_size == 1:\n",
    "            assert padding == 0\n",
    "        super().__init__()\n",
    "        self.C_in = C_in\n",
    "        self.C_out = C_out\n",
    "\n",
    "        activ = activ_dispatch(activ, norm)\n",
    "        norm = norm_dispatch(norm)\n",
    "        w_norm = w_norm_dispatch(w_norm)\n",
    "        pad = pad_dispatch(pad_type)\n",
    "        self.upsample = upsample\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.norm = norm(C_in)\n",
    "        self.activ = activ()\n",
    "        if dropout > 0.:\n",
    "            self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.pad = pad(padding)\n",
    "        self.conv = w_norm(nn.Conv2d(C_in, C_out, kernel_size, stride, bias=bias))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.activ(x)\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=2)\n",
    "        if hasattr(self, 'dropout'):\n",
    "            x = self.dropout(x)\n",
    "        x = self.conv(self.pad(x))\n",
    "        if self.downsample:\n",
    "            x = F.avg_pool2d(x, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\" Pre-activate ResBlock with spectral normalization \"\"\"\n",
    "    def __init__(self, C_in, C_out, kernel_size=3, padding=1, upsample=False, downsample=False,\n",
    "                 norm='none', w_norm='none', activ='relu', pad_type='zero', dropout=0.,\n",
    "                 scale_var=False):\n",
    "        assert not (upsample and downsample)\n",
    "        super().__init__()\n",
    "        w_norm = w_norm_dispatch(w_norm)\n",
    "        self.C_in = C_in\n",
    "        self.C_out = C_out\n",
    "        self.upsample = upsample\n",
    "        self.downsample = downsample\n",
    "        self.scale_var = scale_var\n",
    "\n",
    "        self.conv1 = ConvBlock(C_in, C_out, kernel_size, 1, padding, norm, activ,\n",
    "                               upsample=upsample, w_norm=w_norm, pad_type=pad_type,\n",
    "                               dropout=dropout)\n",
    "        self.conv2 = ConvBlock(C_out, C_out, kernel_size, 1, padding, norm, activ,\n",
    "                               w_norm=w_norm, pad_type=pad_type, dropout=dropout)\n",
    "\n",
    "        # XXX upsample / downsample needs skip conv?\n",
    "        if C_in != C_out or upsample or downsample:\n",
    "            self.skip = w_norm(nn.Conv2d(C_in, C_out, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        normal: pre-activ + convs + skip-con\n",
    "        upsample: pre-activ + upsample + convs + skip-con\n",
    "        downsample: pre-activ + convs + downsample + skip-con\n",
    "        => pre-activ + (upsample) + convs + (downsample) + skip-con\n",
    "        \"\"\"\n",
    "        out = x\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        if self.downsample:\n",
    "            out = F.avg_pool2d(out, 2)\n",
    "\n",
    "        # skip-con\n",
    "        if hasattr(self, 'skip'):\n",
    "            if self.upsample:\n",
    "                x = F.interpolate(x, scale_factor=2)\n",
    "            x = self.skip(x)\n",
    "            if self.downsample:\n",
    "                x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        out = out + x\n",
    "        if self.scale_var:\n",
    "            out = out / np.sqrt(2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 32\n",
    "w_norm = 'none'\n",
    "activ = 'relu'\n",
    "pad_type = 'zero'\n",
    "ConvBlk = partial(ConvBlock, w_norm=w_norm, activ=activ, pad_type=pad_type)\n",
    "ResBlk = partial(ResBlock, w_norm=w_norm, activ=activ, pad_type=pad_type)\n",
    "\n",
    "feats = [\n",
    "    ConvBlk(1, C, stride=2, activ='none'), # 64x64 (stirde==2) -> 3\n",
    "    ResBlk(C*1, C*2, downsample=True),    # 32x32 -> 13\n",
    "    ResBlk(C*2, C*4, downsample=True),    # 16x16 -> 33\n",
    "    ResBlk(C*4, C*8, downsample=True),    # 8x8 -> 73\n",
    "    ResBlk(C*8, C*8, downsample=False),   # 8x8 -> 125\n",
    "    ResBlk(C*8, C*8, downsample=False),   # 8x8 -> 128\n",
    "]\n",
    "\n",
    "# feats = [\n",
    "#     ConvBlk(1, 1, stride=1, activ='none'), # 3\n",
    "#     ConvBlk(1, 1, stride=1, activ='none'), # 5\n",
    "#     ConvBlk(1, 1, stride=1, activ='none'), # 7\n",
    "#     ConvBlk(1, 1, stride=1, activ='none'), # 9\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [3, 3]\n",
      "1 [13, 13]\n",
      "2 [33, 33]\n",
      "3 [73, 73]\n",
      "4 [125, 125]\n",
      "5 [128, 128]\n"
     ]
    }
   ],
   "source": [
    "for i, _ in enumerate(feats):\n",
    "    net = nn.Sequential(*feats[:i+1])\n",
    "    print(i, calc_rf(net, 1, 128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
