{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaNet\n",
    "\n",
    "Cortes, Corinna, et al. \"Adanet: Adaptive structural learning of artificial neural networks.\" arXiv preprint arXiv:1607.01097 (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig2.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equation\n",
    "\n",
    "### eq (5)\n",
    "\n",
    "$$\n",
    "F_t(w,u)=\\frac{1}{m}\\sum^m_{i=1} \\Phi \\left( 1-y_i(f_{t-1}(x_i)+w\\cdot u(x_i) \\right) + \\Gamma_u ||w||_1\n",
    "$$\n",
    "\n",
    "- $\\Phi$: Loss function\n",
    "- $x_i$: data point\n",
    "- $u \\in {h, h'}$: 위 그림에서 두 가지 옵션 중 하나 (L, L+1).\n",
    "- $w \\in R^B$: 새로이 추가된 f weight (최종 아웃풋으로 연결되는 가중치)\n",
    "- $f_{t-1}$: 이전 타임스텝 t-1 까지 생성한 네트워크\n",
    "- $\\Gamma_u = \\lambda r_u + \\beta$\n",
    "    - $r_u$: u의 Rademacher complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eq (6)\n",
    "\n",
    "$$\n",
    "F_t(w,h)=\\frac{1}{m}\\sum^m_{i=1} \\Phi \\left( 1-y_i(f_{t-1}(x_i)+w\\cdot h(x_i) \\right) + R(w, h)\n",
    "$$\n",
    "\n",
    "- (5) 와 같지만 h 를 찾고 w 를 찾는 2-step 이 아니라 한번에 w 와 h 를 같이 찾는 방식.\n",
    "- NN 을 사용할 때에는 이렇게 end-to-end 로 하는게 나음.\n",
    "- 이 경우 R(w, h) 에는 원래의 w 에 대한 regularization term 뿐만 아니라 h 에 대한 regularization 도 새로이 들어간다:\n",
    "    - $||h_s||_p \\le \\Lambda_{k,s}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "```py\n",
    "def AdaNet():\n",
    "    for t in range(T):\n",
    "        h1, w1 = WeakLearner(f[t-1], depth=L)\n",
    "        h2, w2 = WeakLearner(f[t-1], depth=L+1)\n",
    "        h, w = ArgMin(Loss(h1, w1), Loss(h2, w2))\n",
    "\n",
    "        if Loss(f[t-1] + (h, w)) < Loss(f[t-1]):\n",
    "            f[t] = f[t-1] + (h, w)\n",
    "        else:\n",
    "            return f[t-1]\n",
    "\n",
    "    return f[T]\n",
    "```\n",
    "\n",
    "- 원래는 WeakLearner 에서 h 만 찾음. 그리고 optimize w.\n",
    "    - 이 때 여기서 증명한 learning guarantee 는 w 를 최적화 할 때 적절하게 regularization 을 걸어서 트레이닝 로스가 좋아지면 밸리데이션 로스도 항상 좋아지도록 해줌\n",
    "    - 또한 w 를 최적화하는 과정은 convex.\n",
    "- 하지만 NN 으로 오면서 end-to-end 가 더 나을 것 같다 => h 와 w 를 같이 찾음\n",
    "    - 이러면서 non-convex 가 되지만 수렴은 하므로 괜찮.\n",
    "    - 이 때 w 에 대한 regularization 뿐만 아니라 h 에 대한 regularization 도 같이 걸어주자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1.post2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubLinearNet(nn.Module):\n",
    "    \"\"\" Sub-network builder for DNN \"\"\"\n",
    "    def __init__(self, n_inputs, n_units, n_layers, n_cons):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_inputs: size of input data\n",
    "            n_units : # of hidden units\n",
    "            n_layers: # of hidden layers\n",
    "            n_cons: # of connections to embeddings\n",
    "        \"\"\"\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            if i == 0: # first layer\n",
    "                n_in = n_inputs\n",
    "            else:\n",
    "                n_in = n_units * n_cons\n",
    "            linear_block = nn.Sequential([\n",
    "                nn.Linear(n_in, n_units),\n",
    "                #nn.Dropout(p=dropout, inplace=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ])\n",
    "            self.layers.append(linear_block)\n",
    "        \n",
    "        self.layers.append(nn.Linear())\n",
    "        \n",
    "    def forward(self, x, embeds):\n",
    "        outs = [self.layers[0](x)]\n",
    "        # embeds[0].cat(self.layers[0](x))\n",
    "        for i, layer in enumerate(self.layers[1:], 1):\n",
    "            inputs = embeds[i-1]\n",
    "            out = layer(inputs)\n",
    "            # embdes[i].cat(out)\n",
    "\n",
    "#         return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [4,5,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[-2+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4\n",
      "3 5\n",
      "4 7\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(t, 2):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
