# Seq2Seq with Attention

## Run

1. Download dataset from https://download.pytorch.org/tutorial/data.zip
2. Prepare validation indices by `python data_prepare.py`
3. Run: `python train.py`

Etc:

- Hyperparams are in `train.py`.

## ToDo

- Torchtext
- Self-attention
- Transformer
- ConvS2S
- Config
- Add pre-trained word embeddings
- Other dataset

## References

- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
- https://github.com/bentrevett/pytorch-seq2seq
