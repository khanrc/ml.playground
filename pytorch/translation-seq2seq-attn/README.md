# Seq2Seq with Attention

## Run

1. Download dataset from https://download.pytorch.org/tutorial/data.zip
2. Prepare validation indices by `python dataset.py`
3. Run: `python train.py`

Etc:

- Hyperparams are in `train.py`.
- `mkdir evals` for visualizing attention.

## ToDo

- Transformer
- ConvS2S
- Other dataset
- Torchtext?

## References

- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
- https://github.com/bentrevett/pytorch-seq2seq
