{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Self-Normalizing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Papers and references\n",
    "\n",
    "* Paper:\n",
    "    * https://arxiv.org/abs/1706.02515\n",
    "* Official github: tutorials\n",
    "    * https://github.com/bioinf-jku/SNNs\n",
    "* Paper notes:\n",
    "    * https://github.com/kevinzakka/research-paper-notes/blob/master/snn.md\n",
    "* Activation visualization:\n",
    "    * https://github.com/shaohua0116/Activation-Visualization-Histogram\n",
    "* Reddits:\n",
    "    * https://www.reddit.com/r/MachineLearning/comments/6gd704/d_tutorials_and_implementations_for/\n",
    "    * https://www.reddit.com/r/MachineLearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/ (more replies)\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "* BN 을 사용하지 않고 activation function 인 elu 의 파라메터를 조정하여 자동으로 mean/variance 를 normalize 시킴\n",
    "    * 어떻게 이게 되지...?\n",
    "* FNN 의 경우 CNN/RNN 과 달리 BN 이 잘 작동하지 않는데, SELU 를 사용한 FNN 인 SNN 으로 성공적인 결과를 이끌어냄\n",
    "    * 대체로 CNN/RNN 과 달리 FNN 은 XGBoost 등 RF 류 알고리즘들에 밀려 왔는데, 여기서 FNN 의 발전 가능성을 제시함\n",
    "    * 다만, FNN 이 왜 BN 이 잘 작동하지 않는가? 간단하게 설명하는데 이해가 안 감.\n",
    "    * 또한 CNN/RNN 의 경우 그럼 BN 에 비해서 장점이 없나? 이걸 실험해보자.\n",
    "* 또한 SNN 에서의 dropout 도 제안함.\n",
    "    * 뭔가 다르니까 제안했겠지?\n",
    "\n",
    "\n",
    "### Scaled Exponential Linear Units (SELUs)\n",
    "\n",
    "![selu](selu_eq.png)\n",
    "\n",
    "* 사실 이 식에서 scale factor $\\lambda$ 를 빼면 ELUs (Exponential Linear Units) 식이다. \n",
    "* 여기에 SELU는 zero-mean and unit-variance 를 만들기 위해 `alpha = 1.6732` and `lambda = 1.0507` 를 사용.\n",
    "    * 원하는 mean/var 를 세팅하기 위해 다른 값도 지정이 가능하다. 오피셜 코드에서도 계산하는 방법을 제공.\n",
    "\n",
    "* Properties\n",
    "    * negative and positive values to control the mean\n",
    "    * derivatives approaching 0 to dampen variance\n",
    "    * slope larger than 1 to increase variance\n",
    "    * continuous curve\n",
    "    * 사실 이 프로퍼티는 잘 이해가 안 감. 논문을 더 봐야 할 듯.\n",
    "\n",
    "\n",
    "### What I want to do\n",
    "\n",
    "* 위에서 말했듯이 SNN 을 CNN/BN 구조에서 구현해보고, 이해하고, 결과를 비교해보자.\n",
    "* 또한 dropout 도 구현해보고 이해하고, 결과를 비교해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "* 논문에서 언급이 없는 것처럼, 딱히 BN 에 비해서 더 좋은 결과를 보여주지는 못하는 듯 하다.\n",
    "* BN 이 적용하기 어려운 경우 대안으로 쓸만한 정도인 듯.\n",
    "* 오피셜 리포지토리에 가보면 MNIST/CIFAR10 에 대해서 비교한 게 있는데, ELU/RELU 에 비해서 좋은 성능을 보이나, BN 과 비교는 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "* 사실 이 실험을 하면서 발견한 것 중 의아한 건 BN 을 적용하면 step 이 어느정도 차기 전까지는 성능이 안 나온다.\n",
    "* 왜...? 그럴 이유가 있나...?\n",
    "* 흠..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def selu(x):\n",
    "    with ops.name_scope('elu') as scope:\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# official tutorial 에서, \n",
    "# SNN init 은 FAN_IN / factor=1.0 / normal_dist 로 함.\n",
    "# 그게 아래.\n",
    "# MSRA (he) init 은 여기에 factor=2.0 임. relu 는 요걸로 사용함.\n",
    "def snn_init():\n",
    "    return tf.contrib.layers.variance_scaling_initializer(factor=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout 의 경우 아래 테스트 모델에서는 붙일 데가 없음.\n",
    "# 붙이면 CNN 에 붙여야 함. 붙나?\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "def dropout_selu(x, rate, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, \n",
    "                 noise_shape=None, seed=None, name=None, training=False):\n",
    "    \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "        keep_prob = 1.0 - rate\n",
    "        x = ops.convert_to_tensor(x, name=\"x\")\n",
    "        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                                             \"range (0, 1], got %g\" % keep_prob)\n",
    "        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        if tensor_util.constant_value(keep_prob) == 1:\n",
    "            return x\n",
    "\n",
    "        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "        binary_tensor = math_ops.floor(random_tensor)\n",
    "        ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "\n",
    "        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "        ret = a * ret + b\n",
    "        ret.set_shape(x.get_shape())\n",
    "        return ret\n",
    "\n",
    "    with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "        return utils.smart_cond(training,\n",
    "            lambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n",
    "            lambda: array_ops.identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, name, activ_fn, kernel_init=None, use_BN=False, lr=0.001):\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, 784], name='X')\n",
    "            self.y = tf.placeholder(tf.float32, shape=[None, 10], name='y')\n",
    "            self.training = tf.placeholder(tf.bool, name='training')\n",
    "\n",
    "            net = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "\n",
    "            n_filters = 32\n",
    "            for i in range(3):\n",
    "                with tf.variable_scope(\"conv{}\".format(i)):\n",
    "                    # conv\n",
    "                    net = tf.layers.conv2d(net, n_filters, [3,3], padding='same', kernel_initializer=kernel_init)\n",
    "                    if use_BN:\n",
    "                        net = tf.layers.batch_normalization(net, training=self.training)\n",
    "                    net = activ_fn(net)\n",
    "\n",
    "                with tf.variable_scope(\"maxpool{}\".format(i)):\n",
    "                    # max_pool\n",
    "                    net = tf.layers.max_pooling2d(net, [2,2], strides=2, padding='same')\n",
    "\n",
    "                n_filters *= 2\n",
    "                # [14, 14, 32], [7, 7, 64], [4, 4, 128]\n",
    "                # 2048\n",
    "\n",
    "            with tf.variable_scope(\"dense\"):\n",
    "                net = tf.contrib.layers.flatten(net)\n",
    "                self.logits = tf.layers.dense(net, 10)\n",
    "\n",
    "            with tf.variable_scope(\"prob\"):\n",
    "                self.prob = tf.nn.softmax(self.logits)\n",
    "\n",
    "            with tf.variable_scope(\"accuracy\"):\n",
    "                self.accuracy = tf.equal(tf.argmax(self.logits, axis=1), tf.argmax(self.y, axis=1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(self.accuracy, tf.float32))\n",
    "\n",
    "            with tf.variable_scope(\"loss\"):\n",
    "                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y)\n",
    "                self.loss = tf.reduce_mean(self.loss)\n",
    "\n",
    "            with tf.variable_scope(\"optimizer\"):\n",
    "#                 if use_BN: # 여기서 굳이 use_BN 체크를 할 필요는 없는듯. \n",
    "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n",
    "                with tf.control_dependencies(update_ops):\n",
    "                    self.train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(self.loss)\n",
    "\n",
    "            # summaries\n",
    "            # Caution: When design multiple models in a single graph,\n",
    "            # `tf.summary.merge_all` function tries merging every summaries of models.\n",
    "            self.summary_op = tf.summary.merge([\n",
    "                tf.summary.scalar(\"loss\", self.loss),\n",
    "                tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "snn = Model(\"SNN\", activ_fn=selu, use_BN=False, kernel_init=snn_init())\n",
    "relu = Model(\"ReLU\", activ_fn=tf.nn.relu, use_BN=False)\n",
    "relu_bn = Model(\"ReLU_BN\", activ_fn=tf.nn.relu, use_BN=True)\n",
    "# how about ELU?\n",
    "elu = Model(\"ELU\", activ_fn=tf.nn.elu, use_BN=False)\n",
    "elu_bn = Model(\"ELU_BN\", activ_fn=tf.nn.elu, use_BN=True)\n",
    "\n",
    "models = [snn, relu, relu_bn, elu, elu_bn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNN\n",
      "[1/10] (train) acc: 43.20%, loss: 1.816 | (test) acc: 66.48%, loss: 1.132\n",
      "[2/10] (train) acc: 74.60%, loss: 0.826 | (test) acc: 85.36%, loss: 0.539\n",
      "[3/10] (train) acc: 85.60%, loss: 0.501 | (test) acc: 90.27%, loss: 0.341\n",
      "[4/10] (train) acc: 90.00%, loss: 0.325 | (test) acc: 92.03%, loss: 0.270\n",
      "[5/10] (train) acc: 90.70%, loss: 0.287 | (test) acc: 93.93%, loss: 0.221\n",
      "[6/10] (train) acc: 92.60%, loss: 0.244 | (test) acc: 94.09%, loss: 0.196\n",
      "[7/10] (train) acc: 94.20%, loss: 0.175 | (test) acc: 95.17%, loss: 0.160\n",
      "[8/10] (train) acc: 95.40%, loss: 0.167 | (test) acc: 95.20%, loss: 0.161\n",
      "[9/10] (train) acc: 95.00%, loss: 0.168 | (test) acc: 95.45%, loss: 0.145\n",
      "[10/10] (train) acc: 95.60%, loss: 0.142 | (test) acc: 95.22%, loss: 0.151\n",
      "\n",
      "ReLU\n",
      "[1/10] (train) acc: 26.20%, loss: 2.187 | (test) acc: 45.68%, loss: 1.973\n",
      "[2/10] (train) acc: 60.50%, loss: 1.616 | (test) acc: 77.54%, loss: 1.048\n",
      "[3/10] (train) acc: 79.20%, loss: 0.779 | (test) acc: 80.83%, loss: 0.578\n",
      "[4/10] (train) acc: 83.90%, loss: 0.475 | (test) acc: 85.40%, loss: 0.460\n",
      "[5/10] (train) acc: 88.20%, loss: 0.355 | (test) acc: 90.82%, loss: 0.314\n",
      "[6/10] (train) acc: 91.50%, loss: 0.284 | (test) acc: 92.75%, loss: 0.246\n",
      "[7/10] (train) acc: 92.30%, loss: 0.319 | (test) acc: 92.87%, loss: 0.245\n",
      "[8/10] (train) acc: 93.80%, loss: 0.194 | (test) acc: 94.14%, loss: 0.199\n",
      "[9/10] (train) acc: 91.90%, loss: 0.266 | (test) acc: 95.02%, loss: 0.168\n",
      "[10/10] (train) acc: 93.80%, loss: 0.218 | (test) acc: 95.39%, loss: 0.153\n",
      "\n",
      "ReLU_BN\n",
      "[1/10] (train) acc: 46.00%, loss: 1.804 | (test) acc: 66.74%, loss: 2.151\n",
      "[2/10] (train) acc: 84.30%, loss: 0.551 | (test) acc: 53.34%, loss: 2.085\n",
      "[3/10] (train) acc: 91.00%, loss: 0.326 | (test) acc: 12.63%, loss: 2.088\n",
      "[4/10] (train) acc: 92.60%, loss: 0.250 | (test) acc: 11.37%, loss: 2.134\n",
      "[5/10] (train) acc: 94.30%, loss: 0.211 | (test) acc: 11.35%, loss: 2.165\n",
      "[6/10] (train) acc: 95.30%, loss: 0.179 | (test) acc: 11.36%, loss: 2.172\n",
      "[7/10] (train) acc: 95.50%, loss: 0.161 | (test) acc: 11.35%, loss: 2.245\n",
      "[8/10] (train) acc: 96.60%, loss: 0.144 | (test) acc: 11.35%, loss: 2.213\n",
      "[9/10] (train) acc: 96.30%, loss: 0.126 | (test) acc: 11.35%, loss: 2.323\n",
      "[10/10] (train) acc: 96.10%, loss: 0.137 | (test) acc: 11.35%, loss: 2.258\n",
      "\n",
      "ELU\n",
      "[1/10] (train) acc: 26.00%, loss: 2.114 | (test) acc: 60.80%, loss: 1.741\n",
      "[2/10] (train) acc: 68.90%, loss: 1.301 | (test) acc: 80.39%, loss: 0.747\n",
      "[3/10] (train) acc: 82.50%, loss: 0.577 | (test) acc: 86.82%, loss: 0.432\n",
      "[4/10] (train) acc: 86.10%, loss: 0.446 | (test) acc: 88.98%, loss: 0.377\n",
      "[5/10] (train) acc: 88.60%, loss: 0.361 | (test) acc: 92.62%, loss: 0.270\n",
      "[6/10] (train) acc: 92.50%, loss: 0.262 | (test) acc: 93.80%, loss: 0.225\n",
      "[7/10] (train) acc: 93.00%, loss: 0.266 | (test) acc: 93.56%, loss: 0.217\n",
      "[8/10] (train) acc: 94.70%, loss: 0.181 | (test) acc: 94.31%, loss: 0.184\n",
      "[9/10] (train) acc: 94.70%, loss: 0.177 | (test) acc: 94.76%, loss: 0.169\n",
      "[10/10] (train) acc: 95.40%, loss: 0.172 | (test) acc: 95.45%, loss: 0.157\n",
      "\n",
      "ELU_BN\n",
      "[1/10] (train) acc: 47.00%, loss: 1.703 | (test) acc: 48.05%, loss: 2.047\n",
      "[2/10] (train) acc: 79.60%, loss: 0.654 | (test) acc: 76.85%, loss: 1.872\n",
      "[3/10] (train) acc: 88.00%, loss: 0.393 | (test) acc: 45.14%, loss: 1.821\n",
      "[4/10] (train) acc: 92.90%, loss: 0.252 | (test) acc: 26.15%, loss: 1.814\n",
      "[5/10] (train) acc: 93.70%, loss: 0.219 | (test) acc: 12.67%, loss: 1.933\n",
      "[6/10] (train) acc: 93.00%, loss: 0.229 | (test) acc: 11.36%, loss: 1.974\n",
      "[7/10] (train) acc: 95.30%, loss: 0.166 | (test) acc: 11.35%, loss: 2.140\n",
      "[8/10] (train) acc: 96.40%, loss: 0.144 | (test) acc: 11.35%, loss: 2.118\n",
      "[9/10] (train) acc: 95.90%, loss: 0.134 | (test) acc: 11.35%, loss: 2.270\n",
      "[10/10] (train) acc: 95.00%, loss: 0.146 | (test) acc: 11.35%, loss: 2.202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 100\n",
    "epoch_n = 10\n",
    "N = mnist.train.num_examples\n",
    "n_iter = N // batch_size\n",
    "\n",
    "n_iter = 10 # for test\n",
    "models = [snn, relu, relu_bn, elu, elu_bn]\n",
    "# models = [snn, relu_bn, elu_bn]\n",
    "\n",
    "for model in models:\n",
    "    print model.name\n",
    "    for epoch in range(epoch_n):\n",
    "        avg_acc = 0.\n",
    "        avg_loss = 0.\n",
    "        for _ in range(n_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, cur_acc, cur_loss = sess.run([model.train_op, model.accuracy, model.loss], \n",
    "                                            {model.X: batch_x, model.y: batch_y, model.training: True})\n",
    "            avg_acc += cur_acc\n",
    "            avg_loss += cur_loss\n",
    "\n",
    "        avg_acc /= n_iter\n",
    "        avg_loss /= n_iter\n",
    "\n",
    "        test_acc, test_loss = sess.run([model.accuracy, model.loss], \n",
    "                                       {model.X: mnist.test.images, model.y: mnist.test.labels, model.training: False})\n",
    "\n",
    "        print(\"[{}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f}\".\n",
    "              format(epoch+1, epoch_n, avg_acc, avg_loss, test_acc, test_loss))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - tf 1.1",
   "language": "python",
   "name": "python2-tf1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
