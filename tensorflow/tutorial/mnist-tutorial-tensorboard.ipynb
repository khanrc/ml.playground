{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "\n",
    "Use simple model (no our best model)\n",
    "\n",
    "* Graph\n",
    "    * network scoping is important\n",
    "* Summaries\n",
    "    * scalar\n",
    "    * image\n",
    "    * text\n",
    "    * histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(0) # does not ensure perfect reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_text_matrix(dic):\n",
    "    return [[k, str(w)] for k, w in sorted(dic.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same as the slim-BN model\n",
    "def build_net(name, config):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "#         config_summary = tf.summary.text('config',  tf.convert_to_tensor(as_text_matrix(config)), collections=[])\n",
    "        with tf.variable_scope(\"input\"):\n",
    "            X = tf.placeholder(tf.float32, [None, 784], name='X')\n",
    "            y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "        training = tf.placeholder(tf.bool, name='training')\n",
    "\n",
    "        net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "        tf.summary.image('X', net)\n",
    "\n",
    "        n_filters = 32\n",
    "        bn_param = {'is_training': training, 'scale': config['bn_scale'], 'decay': config['bn_decay']}\n",
    "        with slim.arg_scope([slim.conv2d], kernel_size=[3,3],\n",
    "                            normalizer_fn=slim.batch_norm, normalizer_params=bn_param):\n",
    "            for i in range(2):\n",
    "                with tf.variable_scope('conv-block{}'.format(i)):\n",
    "                    net = slim.conv2d(net, n_filters)\n",
    "                    net = slim.max_pool2d(net, kernel_size=[2,2], padding='same')\n",
    "\n",
    "                    n_filters *= 2\n",
    "\n",
    "        with tf.variable_scope('fc'):\n",
    "            flat = slim.flatten(net)\n",
    "            logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "\n",
    "        prob = tf.nn.softmax(logits, name='prob')\n",
    "\n",
    "        with tf.variable_scope('accuracy'):\n",
    "            correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        with tf.variable_scope('loss'):\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=config['learning_rate']).minimize(loss)\n",
    "\n",
    "        tf.summary.scalar('acc', accuracy)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        # heavy operation\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        \n",
    "        return X, y, training, train_op, summary_op, accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries\n",
    "\n",
    "* Generally, official code evaluate results using checkpoints, therefore it does not need two `summary_writer` or `summary_op`.\n",
    "\n",
    "Other usages:\n",
    "\n",
    "(check)\n",
    "Two summary_op:\n",
    "```python\n",
    "tf.summary.scalar('test/acc', accuracy)\n",
    "tf.summary.scalar('test/loss', loss)\n",
    "test_summary_op = tf.summary.merge_all('test/')\n",
    "tf.summary.scalar('train/acc', accuracy)\n",
    "tf.summary.scalar('train/loss', loss)\n",
    "train_summary_op = tf.summary.merge_all('train/')\n",
    "```\n",
    "\n",
    "`summary.value`:\n",
    "```python\n",
    "precision_at_1 = count_top_1 / total_eval_count\n",
    "recall_at_5 = count_top_5 / total_eval_count\n",
    "summary = tf.Summary()\n",
    "summary.value.add(tag='eval/Accuracy@1', simple_value=precision_at_1)\n",
    "summary.value.add(tag='eval/Recall@5', simple_value=recall_at_5)\n",
    "summary_writer.add_summary(summary, global_step)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_with_config(epoch_n, batch_size, learning_rate, bn_decay, bn_scale, reset_summary=False):\n",
    "    ### CONFIG\n",
    "    # for text summary\n",
    "    name = 'tb-epoch{}-batch{}-lr{}-decay{}-scale{}'.format(epoch_n, batch_size, learning_rate, bn_decay, bn_scale)\n",
    "    config = {\n",
    "        'name': name,\n",
    "        'epoch_n': epoch_n,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'bn_decay': bn_decay,\n",
    "        'bn_scale': bn_scale\n",
    "    }\n",
    "    print('run {} ...'.format(name))\n",
    "\n",
    "    ### BUILD NET\n",
    "    # with the same network scope, you can see the graphs overlaped.\n",
    "    # with the different network scope, the graphs for each network are separated.\n",
    "    tf.reset_default_graph()\n",
    "    X, y, training, train_op, summary_op, accuracy, loss = build_net('tb-tutorial', config=config)\n",
    "\n",
    "    # As this is also op, so originally it should be in name scope of the build_net method.\n",
    "    # Set `collections=[]` to prevent entry into the GraphKeys.SUMMARIES collections (default).\n",
    "    config_summary = tf.summary.text('config', \n",
    "                                     tf.convert_to_tensor(as_text_matrix(config)), \n",
    "                                     collections=[])\n",
    "\n",
    "    ### RUN\n",
    "    if reset_summary and tf.gfile.Exists('./summary/mnist-tutorial'):\n",
    "        tf.gfile.DeleteRecursively('./summary/mnist-tutorial')\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    dir_name = './summary/mnist-tutorial/' + name\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(dir_name + '/train', graph=sess.graph, flush_secs=10)\n",
    "    test_writer = tf.summary.FileWriter(dir_name + '/test', flush_secs=10)\n",
    "\n",
    "    # warning issue: https://github.com/tensorflow/tensorboard/issues/124\n",
    "    # MetaGraph + tf.summary.text => warning (fixed in tf 1.3)\n",
    "    train_writer.add_summary(config_summary.eval(session=sess))\n",
    "    # test_writer.add_summary(config_summary.eval(session=sess)) \n",
    "    # test_writer does not work, I dont know why\n",
    "    # I think text_summary still unstable\n",
    "\n",
    "    N = mnist.train.num_examples\n",
    "    n_iter = N // batch_size\n",
    "    dq = collections.deque(maxlen=5)\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epoch_n):\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "        for _ in range(n_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, cur_acc, cur_loss, cur_summary = sess.run([train_op, accuracy, loss, summary_op], \n",
    "                                                         {X: batch_x, y: batch_y, training: True})\n",
    "            avg_acc += cur_acc\n",
    "            avg_loss += cur_loss\n",
    "\n",
    "            train_writer.add_summary(cur_summary, global_step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        avg_acc /= n_iter\n",
    "        avg_loss /= n_iter\n",
    "\n",
    "        feed_dict = {X: mnist.test.images, y: mnist.test.labels, training: False}\n",
    "        test_acc, test_loss, cur_summary = sess.run([accuracy, loss, summary_op], feed_dict=feed_dict)\n",
    "        test_writer.add_summary(cur_summary, global_step=global_step)\n",
    "\n",
    "        print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f}\".\n",
    "              format(epoch+1, epoch_n, avg_acc, avg_loss, test_acc, test_loss))\n",
    "        dq.append(test_acc)\n",
    "\n",
    "    score = np.average(dq)\n",
    "    print(\"average of last 5 test acc: {:.2%}\\n\".format(score))\n",
    "\n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run tb-epoch10-batch200-lr0.002-decay0.99-scaleTrue ...\n",
      "WARNING:tensorflow:Error encountered when serializing __tensorboard_plugin_asset__tensorboard_text.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'TextSummaryPluginAsset' object has no attribute 'name'\n",
      "[ 1/10] (train) acc: 94.32%, loss: 0.206 | (test) acc: 57.75%, loss: 1.166\n",
      "[ 2/10] (train) acc: 98.25%, loss: 0.058 | (test) acc: 96.42%, loss: 0.112\n",
      "[ 3/10] (train) acc: 98.63%, loss: 0.043 | (test) acc: 96.78%, loss: 0.101\n",
      "[ 4/10] (train) acc: 98.92%, loss: 0.034 | (test) acc: 98.24%, loss: 0.056\n",
      "[ 5/10] (train) acc: 99.04%, loss: 0.030 | (test) acc: 98.28%, loss: 0.056\n",
      "[ 6/10] (train) acc: 99.12%, loss: 0.027 | (test) acc: 98.18%, loss: 0.057\n",
      "[ 7/10] (train) acc: 99.41%, loss: 0.018 | (test) acc: 98.62%, loss: 0.048\n",
      "[ 8/10] (train) acc: 99.28%, loss: 0.021 | (test) acc: 98.67%, loss: 0.053\n",
      "[ 9/10] (train) acc: 99.51%, loss: 0.015 | (test) acc: 98.97%, loss: 0.033\n",
      "[10/10] (train) acc: 99.63%, loss: 0.011 | (test) acc: 98.66%, loss: 0.045\n",
      "average of last 5 test acc: 98.62%\n",
      "\n",
      "run tb-epoch10-batch300-lr0.003-decay0.99-scaleTrue ...\n",
      "WARNING:tensorflow:Error encountered when serializing __tensorboard_plugin_asset__tensorboard_text.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'TextSummaryPluginAsset' object has no attribute 'name'\n",
      "[ 1/10] (train) acc: 91.77%, loss: 0.334 | (test) acc: 13.87%, loss: 2.763\n",
      "[ 2/10] (train) acc: 98.14%, loss: 0.060 | (test) acc: 85.62%, loss: 0.462\n",
      "[ 3/10] (train) acc: 98.56%, loss: 0.047 | (test) acc: 95.57%, loss: 0.144\n",
      "[ 4/10] (train) acc: 98.83%, loss: 0.036 | (test) acc: 98.57%, loss: 0.047\n",
      "[ 5/10] (train) acc: 99.02%, loss: 0.030 | (test) acc: 98.49%, loss: 0.047\n",
      "[ 6/10] (train) acc: 99.23%, loss: 0.024 | (test) acc: 97.95%, loss: 0.067\n",
      "[ 7/10] (train) acc: 99.26%, loss: 0.022 | (test) acc: 98.98%, loss: 0.029\n",
      "[ 8/10] (train) acc: 99.33%, loss: 0.019 | (test) acc: 98.77%, loss: 0.036\n",
      "[ 9/10] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.00%, loss: 0.032\n",
      "[10/10] (train) acc: 99.50%, loss: 0.014 | (test) acc: 98.58%, loss: 0.051\n",
      "average of last 5 test acc: 98.66%\n",
      "\n",
      "run tb-epoch10-batch100-lr0.001-decay0.99-scaleTrue ...\n",
      "WARNING:tensorflow:Error encountered when serializing __tensorboard_plugin_asset__tensorboard_text.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'TextSummaryPluginAsset' object has no attribute 'name'\n",
      "[ 1/10] (train) acc: 95.25%, loss: 0.156 | (test) acc: 89.65%, loss: 0.328\n",
      "[ 2/10] (train) acc: 98.36%, loss: 0.053 | (test) acc: 98.59%, loss: 0.044\n",
      "[ 3/10] (train) acc: 98.68%, loss: 0.041 | (test) acc: 98.71%, loss: 0.041\n",
      "[ 4/10] (train) acc: 98.92%, loss: 0.034 | (test) acc: 98.71%, loss: 0.040\n",
      "[ 5/10] (train) acc: 99.13%, loss: 0.028 | (test) acc: 98.68%, loss: 0.043\n",
      "[ 6/10] (train) acc: 99.26%, loss: 0.023 | (test) acc: 98.44%, loss: 0.052\n",
      "[ 7/10] (train) acc: 99.41%, loss: 0.019 | (test) acc: 98.83%, loss: 0.039\n",
      "[ 8/10] (train) acc: 99.49%, loss: 0.016 | (test) acc: 98.49%, loss: 0.049\n",
      "[ 9/10] (train) acc: 99.45%, loss: 0.016 | (test) acc: 98.54%, loss: 0.049\n",
      "[10/10] (train) acc: 99.52%, loss: 0.013 | (test) acc: 98.91%, loss: 0.038\n",
      "average of last 5 test acc: 98.64%\n",
      "\n",
      "run tb-epoch10-batch100-lr0.001-decay0.999-scaleTrue ...\n",
      "WARNING:tensorflow:Error encountered when serializing __tensorboard_plugin_asset__tensorboard_text.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'TextSummaryPluginAsset' object has no attribute 'name'\n",
      "[ 1/10] (train) acc: 95.33%, loss: 0.158 | (test) acc: 17.52%, loss: 1.956\n",
      "[ 2/10] (train) acc: 98.20%, loss: 0.058 | (test) acc: 15.48%, loss: 2.144\n",
      "[ 3/10] (train) acc: 98.70%, loss: 0.040 | (test) acc: 24.03%, loss: 1.845\n",
      "[ 4/10] (train) acc: 98.94%, loss: 0.033 | (test) acc: 76.09%, loss: 0.791\n",
      "[ 5/10] (train) acc: 99.20%, loss: 0.027 | (test) acc: 85.78%, loss: 0.473\n",
      "[ 6/10] (train) acc: 99.19%, loss: 0.023 | (test) acc: 92.21%, loss: 0.250\n",
      "[ 7/10] (train) acc: 99.26%, loss: 0.022 | (test) acc: 97.45%, loss: 0.093\n",
      "[ 8/10] (train) acc: 99.43%, loss: 0.016 | (test) acc: 98.29%, loss: 0.054\n",
      "[ 9/10] (train) acc: 99.60%, loss: 0.014 | (test) acc: 98.05%, loss: 0.064\n",
      "[10/10] (train) acc: 99.64%, loss: 0.011 | (test) acc: 98.45%, loss: 0.052\n",
      "average of last 5 test acc: 96.89%\n",
      "\n",
      "run tb-epoch10-batch100-lr0.001-decay0.99-scaleFalse ...\n",
      "WARNING:tensorflow:Error encountered when serializing __tensorboard_plugin_asset__tensorboard_text.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'TextSummaryPluginAsset' object has no attribute 'name'\n",
      "[ 1/10] (train) acc: 95.07%, loss: 0.163 | (test) acc: 97.25%, loss: 0.092\n",
      "[ 2/10] (train) acc: 98.31%, loss: 0.055 | (test) acc: 97.60%, loss: 0.078\n",
      "[ 3/10] (train) acc: 98.75%, loss: 0.041 | (test) acc: 98.70%, loss: 0.039\n",
      "[ 4/10] (train) acc: 98.87%, loss: 0.035 | (test) acc: 98.24%, loss: 0.061\n",
      "[ 5/10] (train) acc: 99.02%, loss: 0.029 | (test) acc: 98.43%, loss: 0.044\n",
      "[ 6/10] (train) acc: 99.29%, loss: 0.022 | (test) acc: 98.42%, loss: 0.056\n",
      "[ 7/10] (train) acc: 99.32%, loss: 0.020 | (test) acc: 98.95%, loss: 0.033\n",
      "[ 8/10] (train) acc: 99.48%, loss: 0.016 | (test) acc: 98.91%, loss: 0.034\n",
      "[ 9/10] (train) acc: 99.49%, loss: 0.015 | (test) acc: 98.82%, loss: 0.040\n",
      "[10/10] (train) acc: 99.58%, loss: 0.013 | (test) acc: 98.97%, loss: 0.037\n",
      "average of last 5 test acc: 98.81%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_with_config(10, 200, 0.002, 0.99, True, reset_summary=True)\n",
    "run_with_config(10, 300, 0.003, 0.99, True)\n",
    "run_with_config(10, 100, 0.001, 0.99, True)\n",
    "run_with_config(10, 100, 0.001, 0.999, True)\n",
    "run_with_config(10, 100, 0.001, 0.99, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - tf.latest",
   "language": "python",
   "name": "python2-tf-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
