{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on tensorflow 1.2 (but it will probably work well on 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(0) # does not ensure perfect reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch_n=20, batch_size=100, n_iter=None, use_training_ph=False):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    N = mnist.train.num_examples\n",
    "    if n_iter is None:\n",
    "        n_iter = N // batch_size\n",
    "    # the last 5 accuracies will be used for the evaluation of a model\n",
    "    dq = collections.deque(maxlen=5)\n",
    "        \n",
    "    for epoch in range(epoch_n):\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "        for _ in range(n_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X: batch_x, y: batch_y}\n",
    "            # In some algorithms, training phase is a little difference with test phase\n",
    "            # so training flag is required.\n",
    "            if use_training_ph:\n",
    "                feed_dict[training] = True\n",
    "            _, cur_acc, cur_loss = sess.run([train_op, accuracy, loss], feed_dict=feed_dict)\n",
    "            avg_acc += cur_acc\n",
    "            avg_loss += cur_loss\n",
    "\n",
    "        avg_acc /= n_iter\n",
    "        avg_loss /= n_iter\n",
    "\n",
    "        # test acc/loss does not depend on batch size\n",
    "        # so we can use the large batch size.\n",
    "        test_acc = 0.\n",
    "        test_loss = 0.\n",
    "        for _ in range(mnist.test.num_examples // 1000):\n",
    "            batch_x, batch_y = mnist.test.next_batch(1000)\n",
    "            feed_dict = {X: batch_x, y: batch_y}\n",
    "            if use_training_ph:\n",
    "                feed_dict[training] = False\n",
    "            cur_acc, cur_loss = sess.run([accuracy, loss], feed_dict=feed_dict)\n",
    "            test_acc += cur_acc\n",
    "            test_loss += cur_loss\n",
    "        test_acc /= (mnist.test.num_examples // 1000)\n",
    "        test_loss /= (mnist.test.num_examples // 1000)\n",
    "        \n",
    "        print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f}\".\n",
    "              format(epoch+1, epoch_n, avg_acc, avg_loss, test_acc, test_loss))\n",
    "        dq.append(test_acc)\n",
    "    \n",
    "    score = np.average(dq)\n",
    "    print(\"average of last 5 test acc: {:.2%}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-hidden layer percentron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w1 = tf.get_variable('w1', [784, 256], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.1))\n",
    "b1 = tf.get_variable('b1', [256], initializer=tf.zeros_initializer)\n",
    "w2 = tf.get_variable('w2', [256, 10], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.1))\n",
    "b2 = tf.get_variable('b2', [10], initializer=tf.zeros_initializer)\n",
    "\n",
    "h1 = tf.nn.sigmoid(tf.matmul(X, w1) + b1)\n",
    "logits = tf.matmul(h1, w2) + b2\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/20] (train) acc: 81.04%, loss: 0.775 | (test) acc: 88.77%, loss: 0.433\n",
      "[ 2/20] (train) acc: 88.86%, loss: 0.400 | (test) acc: 90.35%, loss: 0.347\n",
      "[ 3/20] (train) acc: 90.15%, loss: 0.346 | (test) acc: 91.12%, loss: 0.314\n",
      "[ 4/20] (train) acc: 90.77%, loss: 0.319 | (test) acc: 91.49%, loss: 0.295\n",
      "[ 5/20] (train) acc: 91.33%, loss: 0.301 | (test) acc: 91.78%, loss: 0.281\n",
      "[ 6/20] (train) acc: 91.70%, loss: 0.288 | (test) acc: 92.12%, loss: 0.271\n",
      "[ 7/20] (train) acc: 92.05%, loss: 0.276 | (test) acc: 92.25%, loss: 0.263\n",
      "[ 8/20] (train) acc: 92.36%, loss: 0.265 | (test) acc: 92.64%, loss: 0.253\n",
      "[ 9/20] (train) acc: 92.67%, loss: 0.255 | (test) acc: 92.83%, loss: 0.245\n",
      "[10/20] (train) acc: 92.99%, loss: 0.245 | (test) acc: 93.23%, loss: 0.240\n",
      "[11/20] (train) acc: 93.25%, loss: 0.236 | (test) acc: 93.28%, loss: 0.228\n",
      "[12/20] (train) acc: 93.49%, loss: 0.228 | (test) acc: 93.35%, loss: 0.225\n",
      "[13/20] (train) acc: 93.75%, loss: 0.220 | (test) acc: 93.81%, loss: 0.216\n",
      "[14/20] (train) acc: 93.92%, loss: 0.212 | (test) acc: 94.05%, loss: 0.207\n",
      "[15/20] (train) acc: 94.19%, loss: 0.205 | (test) acc: 94.24%, loss: 0.202\n",
      "[16/20] (train) acc: 94.42%, loss: 0.198 | (test) acc: 94.32%, loss: 0.197\n",
      "[17/20] (train) acc: 94.53%, loss: 0.192 | (test) acc: 94.51%, loss: 0.191\n",
      "[18/20] (train) acc: 94.74%, loss: 0.185 | (test) acc: 94.54%, loss: 0.185\n",
      "[19/20] (train) acc: 94.92%, loss: 0.179 | (test) acc: 94.79%, loss: 0.180\n",
      "[20/20] (train) acc: 95.02%, loss: 0.174 | (test) acc: 94.93%, loss: 0.175\n",
      "average of last 5 test acc: 94.62%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\n",
    "\n",
    "TF-Slim is a lightweight library for defining, training and evaluating complex model in TensorFlow. Components of tf-slim can be freely mixed with native tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same as `import tensorflow.contrib.slim as slim`\n",
    "slim = tf.contrib.slim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "h1 = slim.fully_connected(X, 256, activation_fn=tf.nn.sigmoid, \n",
    "                          weights_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.1), \n",
    "                          biases_initializer=tf.zeros_initializer())\n",
    "logits = slim.fully_connected(h1, 10, activation_fn=None, \n",
    "                              weights_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.1), \n",
    "                              biases_initializer=tf.zeros_initializer())\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/20] (train) acc: 80.48%, loss: 0.781 | (test) acc: 88.69%, loss: 0.426\n",
      "[ 2/20] (train) acc: 88.90%, loss: 0.397 | (test) acc: 90.14%, loss: 0.346\n",
      "[ 3/20] (train) acc: 90.16%, loss: 0.344 | (test) acc: 91.11%, loss: 0.312\n",
      "[ 4/20] (train) acc: 90.81%, loss: 0.317 | (test) acc: 91.66%, loss: 0.293\n",
      "[ 5/20] (train) acc: 91.39%, loss: 0.300 | (test) acc: 91.85%, loss: 0.279\n",
      "[ 6/20] (train) acc: 91.79%, loss: 0.286 | (test) acc: 92.25%, loss: 0.267\n",
      "[ 7/20] (train) acc: 92.08%, loss: 0.274 | (test) acc: 92.62%, loss: 0.260\n",
      "[ 8/20] (train) acc: 92.43%, loss: 0.264 | (test) acc: 92.75%, loss: 0.254\n",
      "[ 9/20] (train) acc: 92.76%, loss: 0.254 | (test) acc: 93.09%, loss: 0.245\n",
      "[10/20] (train) acc: 92.97%, loss: 0.245 | (test) acc: 93.12%, loss: 0.234\n",
      "[11/20] (train) acc: 93.23%, loss: 0.236 | (test) acc: 93.48%, loss: 0.228\n",
      "[12/20] (train) acc: 93.48%, loss: 0.228 | (test) acc: 93.57%, loss: 0.223\n",
      "[13/20] (train) acc: 93.74%, loss: 0.220 | (test) acc: 93.89%, loss: 0.214\n",
      "[14/20] (train) acc: 93.95%, loss: 0.213 | (test) acc: 93.90%, loss: 0.217\n",
      "[15/20] (train) acc: 94.17%, loss: 0.205 | (test) acc: 94.17%, loss: 0.202\n",
      "[16/20] (train) acc: 94.36%, loss: 0.199 | (test) acc: 94.26%, loss: 0.195\n",
      "[17/20] (train) acc: 94.53%, loss: 0.193 | (test) acc: 94.46%, loss: 0.190\n",
      "[18/20] (train) acc: 94.71%, loss: 0.186 | (test) acc: 94.60%, loss: 0.186\n",
      "[19/20] (train) acc: 94.87%, loss: 0.181 | (test) acc: 94.74%, loss: 0.179\n",
      "[20/20] (train) acc: 95.01%, loss: 0.175 | (test) acc: 94.91%, loss: 0.175\n",
      "average of last 5 test acc: 94.59%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better MLP - ReLU, xavier_init, adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nair, Vinod, and Geoffrey E. Hinton. \"Rectified linear units improve restricted boltzmann machines.\" Proceedings of the 27th international conference on machine learning (ICML-10). 2010.\n",
    "* Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. 2010.\n",
    "* Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic optimization.\" arXiv preprint arXiv:1412.6980 (2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "h1 = slim.fully_connected(X, 256, activation_fn=tf.nn.relu, weights_initializer=slim.xavier_initializer())\n",
    "logits = slim.fully_connected(h1, 10, activation_fn=None, weights_initializer=slim.xavier_initializer())\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/20] (train) acc: 91.44%, loss: 0.306 | (test) acc: 95.38%, loss: 0.156\n",
      "[ 2/20] (train) acc: 96.25%, loss: 0.130 | (test) acc: 96.38%, loss: 0.120\n",
      "[ 3/20] (train) acc: 97.44%, loss: 0.088 | (test) acc: 97.30%, loss: 0.090\n",
      "[ 4/20] (train) acc: 98.11%, loss: 0.064 | (test) acc: 97.43%, loss: 0.080\n",
      "[ 5/20] (train) acc: 98.59%, loss: 0.049 | (test) acc: 97.56%, loss: 0.074\n",
      "[ 6/20] (train) acc: 98.94%, loss: 0.038 | (test) acc: 97.78%, loss: 0.069\n",
      "[ 7/20] (train) acc: 99.19%, loss: 0.029 | (test) acc: 98.06%, loss: 0.064\n",
      "[ 8/20] (train) acc: 99.38%, loss: 0.023 | (test) acc: 97.88%, loss: 0.067\n",
      "[ 9/20] (train) acc: 99.48%, loss: 0.019 | (test) acc: 97.87%, loss: 0.064\n",
      "[10/20] (train) acc: 99.61%, loss: 0.015 | (test) acc: 97.72%, loss: 0.074\n",
      "[11/20] (train) acc: 99.72%, loss: 0.012 | (test) acc: 98.14%, loss: 0.064\n",
      "[12/20] (train) acc: 99.85%, loss: 0.008 | (test) acc: 97.99%, loss: 0.071\n",
      "[13/20] (train) acc: 99.78%, loss: 0.009 | (test) acc: 98.05%, loss: 0.071\n",
      "[14/20] (train) acc: 99.86%, loss: 0.007 | (test) acc: 98.03%, loss: 0.071\n",
      "[15/20] (train) acc: 99.91%, loss: 0.005 | (test) acc: 97.98%, loss: 0.072\n",
      "[16/20] (train) acc: 99.94%, loss: 0.003 | (test) acc: 98.00%, loss: 0.076\n",
      "[17/20] (train) acc: 99.81%, loss: 0.007 | (test) acc: 97.77%, loss: 0.080\n",
      "[18/20] (train) acc: 99.80%, loss: 0.007 | (test) acc: 98.09%, loss: 0.070\n",
      "[19/20] (train) acc: 99.99%, loss: 0.002 | (test) acc: 98.01%, loss: 0.087\n",
      "[20/20] (train) acc: 99.98%, loss: 0.002 | (test) acc: 98.11%, loss: 0.072\n",
      "average of last 5 test acc: 98.00%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# slim.fully_connected use relu and xavier_init as default\n",
    "h1 = slim.fully_connected(X, 256)\n",
    "h2 = slim.fully_connected(h1, 128)\n",
    "logits = slim.fully_connected(h2, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/20] (train) acc: 92.23%, loss: 0.272 | (test) acc: 96.31%, loss: 0.122\n",
      "[ 2/20] (train) acc: 97.11%, loss: 0.099 | (test) acc: 97.19%, loss: 0.091\n",
      "[ 3/20] (train) acc: 97.96%, loss: 0.066 | (test) acc: 97.74%, loss: 0.076\n",
      "[ 4/20] (train) acc: 98.58%, loss: 0.046 | (test) acc: 97.98%, loss: 0.068\n",
      "[ 5/20] (train) acc: 98.86%, loss: 0.035 | (test) acc: 97.65%, loss: 0.081\n",
      "[ 6/20] (train) acc: 99.18%, loss: 0.026 | (test) acc: 97.54%, loss: 0.082\n",
      "[ 7/20] (train) acc: 99.31%, loss: 0.022 | (test) acc: 97.87%, loss: 0.079\n",
      "[ 8/20] (train) acc: 99.49%, loss: 0.017 | (test) acc: 97.48%, loss: 0.093\n",
      "[ 9/20] (train) acc: 99.52%, loss: 0.015 | (test) acc: 98.01%, loss: 0.076\n",
      "[10/20] (train) acc: 99.51%, loss: 0.014 | (test) acc: 97.78%, loss: 0.082\n",
      "[11/20] (train) acc: 99.69%, loss: 0.010 | (test) acc: 97.71%, loss: 0.095\n",
      "[12/20] (train) acc: 99.67%, loss: 0.010 | (test) acc: 97.84%, loss: 0.089\n",
      "[13/20] (train) acc: 99.50%, loss: 0.014 | (test) acc: 97.86%, loss: 0.094\n",
      "[14/20] (train) acc: 99.65%, loss: 0.010 | (test) acc: 98.32%, loss: 0.075\n",
      "[15/20] (train) acc: 99.82%, loss: 0.006 | (test) acc: 98.02%, loss: 0.088\n",
      "[16/20] (train) acc: 99.59%, loss: 0.012 | (test) acc: 97.91%, loss: 0.101\n",
      "[17/20] (train) acc: 99.77%, loss: 0.007 | (test) acc: 97.97%, loss: 0.104\n",
      "[18/20] (train) acc: 99.80%, loss: 0.007 | (test) acc: 97.98%, loss: 0.106\n",
      "[19/20] (train) acc: 99.73%, loss: 0.008 | (test) acc: 98.09%, loss: 0.096\n",
      "[20/20] (train) acc: 99.73%, loss: 0.008 | (test) acc: 98.02%, loss: 0.094\n",
      "average of last 5 test acc: 97.99%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No better acc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN)\n",
    "\n",
    "* [`conv`, `maxpool`] * 2 + `fc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "# slim.conv2d uses relu and xavier init as default\n",
    "h1 = slim.conv2d(x_img, 32, kernel_size=[3,3]) \n",
    "p1 = slim.max_pool2d(h1, kernel_size=[2,2])\n",
    "# checking dimensions for each layer is very helpful\n",
    "assert p1.shape[1:] == [14, 14, 32]\n",
    "\n",
    "h2 = slim.conv2d(p1, 64, kernel_size=[3,3])\n",
    "p2 = slim.max_pool2d(h2, kernel_size=[2,2])\n",
    "assert p2.shape[1:] == [7, 7, 64]\n",
    "\n",
    "flat = slim.flatten(p2)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/20] (train) acc: 92.74%, loss: 0.244 | (test) acc: 97.86%, loss: 0.060\n",
      "[ 2/20] (train) acc: 97.98%, loss: 0.066 | (test) acc: 98.42%, loss: 0.048\n",
      "[ 3/20] (train) acc: 98.48%, loss: 0.050 | (test) acc: 98.65%, loss: 0.042\n",
      "[ 4/20] (train) acc: 98.76%, loss: 0.040 | (test) acc: 98.76%, loss: 0.038\n",
      "[ 5/20] (train) acc: 98.95%, loss: 0.034 | (test) acc: 98.76%, loss: 0.038\n",
      "[ 6/20] (train) acc: 99.09%, loss: 0.028 | (test) acc: 98.69%, loss: 0.037\n",
      "[ 7/20] (train) acc: 99.26%, loss: 0.024 | (test) acc: 98.74%, loss: 0.035\n",
      "[ 8/20] (train) acc: 99.34%, loss: 0.021 | (test) acc: 98.85%, loss: 0.034\n",
      "[ 9/20] (train) acc: 99.42%, loss: 0.018 | (test) acc: 98.71%, loss: 0.037\n",
      "[10/20] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.01%, loss: 0.036\n",
      "[11/20] (train) acc: 99.63%, loss: 0.012 | (test) acc: 98.95%, loss: 0.034\n",
      "[12/20] (train) acc: 99.64%, loss: 0.011 | (test) acc: 98.93%, loss: 0.035\n",
      "[13/20] (train) acc: 99.65%, loss: 0.011 | (test) acc: 98.99%, loss: 0.036\n",
      "[14/20] (train) acc: 99.73%, loss: 0.008 | (test) acc: 98.82%, loss: 0.041\n",
      "[15/20] (train) acc: 99.77%, loss: 0.007 | (test) acc: 98.71%, loss: 0.045\n",
      "[16/20] (train) acc: 99.81%, loss: 0.006 | (test) acc: 99.03%, loss: 0.041\n",
      "[17/20] (train) acc: 99.80%, loss: 0.006 | (test) acc: 99.01%, loss: 0.042\n",
      "[18/20] (train) acc: 99.83%, loss: 0.006 | (test) acc: 98.93%, loss: 0.043\n",
      "[19/20] (train) acc: 99.87%, loss: 0.004 | (test) acc: 98.90%, loss: 0.046\n",
      "[20/20] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.02%, loss: 0.043\n",
      "average of last 5 test acc: 98.98%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper ... \n",
    "\n",
    "* [`conv`, `maxpool`] * 3 + `fc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "h1 = slim.conv2d(x_img, 32, kernel_size=[3,3]) \n",
    "p1 = slim.max_pool2d(h1, kernel_size=[2,2])\n",
    "assert p1.shape[1:] == [14, 14, 32]\n",
    "\n",
    "h2 = slim.conv2d(p1, 64, kernel_size=[3,3])\n",
    "p2 = slim.max_pool2d(h2, kernel_size=[2,2])\n",
    "assert p2.shape[1:] == [7, 7, 64]\n",
    "\n",
    "h3 = slim.conv2d(p2, 128, kernel_size=[3,3])\n",
    "p3 = slim.max_pool2d(h3, kernel_size=[2,2], padding='same')\n",
    "assert p3.shape[1:] == [4, 4, 128]\n",
    "\n",
    "flat = slim.flatten(p3)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 93.49%, loss: 0.211 | (test) acc: 98.20%, loss: 0.053\n",
      "[ 2/100] (train) acc: 98.27%, loss: 0.054 | (test) acc: 98.85%, loss: 0.038\n",
      "[ 3/100] (train) acc: 98.90%, loss: 0.035 | (test) acc: 99.09%, loss: 0.029\n",
      "[ 4/100] (train) acc: 99.16%, loss: 0.027 | (test) acc: 98.95%, loss: 0.033\n",
      "[ 5/100] (train) acc: 99.27%, loss: 0.022 | (test) acc: 99.05%, loss: 0.028\n",
      "[ 6/100] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.17%, loss: 0.024\n",
      "[ 7/100] (train) acc: 99.56%, loss: 0.014 | (test) acc: 98.98%, loss: 0.034\n",
      "[ 8/100] (train) acc: 99.62%, loss: 0.013 | (test) acc: 99.31%, loss: 0.021\n",
      "[ 9/100] (train) acc: 99.62%, loss: 0.011 | (test) acc: 99.01%, loss: 0.032\n",
      "[10/100] (train) acc: 99.65%, loss: 0.010 | (test) acc: 99.05%, loss: 0.032\n",
      "[11/100] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.10%, loss: 0.030\n",
      "[12/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.23%, loss: 0.026\n",
      "[13/100] (train) acc: 99.81%, loss: 0.006 | (test) acc: 99.23%, loss: 0.027\n",
      "[14/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 98.86%, loss: 0.048\n",
      "[15/100] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.16%, loss: 0.037\n",
      "[16/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.22%, loss: 0.033\n",
      "[17/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.17%, loss: 0.034\n",
      "[18/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.30%, loss: 0.028\n",
      "[19/100] (train) acc: 99.81%, loss: 0.005 | (test) acc: 99.01%, loss: 0.040\n",
      "[20/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.19%, loss: 0.041\n",
      "[21/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.34%, loss: 0.031\n",
      "[22/100] (train) acc: 99.83%, loss: 0.005 | (test) acc: 99.16%, loss: 0.035\n",
      "[23/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.16%, loss: 0.035\n",
      "[24/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.32%, loss: 0.031\n",
      "[25/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.05%, loss: 0.041\n",
      "[26/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.25%, loss: 0.041\n",
      "[27/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.25%, loss: 0.034\n",
      "[28/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 98.92%, loss: 0.046\n",
      "[29/100] (train) acc: 99.88%, loss: 0.003 | (test) acc: 99.09%, loss: 0.042\n",
      "[30/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.23%, loss: 0.040\n",
      "[31/100] (train) acc: 99.94%, loss: 0.001 | (test) acc: 99.19%, loss: 0.038\n",
      "[32/100] (train) acc: 99.96%, loss: 0.001 | (test) acc: 98.95%, loss: 0.051\n",
      "[33/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.27%, loss: 0.035\n",
      "[34/100] (train) acc: 99.88%, loss: 0.003 | (test) acc: 99.27%, loss: 0.033\n",
      "[35/100] (train) acc: 99.98%, loss: 0.001 | (test) acc: 99.32%, loss: 0.034\n",
      "[36/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.032\n",
      "[37/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.032\n",
      "[38/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.43%, loss: 0.033\n",
      "[39/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.033\n",
      "[40/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.033\n",
      "[41/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.033\n",
      "[42/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.47%, loss: 0.034\n",
      "[43/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.47%, loss: 0.034\n",
      "[44/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.034\n",
      "[45/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.035\n",
      "[46/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.035\n",
      "[47/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.036\n",
      "[48/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.036\n",
      "[49/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.49%, loss: 0.037\n",
      "[50/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.037\n",
      "[51/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.038\n",
      "[52/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.038\n",
      "[53/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.039\n",
      "[54/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.039\n",
      "[55/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.040\n",
      "[56/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.041\n",
      "[57/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.042\n",
      "[58/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.042\n",
      "[59/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.043\n",
      "[60/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.044\n",
      "[61/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.044\n",
      "[62/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.045\n",
      "[63/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.46%, loss: 0.046\n",
      "[64/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.47%, loss: 0.047\n",
      "[65/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.47%, loss: 0.048\n",
      "[66/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.47%, loss: 0.048\n",
      "[67/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.049\n",
      "[68/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.46%, loss: 0.049\n",
      "[69/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.49%, loss: 0.050\n",
      "[70/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.47%, loss: 0.050\n",
      "[71/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.47%, loss: 0.051\n",
      "[72/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.051\n",
      "[73/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.47%, loss: 0.051\n",
      "[74/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.051\n",
      "[75/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.50%, loss: 0.052\n",
      "[76/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.052\n",
      "[77/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.052\n",
      "[78/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.48%, loss: 0.053\n",
      "[79/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.44%, loss: 0.053\n",
      "[80/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.44%, loss: 0.055\n",
      "[81/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.43%, loss: 0.056\n",
      "[82/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.43%, loss: 0.057\n",
      "[83/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.44%, loss: 0.060\n",
      "[84/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.43%, loss: 0.062\n",
      "[85/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.41%, loss: 0.064\n",
      "[86/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.42%, loss: 0.065\n",
      "[87/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.43%, loss: 0.067\n",
      "[88/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.41%, loss: 0.069\n",
      "[89/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.41%, loss: 0.072\n",
      "[90/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.40%, loss: 0.074\n",
      "[91/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.076\n",
      "[92/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.079\n",
      "[93/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.39%, loss: 0.081\n",
      "[94/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.38%, loss: 0.084\n",
      "[95/100] (train) acc: 100.00%, loss: 0.000 | (test) acc: 99.35%, loss: 0.087\n",
      "[96/100] (train) acc: 99.75%, loss: 0.015 | (test) acc: 99.20%, loss: 0.046\n",
      "[97/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.29%, loss: 0.044\n",
      "[98/100] (train) acc: 99.98%, loss: 0.000 | (test) acc: 99.34%, loss: 0.044\n",
      "[99/100] (train) acc: 99.99%, loss: 0.000 | (test) acc: 99.28%, loss: 0.044\n",
      "[100/100] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.03%, loss: 0.066\n",
      "average of last 5 test acc: 99.23%\n"
     ]
    }
   ],
   "source": [
    "train(epoch_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Deeper ... (VGGNet-like)\n",
    "\n",
    "Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognition.\" arXiv preprint arXiv:1409.1556 (2014).\n",
    "\n",
    "* [`conv`, `conv`, `maxpool`] * 3 + `fc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "h1 = slim.conv2d(x_img, 32, kernel_size=[3,3]) # default relu, xavier\n",
    "h1 = slim.conv2d(h1, 32, kernel_size=[3,3])\n",
    "p1 = slim.max_pool2d(h1, kernel_size=[2,2])\n",
    "assert p1.shape[1:] == [14, 14, 32]\n",
    "\n",
    "h2 = slim.conv2d(p1, 64, kernel_size=[3,3])\n",
    "h2 = slim.conv2d(h2, 64, kernel_size=[3,3])\n",
    "p2 = slim.max_pool2d(h2, kernel_size=[2,2])\n",
    "assert p2.shape[1:] == [7, 7, 64]\n",
    "\n",
    "h3 = slim.conv2d(p2, 128, kernel_size=[3,3])\n",
    "h3 = slim.conv2d(h3, 128, kernel_size=[3,3])\n",
    "p3 = slim.max_pool2d(h3, kernel_size=[2,2], padding='same')\n",
    "assert p3.shape[1:] == [4, 4, 128]\n",
    "\n",
    "flat = slim.flatten(p3)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 94.74%, loss: 0.161 | (test) acc: 98.74%, loss: 0.040\n",
      "[ 2/100] (train) acc: 98.67%, loss: 0.042 | (test) acc: 98.99%, loss: 0.028\n",
      "[ 3/100] (train) acc: 99.10%, loss: 0.029 | (test) acc: 98.94%, loss: 0.034\n",
      "[ 4/100] (train) acc: 99.28%, loss: 0.022 | (test) acc: 99.17%, loss: 0.027\n",
      "[ 5/100] (train) acc: 99.44%, loss: 0.017 | (test) acc: 98.95%, loss: 0.035\n",
      "[ 6/100] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.18%, loss: 0.027\n",
      "[ 7/100] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.11%, loss: 0.031\n",
      "[ 8/100] (train) acc: 99.58%, loss: 0.012 | (test) acc: 99.22%, loss: 0.026\n",
      "[ 9/100] (train) acc: 99.70%, loss: 0.010 | (test) acc: 99.27%, loss: 0.029\n",
      "[10/100] (train) acc: 99.69%, loss: 0.010 | (test) acc: 99.25%, loss: 0.025\n",
      "[11/100] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.31%, loss: 0.025\n",
      "[12/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.35%, loss: 0.025\n",
      "[13/100] (train) acc: 99.79%, loss: 0.007 | (test) acc: 99.30%, loss: 0.034\n",
      "[14/100] (train) acc: 99.80%, loss: 0.007 | (test) acc: 98.96%, loss: 0.047\n",
      "[15/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.17%, loss: 0.031\n",
      "[16/100] (train) acc: 99.82%, loss: 0.005 | (test) acc: 99.24%, loss: 0.033\n",
      "[17/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.32%, loss: 0.029\n",
      "[18/100] (train) acc: 99.81%, loss: 0.006 | (test) acc: 99.18%, loss: 0.038\n",
      "[19/100] (train) acc: 99.83%, loss: 0.005 | (test) acc: 98.84%, loss: 0.044\n",
      "[20/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.35%, loss: 0.035\n",
      "[21/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.36%, loss: 0.028\n",
      "[22/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.14%, loss: 0.052\n",
      "[23/100] (train) acc: 99.85%, loss: 0.005 | (test) acc: 99.11%, loss: 0.042\n",
      "[24/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.40%, loss: 0.034\n",
      "[25/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.13%, loss: 0.041\n",
      "[26/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.28%, loss: 0.033\n",
      "[27/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.22%, loss: 0.035\n",
      "[28/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.36%, loss: 0.049\n",
      "[29/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.38%, loss: 0.041\n",
      "[30/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.33%, loss: 0.033\n",
      "[31/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.26%, loss: 0.042\n",
      "[32/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.22%, loss: 0.049\n",
      "[33/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.23%, loss: 0.041\n",
      "[34/100] (train) acc: 99.90%, loss: 0.004 | (test) acc: 99.38%, loss: 0.038\n",
      "[35/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.34%, loss: 0.031\n",
      "[36/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.31%, loss: 0.045\n",
      "[37/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.27%, loss: 0.046\n",
      "[38/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.23%, loss: 0.045\n",
      "[39/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.00%, loss: 0.047\n",
      "[40/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.32%, loss: 0.044\n",
      "[41/100] (train) acc: 99.90%, loss: 0.004 | (test) acc: 99.09%, loss: 0.061\n",
      "[42/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.20%, loss: 0.052\n",
      "[43/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.18%, loss: 0.065\n",
      "[44/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.34%, loss: 0.041\n",
      "[45/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.37%, loss: 0.043\n",
      "[46/100] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.18%, loss: 0.065\n",
      "[47/100] (train) acc: 99.88%, loss: 0.005 | (test) acc: 99.46%, loss: 0.035\n",
      "[48/100] (train) acc: 99.99%, loss: 0.000 | (test) acc: 99.48%, loss: 0.042\n",
      "[49/100] (train) acc: 99.99%, loss: 0.000 | (test) acc: 99.41%, loss: 0.043\n",
      "[50/100] (train) acc: 99.80%, loss: 0.008 | (test) acc: 99.27%, loss: 0.047\n",
      "[51/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.29%, loss: 0.052\n",
      "[52/100] (train) acc: 99.90%, loss: 0.004 | (test) acc: 99.33%, loss: 0.042\n",
      "[53/100] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.30%, loss: 0.044\n",
      "[54/100] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.11%, loss: 0.068\n",
      "[55/100] (train) acc: 99.90%, loss: 0.005 | (test) acc: 99.26%, loss: 0.044\n",
      "[56/100] (train) acc: 99.90%, loss: 0.004 | (test) acc: 99.19%, loss: 0.045\n",
      "[57/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.18%, loss: 0.057\n",
      "[58/100] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.35%, loss: 0.050\n",
      "[59/100] (train) acc: 99.87%, loss: 0.005 | (test) acc: 99.27%, loss: 0.051\n",
      "[60/100] (train) acc: 99.91%, loss: 0.004 | (test) acc: 99.19%, loss: 0.068\n",
      "[61/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.26%, loss: 0.051\n",
      "[62/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.22%, loss: 0.054\n",
      "[63/100] (train) acc: 99.92%, loss: 0.004 | (test) acc: 99.24%, loss: 0.053\n",
      "[64/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.11%, loss: 0.074\n",
      "[65/100] (train) acc: 99.98%, loss: 0.001 | (test) acc: 99.19%, loss: 0.076\n",
      "[66/100] (train) acc: 99.91%, loss: 0.005 | (test) acc: 98.95%, loss: 0.061\n",
      "[67/100] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.34%, loss: 0.055\n",
      "[68/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.37%, loss: 0.057\n",
      "[69/100] (train) acc: 99.87%, loss: 0.006 | (test) acc: 99.13%, loss: 0.069\n",
      "[70/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.30%, loss: 0.061\n",
      "[71/100] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.26%, loss: 0.085\n",
      "[72/100] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.36%, loss: 0.061\n",
      "[73/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.33%, loss: 0.059\n",
      "[74/100] (train) acc: 99.91%, loss: 0.004 | (test) acc: 99.30%, loss: 0.083\n",
      "[75/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.13%, loss: 0.105\n",
      "[76/100] (train) acc: 99.91%, loss: 0.004 | (test) acc: 99.40%, loss: 0.067\n",
      "[77/100] (train) acc: 99.97%, loss: 0.002 | (test) acc: 99.33%, loss: 0.080\n",
      "[78/100] (train) acc: 99.93%, loss: 0.004 | (test) acc: 99.26%, loss: 0.071\n",
      "[79/100] (train) acc: 99.94%, loss: 0.003 | (test) acc: 99.32%, loss: 0.073\n",
      "[80/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.29%, loss: 0.094\n",
      "[81/100] (train) acc: 99.93%, loss: 0.004 | (test) acc: 99.25%, loss: 0.086\n",
      "[82/100] (train) acc: 99.92%, loss: 0.004 | (test) acc: 99.24%, loss: 0.115\n",
      "[83/100] (train) acc: 99.89%, loss: 0.006 | (test) acc: 99.21%, loss: 0.103\n",
      "[84/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.21%, loss: 0.098\n",
      "[85/100] (train) acc: 99.94%, loss: 0.003 | (test) acc: 99.26%, loss: 0.109\n",
      "[86/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.40%, loss: 0.108\n",
      "[87/100] (train) acc: 99.94%, loss: 0.003 | (test) acc: 99.23%, loss: 0.108\n",
      "[88/100] (train) acc: 99.93%, loss: 0.004 | (test) acc: 99.22%, loss: 0.128\n",
      "[89/100] (train) acc: 99.92%, loss: 0.005 | (test) acc: 99.32%, loss: 0.086\n",
      "[90/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.37%, loss: 0.082\n",
      "[91/100] (train) acc: 99.95%, loss: 0.003 | (test) acc: 99.27%, loss: 0.070\n",
      "[92/100] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.36%, loss: 0.085\n",
      "[93/100] (train) acc: 99.95%, loss: 0.003 | (test) acc: 99.22%, loss: 0.096\n",
      "[94/100] (train) acc: 99.95%, loss: 0.003 | (test) acc: 99.20%, loss: 0.090\n",
      "[95/100] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.26%, loss: 0.086\n",
      "[96/100] (train) acc: 99.92%, loss: 0.005 | (test) acc: 99.27%, loss: 0.081\n",
      "[97/100] (train) acc: 99.90%, loss: 0.006 | (test) acc: 99.11%, loss: 0.106\n",
      "[98/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.34%, loss: 0.110\n",
      "[99/100] (train) acc: 99.98%, loss: 0.001 | (test) acc: 99.37%, loss: 0.113\n",
      "[100/100] (train) acc: 99.88%, loss: 0.009 | (test) acc: 99.32%, loss: 0.108\n",
      "average of last 5 test acc: 99.28%\n"
     ]
    }
   ],
   "source": [
    "train(epoch_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Srivastava, Nitish, et al. \"Dropout: a simple way to prevent neural networks from overfitting.\" Journal of Machine Learning Research 15.1 (2014): 1929-1958.\n",
    "\n",
    "* [`conv`, `conv`, `maxpool`, `dropout`] * 3 + `fc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effects:\n",
    "* ensemble\n",
    "* regularization\n",
    "    * avoid co-adaptation of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "x_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "h1 = slim.conv2d(x_img, 32, kernel_size=[3,3]) # default relu, xavier\n",
    "h1 = slim.conv2d(h1, 32, kernel_size=[3,3])\n",
    "p1 = slim.max_pool2d(h1, kernel_size=[2,2])\n",
    "d1 = slim.dropout(p1, keep_prob=0.7, is_training=training)\n",
    "assert d1.shape[1:] == [14, 14, 32]\n",
    "\n",
    "h2 = slim.conv2d(d1, 64, kernel_size=[3,3])\n",
    "h2 = slim.conv2d(h2, 64, kernel_size=[3,3])\n",
    "p2 = slim.max_pool2d(h2, kernel_size=[2,2])\n",
    "d2 = slim.dropout(p2, keep_prob=0.7, is_training=training)\n",
    "assert d2.shape[1:] == [7, 7, 64]\n",
    "\n",
    "h3 = slim.conv2d(d2, 128, kernel_size=[3,3])\n",
    "h3 = slim.conv2d(h3, 128, kernel_size=[3,3])\n",
    "p3 = slim.max_pool2d(h3, kernel_size=[2,2], padding='same')\n",
    "d3 = slim.dropout(p3, keep_prob=0.7, is_training=training)\n",
    "assert d3.shape[1:] == [4, 4, 128]\n",
    "\n",
    "flat = slim.flatten(d3)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 92.82%, loss: 0.221 | (test) acc: 98.41%, loss: 0.052\n",
      "[ 2/100] (train) acc: 98.13%, loss: 0.059 | (test) acc: 98.96%, loss: 0.032\n",
      "[ 3/100] (train) acc: 98.62%, loss: 0.044 | (test) acc: 99.18%, loss: 0.027\n",
      "[ 4/100] (train) acc: 98.91%, loss: 0.035 | (test) acc: 99.35%, loss: 0.021\n",
      "[ 5/100] (train) acc: 99.00%, loss: 0.032 | (test) acc: 99.39%, loss: 0.021\n",
      "[ 6/100] (train) acc: 99.13%, loss: 0.028 | (test) acc: 99.38%, loss: 0.020\n",
      "[ 7/100] (train) acc: 99.17%, loss: 0.026 | (test) acc: 99.24%, loss: 0.023\n",
      "[ 8/100] (train) acc: 99.25%, loss: 0.023 | (test) acc: 99.31%, loss: 0.020\n",
      "[ 9/100] (train) acc: 99.38%, loss: 0.020 | (test) acc: 99.35%, loss: 0.021\n",
      "[10/100] (train) acc: 99.36%, loss: 0.019 | (test) acc: 99.46%, loss: 0.022\n",
      "[11/100] (train) acc: 99.42%, loss: 0.018 | (test) acc: 99.32%, loss: 0.022\n",
      "[12/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.36%, loss: 0.020\n",
      "[13/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.52%, loss: 0.022\n",
      "[14/100] (train) acc: 99.43%, loss: 0.017 | (test) acc: 99.39%, loss: 0.022\n",
      "[15/100] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.24%, loss: 0.032\n",
      "[16/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.49%, loss: 0.021\n",
      "[17/100] (train) acc: 99.65%, loss: 0.011 | (test) acc: 99.32%, loss: 0.025\n",
      "[18/100] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.49%, loss: 0.020\n",
      "[19/100] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.43%, loss: 0.025\n",
      "[20/100] (train) acc: 99.65%, loss: 0.011 | (test) acc: 99.40%, loss: 0.024\n",
      "[21/100] (train) acc: 99.64%, loss: 0.011 | (test) acc: 99.41%, loss: 0.021\n",
      "[22/100] (train) acc: 99.63%, loss: 0.011 | (test) acc: 99.45%, loss: 0.022\n",
      "[23/100] (train) acc: 99.64%, loss: 0.011 | (test) acc: 99.43%, loss: 0.023\n",
      "[24/100] (train) acc: 99.73%, loss: 0.009 | (test) acc: 99.39%, loss: 0.028\n",
      "[25/100] (train) acc: 99.67%, loss: 0.010 | (test) acc: 99.43%, loss: 0.022\n",
      "[26/100] (train) acc: 99.70%, loss: 0.009 | (test) acc: 99.39%, loss: 0.026\n",
      "[27/100] (train) acc: 99.70%, loss: 0.010 | (test) acc: 99.43%, loss: 0.026\n",
      "[28/100] (train) acc: 99.70%, loss: 0.009 | (test) acc: 99.42%, loss: 0.025\n",
      "[29/100] (train) acc: 99.73%, loss: 0.009 | (test) acc: 99.48%, loss: 0.025\n",
      "[30/100] (train) acc: 99.69%, loss: 0.009 | (test) acc: 99.40%, loss: 0.027\n",
      "[31/100] (train) acc: 99.69%, loss: 0.010 | (test) acc: 99.48%, loss: 0.025\n",
      "[32/100] (train) acc: 99.73%, loss: 0.009 | (test) acc: 99.44%, loss: 0.028\n",
      "[33/100] (train) acc: 99.76%, loss: 0.008 | (test) acc: 99.54%, loss: 0.023\n",
      "[34/100] (train) acc: 99.71%, loss: 0.009 | (test) acc: 99.51%, loss: 0.025\n",
      "[35/100] (train) acc: 99.70%, loss: 0.010 | (test) acc: 99.46%, loss: 0.027\n",
      "[36/100] (train) acc: 99.70%, loss: 0.009 | (test) acc: 99.46%, loss: 0.027\n",
      "[37/100] (train) acc: 99.70%, loss: 0.010 | (test) acc: 99.44%, loss: 0.024\n",
      "[38/100] (train) acc: 99.72%, loss: 0.008 | (test) acc: 99.49%, loss: 0.027\n",
      "[39/100] (train) acc: 99.76%, loss: 0.008 | (test) acc: 99.41%, loss: 0.026\n",
      "[40/100] (train) acc: 99.75%, loss: 0.008 | (test) acc: 99.41%, loss: 0.030\n",
      "[41/100] (train) acc: 99.72%, loss: 0.009 | (test) acc: 99.49%, loss: 0.023\n",
      "[42/100] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.49%, loss: 0.021\n",
      "[43/100] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.42%, loss: 0.027\n",
      "[44/100] (train) acc: 99.70%, loss: 0.010 | (test) acc: 99.50%, loss: 0.024\n",
      "[45/100] (train) acc: 99.76%, loss: 0.008 | (test) acc: 99.46%, loss: 0.030\n",
      "[46/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.45%, loss: 0.032\n",
      "[47/100] (train) acc: 99.72%, loss: 0.009 | (test) acc: 99.44%, loss: 0.036\n",
      "[48/100] (train) acc: 99.77%, loss: 0.008 | (test) acc: 99.36%, loss: 0.031\n",
      "[49/100] (train) acc: 99.77%, loss: 0.007 | (test) acc: 99.46%, loss: 0.032\n",
      "[50/100] (train) acc: 99.74%, loss: 0.009 | (test) acc: 99.47%, loss: 0.034\n",
      "[51/100] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.36%, loss: 0.033\n",
      "[52/100] (train) acc: 99.73%, loss: 0.009 | (test) acc: 99.46%, loss: 0.030\n",
      "[53/100] (train) acc: 99.81%, loss: 0.007 | (test) acc: 99.37%, loss: 0.028\n",
      "[54/100] (train) acc: 99.78%, loss: 0.007 | (test) acc: 99.42%, loss: 0.028\n",
      "[55/100] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.48%, loss: 0.028\n",
      "[56/100] (train) acc: 99.71%, loss: 0.009 | (test) acc: 99.47%, loss: 0.026\n",
      "[57/100] (train) acc: 99.77%, loss: 0.007 | (test) acc: 99.50%, loss: 0.034\n",
      "[58/100] (train) acc: 99.79%, loss: 0.008 | (test) acc: 99.42%, loss: 0.031\n",
      "[59/100] (train) acc: 99.75%, loss: 0.008 | (test) acc: 99.40%, loss: 0.038\n",
      "[60/100] (train) acc: 99.76%, loss: 0.008 | (test) acc: 99.39%, loss: 0.036\n",
      "[61/100] (train) acc: 99.74%, loss: 0.008 | (test) acc: 99.42%, loss: 0.036\n",
      "[62/100] (train) acc: 99.78%, loss: 0.008 | (test) acc: 99.42%, loss: 0.038\n",
      "[63/100] (train) acc: 99.78%, loss: 0.007 | (test) acc: 99.49%, loss: 0.033\n",
      "[64/100] (train) acc: 99.75%, loss: 0.008 | (test) acc: 99.43%, loss: 0.038\n",
      "[65/100] (train) acc: 99.77%, loss: 0.008 | (test) acc: 99.47%, loss: 0.041\n",
      "[66/100] (train) acc: 99.81%, loss: 0.007 | (test) acc: 99.40%, loss: 0.043\n",
      "[67/100] (train) acc: 99.75%, loss: 0.009 | (test) acc: 99.53%, loss: 0.036\n",
      "[68/100] (train) acc: 99.80%, loss: 0.007 | (test) acc: 99.43%, loss: 0.035\n",
      "[69/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.40%, loss: 0.039\n",
      "[70/100] (train) acc: 99.73%, loss: 0.010 | (test) acc: 99.45%, loss: 0.034\n",
      "[71/100] (train) acc: 99.73%, loss: 0.008 | (test) acc: 99.44%, loss: 0.038\n",
      "[72/100] (train) acc: 99.80%, loss: 0.007 | (test) acc: 99.40%, loss: 0.035\n",
      "[73/100] (train) acc: 99.81%, loss: 0.007 | (test) acc: 99.42%, loss: 0.036\n",
      "[74/100] (train) acc: 99.78%, loss: 0.008 | (test) acc: 99.47%, loss: 0.041\n",
      "[75/100] (train) acc: 99.77%, loss: 0.009 | (test) acc: 99.39%, loss: 0.035\n",
      "[76/100] (train) acc: 99.77%, loss: 0.008 | (test) acc: 99.40%, loss: 0.034\n",
      "[77/100] (train) acc: 99.81%, loss: 0.006 | (test) acc: 99.46%, loss: 0.042\n",
      "[78/100] (train) acc: 99.79%, loss: 0.007 | (test) acc: 99.41%, loss: 0.042\n",
      "[79/100] (train) acc: 99.82%, loss: 0.006 | (test) acc: 99.36%, loss: 0.039\n",
      "[80/100] (train) acc: 99.77%, loss: 0.009 | (test) acc: 99.45%, loss: 0.039\n",
      "[81/100] (train) acc: 99.76%, loss: 0.008 | (test) acc: 99.51%, loss: 0.042\n",
      "[82/100] (train) acc: 99.79%, loss: 0.008 | (test) acc: 99.30%, loss: 0.049\n",
      "[83/100] (train) acc: 99.77%, loss: 0.008 | (test) acc: 99.47%, loss: 0.042\n",
      "[84/100] (train) acc: 99.81%, loss: 0.008 | (test) acc: 99.47%, loss: 0.035\n",
      "[85/100] (train) acc: 99.75%, loss: 0.009 | (test) acc: 99.51%, loss: 0.036\n",
      "[86/100] (train) acc: 99.79%, loss: 0.007 | (test) acc: 99.56%, loss: 0.034\n",
      "[87/100] (train) acc: 99.84%, loss: 0.006 | (test) acc: 99.42%, loss: 0.046\n",
      "[88/100] (train) acc: 99.66%, loss: 0.012 | (test) acc: 99.45%, loss: 0.050\n",
      "[89/100] (train) acc: 99.76%, loss: 0.010 | (test) acc: 99.43%, loss: 0.044\n",
      "[90/100] (train) acc: 99.82%, loss: 0.007 | (test) acc: 99.48%, loss: 0.042\n",
      "[91/100] (train) acc: 99.75%, loss: 0.010 | (test) acc: 99.38%, loss: 0.047\n",
      "[92/100] (train) acc: 99.78%, loss: 0.008 | (test) acc: 99.53%, loss: 0.042\n",
      "[93/100] (train) acc: 99.82%, loss: 0.006 | (test) acc: 99.49%, loss: 0.049\n",
      "[94/100] (train) acc: 99.78%, loss: 0.010 | (test) acc: 99.47%, loss: 0.038\n",
      "[95/100] (train) acc: 99.80%, loss: 0.008 | (test) acc: 99.43%, loss: 0.052\n",
      "[96/100] (train) acc: 99.82%, loss: 0.007 | (test) acc: 99.41%, loss: 0.052\n",
      "[97/100] (train) acc: 99.75%, loss: 0.010 | (test) acc: 99.36%, loss: 0.046\n",
      "[98/100] (train) acc: 99.77%, loss: 0.010 | (test) acc: 99.45%, loss: 0.033\n",
      "[99/100] (train) acc: 99.81%, loss: 0.007 | (test) acc: 99.42%, loss: 0.040\n",
      "[100/100] (train) acc: 99.78%, loss: 0.009 | (test) acc: 99.54%, loss: 0.029\n",
      "average of last 5 test acc: 99.44%\n"
     ]
    }
   ],
   "source": [
    "# dropout use training/test phase flag\n",
    "train(epoch_n=100, use_training_ph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "\n",
    "Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" International Conference on Machine Learning. 2015.\n",
    "\n",
    "* [`conv`, `BN`, `conv`, `BN`, `maxpool`, `dropout`] * 3 + `fc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "x_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "bn_param = {'is_training': training, 'scale': True, 'decay': 0.99}\n",
    "h1 = slim.conv2d(x_img, 32, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "h1 = slim.conv2d(h1, 32, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "p1 = slim.max_pool2d(h1, kernel_size=[2,2])\n",
    "d1 = slim.dropout(p1, keep_prob=0.7, is_training=training)\n",
    "assert d1.shape[1:] == [14, 14, 32]\n",
    "\n",
    "h2 = slim.conv2d(d1, 64, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "h2 = slim.conv2d(h2, 64, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "p2 = slim.max_pool2d(h2, kernel_size=[2,2])\n",
    "d2 = slim.dropout(p2, keep_prob=0.7, is_training=training)\n",
    "assert d2.shape[1:] == [7, 7, 64]\n",
    "\n",
    "h3 = slim.conv2d(d2, 128, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "h3 = slim.conv2d(h3, 128, kernel_size=[3,3], normalizer_fn=slim.batch_norm, normalizer_params=bn_param)\n",
    "p3 = slim.max_pool2d(h3, kernel_size=[2,2], padding='same')\n",
    "d3 = slim.dropout(p3, keep_prob=0.7, is_training=training)\n",
    "assert d3.shape[1:] == [4, 4, 128]\n",
    "\n",
    "flat = slim.flatten(d3)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# When using batch normalization, update moving average pipeline should be controlled explicitly even with slim\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 93.44%, loss: 0.217 | (test) acc: 92.64%, loss: 0.217\n",
      "[ 2/100] (train) acc: 98.03%, loss: 0.063 | (test) acc: 99.07%, loss: 0.029\n",
      "[ 3/100] (train) acc: 98.46%, loss: 0.048 | (test) acc: 99.17%, loss: 0.025\n",
      "[ 4/100] (train) acc: 98.73%, loss: 0.042 | (test) acc: 98.79%, loss: 0.037\n",
      "[ 5/100] (train) acc: 98.83%, loss: 0.037 | (test) acc: 99.00%, loss: 0.035\n",
      "[ 6/100] (train) acc: 98.99%, loss: 0.034 | (test) acc: 99.04%, loss: 0.028\n",
      "[ 7/100] (train) acc: 99.04%, loss: 0.030 | (test) acc: 99.40%, loss: 0.020\n",
      "[ 8/100] (train) acc: 99.08%, loss: 0.030 | (test) acc: 99.32%, loss: 0.024\n",
      "[ 9/100] (train) acc: 99.23%, loss: 0.024 | (test) acc: 99.40%, loss: 0.019\n",
      "[10/100] (train) acc: 99.17%, loss: 0.025 | (test) acc: 99.48%, loss: 0.017\n",
      "[11/100] (train) acc: 99.28%, loss: 0.023 | (test) acc: 99.53%, loss: 0.016\n",
      "[12/100] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.15%, loss: 0.029\n",
      "[13/100] (train) acc: 99.36%, loss: 0.019 | (test) acc: 98.93%, loss: 0.033\n",
      "[14/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.34%, loss: 0.023\n",
      "[15/100] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.36%, loss: 0.025\n",
      "[16/100] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.36%, loss: 0.022\n",
      "[17/100] (train) acc: 99.53%, loss: 0.013 | (test) acc: 99.27%, loss: 0.025\n",
      "[18/100] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.47%, loss: 0.020\n",
      "[19/100] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.34%, loss: 0.022\n",
      "[20/100] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.37%, loss: 0.021\n",
      "[21/100] (train) acc: 99.60%, loss: 0.012 | (test) acc: 99.48%, loss: 0.019\n",
      "[22/100] (train) acc: 99.66%, loss: 0.011 | (test) acc: 99.46%, loss: 0.021\n",
      "[23/100] (train) acc: 99.61%, loss: 0.011 | (test) acc: 99.52%, loss: 0.017\n",
      "[24/100] (train) acc: 99.68%, loss: 0.010 | (test) acc: 99.51%, loss: 0.018\n",
      "[25/100] (train) acc: 99.67%, loss: 0.010 | (test) acc: 99.36%, loss: 0.023\n",
      "[26/100] (train) acc: 99.65%, loss: 0.010 | (test) acc: 99.46%, loss: 0.022\n",
      "[27/100] (train) acc: 99.70%, loss: 0.008 | (test) acc: 99.51%, loss: 0.020\n",
      "[28/100] (train) acc: 99.73%, loss: 0.008 | (test) acc: 99.57%, loss: 0.021\n",
      "[29/100] (train) acc: 99.73%, loss: 0.008 | (test) acc: 99.44%, loss: 0.022\n",
      "[30/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.42%, loss: 0.020\n",
      "[31/100] (train) acc: 99.68%, loss: 0.010 | (test) acc: 99.57%, loss: 0.018\n",
      "[32/100] (train) acc: 99.80%, loss: 0.006 | (test) acc: 99.49%, loss: 0.019\n",
      "[33/100] (train) acc: 99.80%, loss: 0.006 | (test) acc: 99.54%, loss: 0.020\n",
      "[34/100] (train) acc: 99.75%, loss: 0.007 | (test) acc: 99.54%, loss: 0.020\n",
      "[35/100] (train) acc: 99.77%, loss: 0.006 | (test) acc: 99.58%, loss: 0.018\n",
      "[36/100] (train) acc: 99.81%, loss: 0.006 | (test) acc: 99.40%, loss: 0.023\n",
      "[37/100] (train) acc: 99.79%, loss: 0.006 | (test) acc: 99.50%, loss: 0.020\n",
      "[38/100] (train) acc: 99.83%, loss: 0.005 | (test) acc: 99.36%, loss: 0.024\n",
      "[39/100] (train) acc: 99.76%, loss: 0.006 | (test) acc: 99.45%, loss: 0.024\n",
      "[40/100] (train) acc: 99.82%, loss: 0.005 | (test) acc: 99.53%, loss: 0.017\n",
      "[41/100] (train) acc: 99.79%, loss: 0.006 | (test) acc: 99.43%, loss: 0.028\n",
      "[42/100] (train) acc: 99.85%, loss: 0.005 | (test) acc: 99.43%, loss: 0.027\n",
      "[43/100] (train) acc: 99.81%, loss: 0.006 | (test) acc: 99.50%, loss: 0.020\n",
      "[44/100] (train) acc: 99.82%, loss: 0.005 | (test) acc: 99.52%, loss: 0.022\n",
      "[45/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.47%, loss: 0.023\n",
      "[46/100] (train) acc: 99.82%, loss: 0.005 | (test) acc: 99.43%, loss: 0.024\n",
      "[47/100] (train) acc: 99.85%, loss: 0.005 | (test) acc: 99.57%, loss: 0.018\n",
      "[48/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.56%, loss: 0.021\n",
      "[49/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.61%, loss: 0.019\n",
      "[50/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.48%, loss: 0.024\n",
      "[51/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.53%, loss: 0.026\n",
      "[52/100] (train) acc: 99.85%, loss: 0.005 | (test) acc: 99.39%, loss: 0.027\n",
      "[53/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.45%, loss: 0.025\n",
      "[54/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.59%, loss: 0.022\n",
      "[55/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.48%, loss: 0.023\n",
      "[56/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.62%, loss: 0.023\n",
      "[57/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.47%, loss: 0.023\n",
      "[58/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.52%, loss: 0.025\n",
      "[59/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.59%, loss: 0.022\n",
      "[60/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.61%, loss: 0.021\n",
      "[61/100] (train) acc: 99.87%, loss: 0.003 | (test) acc: 99.55%, loss: 0.022\n",
      "[62/100] (train) acc: 99.88%, loss: 0.003 | (test) acc: 99.60%, loss: 0.019\n",
      "[63/100] (train) acc: 99.90%, loss: 0.002 | (test) acc: 99.64%, loss: 0.021\n",
      "[64/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.51%, loss: 0.026\n",
      "[65/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.47%, loss: 0.028\n",
      "[66/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.60%, loss: 0.024\n",
      "[67/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.51%, loss: 0.026\n",
      "[68/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.52%, loss: 0.024\n",
      "[69/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.47%, loss: 0.023\n",
      "[70/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.48%, loss: 0.027\n",
      "[71/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.51%, loss: 0.022\n",
      "[72/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.47%, loss: 0.024\n",
      "[73/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.63%, loss: 0.023\n",
      "[74/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.59%, loss: 0.021\n",
      "[75/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.55%, loss: 0.028\n",
      "[76/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.58%, loss: 0.024\n",
      "[77/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.50%, loss: 0.028\n",
      "[78/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.61%, loss: 0.024\n",
      "[79/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.50%, loss: 0.024\n",
      "[80/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.53%, loss: 0.024\n",
      "[81/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.51%, loss: 0.029\n",
      "[82/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.55%, loss: 0.023\n",
      "[83/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.55%, loss: 0.026\n",
      "[84/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.61%, loss: 0.024\n",
      "[85/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.62%, loss: 0.022\n",
      "[86/100] (train) acc: 99.96%, loss: 0.002 | (test) acc: 99.58%, loss: 0.027\n",
      "[87/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.56%, loss: 0.027\n",
      "[88/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.61%, loss: 0.025\n",
      "[89/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.55%, loss: 0.026\n",
      "[90/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.50%, loss: 0.025\n",
      "[91/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.56%, loss: 0.020\n",
      "[92/100] (train) acc: 99.95%, loss: 0.001 | (test) acc: 99.52%, loss: 0.025\n",
      "[93/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.62%, loss: 0.023\n",
      "[94/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.58%, loss: 0.024\n",
      "[95/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.52%, loss: 0.028\n",
      "[96/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.54%, loss: 0.027\n",
      "[97/100] (train) acc: 99.97%, loss: 0.001 | (test) acc: 99.56%, loss: 0.028\n",
      "[98/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.46%, loss: 0.031\n",
      "[99/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.57%, loss: 0.024\n",
      "[100/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.55%, loss: 0.024\n",
      "average of last 5 test acc: 99.54%\n"
     ]
    }
   ],
   "source": [
    "train(epoch_n=100, use_training_ph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More slim by slim\n",
    "\n",
    "* Same as BN model - which is our best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "n_filters = 32\n",
    "bn_param = {'is_training': training, 'scale': True, 'decay': 0.99}\n",
    "with slim.arg_scope([slim.conv2d], kernel_size=[3,3],\n",
    "                    normalizer_fn=slim.batch_norm, normalizer_params=bn_param):\n",
    "    for _ in range(3):\n",
    "        net = slim.conv2d(net, n_filters)\n",
    "        net = slim.conv2d(net, n_filters)\n",
    "        net = slim.max_pool2d(net, kernel_size=[2,2], padding='same')\n",
    "        net = slim.dropout(net, keep_prob=0.7, is_training=training)\n",
    "        n_filters *= 2\n",
    "\n",
    "flat = slim.flatten(net)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 93.63%, loss: 0.205 | (test) acc: 97.94%, loss: 0.063\n",
      "[ 2/100] (train) acc: 98.07%, loss: 0.060 | (test) acc: 98.43%, loss: 0.048\n",
      "[ 3/100] (train) acc: 98.44%, loss: 0.051 | (test) acc: 99.02%, loss: 0.028\n",
      "[ 4/100] (train) acc: 98.77%, loss: 0.040 | (test) acc: 98.61%, loss: 0.044\n",
      "[ 5/100] (train) acc: 98.82%, loss: 0.037 | (test) acc: 99.25%, loss: 0.025\n",
      "[ 6/100] (train) acc: 99.03%, loss: 0.033 | (test) acc: 99.27%, loss: 0.024\n",
      "[ 7/100] (train) acc: 99.04%, loss: 0.030 | (test) acc: 99.43%, loss: 0.016\n",
      "[ 8/100] (train) acc: 99.12%, loss: 0.027 | (test) acc: 99.26%, loss: 0.024\n",
      "[ 9/100] (train) acc: 99.20%, loss: 0.025 | (test) acc: 99.36%, loss: 0.022\n",
      "[10/100] (train) acc: 99.24%, loss: 0.026 | (test) acc: 99.25%, loss: 0.027\n",
      "[11/100] (train) acc: 99.24%, loss: 0.024 | (test) acc: 99.29%, loss: 0.027\n",
      "[12/100] (train) acc: 99.41%, loss: 0.019 | (test) acc: 99.12%, loss: 0.028\n",
      "[13/100] (train) acc: 99.29%, loss: 0.021 | (test) acc: 99.25%, loss: 0.024\n",
      "[14/100] (train) acc: 99.51%, loss: 0.017 | (test) acc: 99.37%, loss: 0.022\n",
      "[15/100] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.28%, loss: 0.024\n",
      "[16/100] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.31%, loss: 0.020\n",
      "[17/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.47%, loss: 0.020\n",
      "[18/100] (train) acc: 99.55%, loss: 0.013 | (test) acc: 99.35%, loss: 0.023\n",
      "[19/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.33%, loss: 0.025\n",
      "[20/100] (train) acc: 99.63%, loss: 0.011 | (test) acc: 99.39%, loss: 0.019\n",
      "[21/100] (train) acc: 99.61%, loss: 0.012 | (test) acc: 99.54%, loss: 0.016\n",
      "[22/100] (train) acc: 99.63%, loss: 0.011 | (test) acc: 99.30%, loss: 0.027\n",
      "[23/100] (train) acc: 99.65%, loss: 0.010 | (test) acc: 99.29%, loss: 0.031\n",
      "[24/100] (train) acc: 99.69%, loss: 0.010 | (test) acc: 99.40%, loss: 0.019\n",
      "[25/100] (train) acc: 99.64%, loss: 0.010 | (test) acc: 99.48%, loss: 0.021\n",
      "[26/100] (train) acc: 99.73%, loss: 0.009 | (test) acc: 99.43%, loss: 0.019\n",
      "[27/100] (train) acc: 99.73%, loss: 0.008 | (test) acc: 99.43%, loss: 0.021\n",
      "[28/100] (train) acc: 99.78%, loss: 0.006 | (test) acc: 99.41%, loss: 0.020\n",
      "[29/100] (train) acc: 99.73%, loss: 0.008 | (test) acc: 99.51%, loss: 0.017\n",
      "[30/100] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.52%, loss: 0.022\n",
      "[31/100] (train) acc: 99.71%, loss: 0.009 | (test) acc: 99.45%, loss: 0.026\n",
      "[32/100] (train) acc: 99.80%, loss: 0.006 | (test) acc: 99.40%, loss: 0.024\n",
      "[33/100] (train) acc: 99.76%, loss: 0.008 | (test) acc: 99.46%, loss: 0.025\n",
      "[34/100] (train) acc: 99.81%, loss: 0.006 | (test) acc: 99.41%, loss: 0.027\n",
      "[35/100] (train) acc: 99.79%, loss: 0.006 | (test) acc: 99.56%, loss: 0.017\n",
      "[36/100] (train) acc: 99.76%, loss: 0.007 | (test) acc: 99.46%, loss: 0.022\n",
      "[37/100] (train) acc: 99.83%, loss: 0.006 | (test) acc: 99.49%, loss: 0.021\n",
      "[38/100] (train) acc: 99.80%, loss: 0.006 | (test) acc: 99.51%, loss: 0.019\n",
      "[39/100] (train) acc: 99.79%, loss: 0.006 | (test) acc: 99.49%, loss: 0.021\n",
      "[40/100] (train) acc: 99.78%, loss: 0.006 | (test) acc: 99.34%, loss: 0.025\n",
      "[41/100] (train) acc: 99.81%, loss: 0.005 | (test) acc: 99.57%, loss: 0.018\n",
      "[42/100] (train) acc: 99.85%, loss: 0.005 | (test) acc: 99.41%, loss: 0.022\n",
      "[43/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.55%, loss: 0.021\n",
      "[44/100] (train) acc: 99.80%, loss: 0.005 | (test) acc: 99.47%, loss: 0.027\n",
      "[45/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.52%, loss: 0.022\n",
      "[46/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.40%, loss: 0.027\n",
      "[47/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.59%, loss: 0.019\n",
      "[48/100] (train) acc: 99.84%, loss: 0.005 | (test) acc: 99.40%, loss: 0.026\n",
      "[49/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.54%, loss: 0.020\n",
      "[50/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.49%, loss: 0.026\n",
      "[51/100] (train) acc: 99.86%, loss: 0.005 | (test) acc: 99.50%, loss: 0.022\n",
      "[52/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.43%, loss: 0.024\n",
      "[53/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.41%, loss: 0.024\n",
      "[54/100] (train) acc: 99.87%, loss: 0.004 | (test) acc: 99.50%, loss: 0.022\n",
      "[55/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.48%, loss: 0.023\n",
      "[56/100] (train) acc: 99.87%, loss: 0.005 | (test) acc: 99.47%, loss: 0.023\n",
      "[57/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.56%, loss: 0.024\n",
      "[58/100] (train) acc: 99.86%, loss: 0.004 | (test) acc: 99.44%, loss: 0.025\n",
      "[59/100] (train) acc: 99.83%, loss: 0.004 | (test) acc: 99.52%, loss: 0.020\n",
      "[60/100] (train) acc: 99.85%, loss: 0.004 | (test) acc: 99.54%, loss: 0.021\n",
      "[61/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.49%, loss: 0.022\n",
      "[62/100] (train) acc: 99.84%, loss: 0.004 | (test) acc: 99.52%, loss: 0.021\n",
      "[63/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.56%, loss: 0.022\n",
      "[64/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.58%, loss: 0.024\n",
      "[65/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.57%, loss: 0.023\n",
      "[66/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.45%, loss: 0.028\n",
      "[67/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.60%, loss: 0.020\n",
      "[68/100] (train) acc: 99.89%, loss: 0.004 | (test) acc: 99.57%, loss: 0.020\n",
      "[69/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.46%, loss: 0.024\n",
      "[70/100] (train) acc: 99.88%, loss: 0.004 | (test) acc: 99.54%, loss: 0.022\n",
      "[71/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.58%, loss: 0.023\n",
      "[72/100] (train) acc: 99.88%, loss: 0.003 | (test) acc: 99.53%, loss: 0.022\n",
      "[73/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.55%, loss: 0.025\n",
      "[74/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.42%, loss: 0.031\n",
      "[75/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.52%, loss: 0.025\n",
      "[76/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.45%, loss: 0.025\n",
      "[77/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.54%, loss: 0.027\n",
      "[78/100] (train) acc: 99.95%, loss: 0.002 | (test) acc: 99.55%, loss: 0.022\n",
      "[79/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.49%, loss: 0.031\n",
      "[80/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.45%, loss: 0.029\n",
      "[81/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.58%, loss: 0.025\n",
      "[82/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.41%, loss: 0.026\n",
      "[83/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.56%, loss: 0.025\n",
      "[84/100] (train) acc: 99.93%, loss: 0.003 | (test) acc: 99.46%, loss: 0.027\n",
      "[85/100] (train) acc: 99.89%, loss: 0.003 | (test) acc: 99.52%, loss: 0.029\n",
      "[86/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.48%, loss: 0.029\n",
      "[87/100] (train) acc: 99.90%, loss: 0.003 | (test) acc: 99.52%, loss: 0.026\n",
      "[88/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.51%, loss: 0.026\n",
      "[89/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.52%, loss: 0.026\n",
      "[90/100] (train) acc: 99.92%, loss: 0.003 | (test) acc: 99.55%, loss: 0.030\n",
      "[91/100] (train) acc: 99.92%, loss: 0.002 | (test) acc: 99.45%, loss: 0.029\n",
      "[92/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.56%, loss: 0.027\n",
      "[93/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.45%, loss: 0.023\n",
      "[94/100] (train) acc: 99.91%, loss: 0.003 | (test) acc: 99.44%, loss: 0.031\n",
      "[95/100] (train) acc: 99.96%, loss: 0.001 | (test) acc: 99.42%, loss: 0.036\n",
      "[96/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.56%, loss: 0.025\n",
      "[97/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.47%, loss: 0.031\n",
      "[98/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.52%, loss: 0.026\n",
      "[99/100] (train) acc: 99.94%, loss: 0.002 | (test) acc: 99.56%, loss: 0.024\n",
      "[100/100] (train) acc: 99.93%, loss: 0.002 | (test) acc: 99.57%, loss: 0.024\n",
      "average of last 5 test acc: 99.54%\n"
     ]
    }
   ],
   "source": [
    "train(epoch_n=100, use_training_ph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator\n",
    "\n",
    "* Apply rotation, shift, and zoom\n",
    "* I used `keras.preprocessing.image.ImageDataGenerator`, but https://github.com/aleju/imgaug is also good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AffineGenerator():\n",
    "    def __init__(self, mnist):\n",
    "        # keras become available directly as part of tensorflow from TF 1.1.\n",
    "        ImageDataGenerator = tf.contrib.keras.preprocessing.image.ImageDataGenerator\n",
    "        \n",
    "        self.mnist = mnist\n",
    "        self.datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, \n",
    "                                          zoom_range=0.1)\n",
    "        self.train_x = np.reshape(self.mnist.train.images, [-1, 28, 28, 1])\n",
    "        self.train_y = self.mnist.train.labels\n",
    "\n",
    "    def generate(self, batch_size=64):\n",
    "        cnt = 0\n",
    "        batch_n = self.train_x.shape[0] // batch_size\n",
    "        for x, y in self.datagen.flow(self.train_x, self.train_y, batch_size=batch_size):\n",
    "            ret_x = x.reshape(-1, 784)\n",
    "            yield ret_x, y\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt == batch_n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_da(epoch_n=20, batch_size=100, use_training_ph=False, datagen=AffineGenerator(mnist)):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    N = mnist.train.num_examples\n",
    "    dq = collections.deque(maxlen=5)\n",
    "\n",
    "    for epoch in range(epoch_n):\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "\n",
    "        n_iter = 0\n",
    "        for batch_x, batch_y in datagen.generate(batch_size=batch_size):\n",
    "            feed_dict = {X: batch_x, y: batch_y}\n",
    "            if use_training_ph:\n",
    "                feed_dict[training] = True\n",
    "            _, cur_acc, cur_loss = sess.run([train_op, accuracy, loss], feed_dict=feed_dict)\n",
    "            avg_acc += cur_acc\n",
    "            avg_loss += cur_loss\n",
    "            n_iter += 1\n",
    "\n",
    "        avg_acc /= n_iter\n",
    "        avg_loss /= n_iter\n",
    "\n",
    "        test_acc = 0.\n",
    "        test_loss = 0.\n",
    "        for _ in range(mnist.test.num_examples // 1000):\n",
    "            batch_x, batch_y = mnist.test.next_batch(1000)\n",
    "            feed_dict = {X: batch_x, y: batch_y}\n",
    "            if use_training_ph:\n",
    "                feed_dict[training] = False\n",
    "            cur_acc, cur_loss = sess.run([accuracy, loss], feed_dict=feed_dict)\n",
    "            test_acc += cur_acc\n",
    "            test_loss += cur_loss\n",
    "        test_acc /= (mnist.test.num_examples // 1000)\n",
    "        test_loss /= (mnist.test.num_examples // 1000)\n",
    "\n",
    "        print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f}\".\n",
    "              format(epoch+1, epoch_n, avg_acc, avg_loss, test_acc, test_loss))\n",
    "        dq.append(test_acc)\n",
    "\n",
    "    score = np.average(dq)\n",
    "    print(\"average of last 5 test acc: {:.2%}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same as the slim-BN model\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "n_filters = 32\n",
    "bn_param = {'is_training': training, 'scale': True, 'decay': 0.99}\n",
    "with slim.arg_scope([slim.conv2d], kernel_size=[3,3],\n",
    "                    normalizer_fn=slim.batch_norm, normalizer_params=bn_param):\n",
    "    for _ in range(3):\n",
    "        net = slim.conv2d(net, n_filters)\n",
    "        net = slim.conv2d(net, n_filters)\n",
    "        net = slim.max_pool2d(net, kernel_size=[2,2], padding='same')\n",
    "        net = slim.dropout(net, keep_prob=0.7, is_training=training)\n",
    "        n_filters *= 2\n",
    "\n",
    "flat = slim.flatten(net)\n",
    "logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100] (train) acc: 88.79%, loss: 0.362 | (test) acc: 97.99%, loss: 0.059\n",
      "[ 2/100] (train) acc: 96.63%, loss: 0.110 | (test) acc: 98.57%, loss: 0.042\n",
      "[ 3/100] (train) acc: 97.37%, loss: 0.084 | (test) acc: 99.13%, loss: 0.026\n",
      "[ 4/100] (train) acc: 97.63%, loss: 0.075 | (test) acc: 99.30%, loss: 0.022\n",
      "[ 5/100] (train) acc: 98.07%, loss: 0.065 | (test) acc: 99.18%, loss: 0.023\n",
      "[ 6/100] (train) acc: 98.07%, loss: 0.063 | (test) acc: 98.83%, loss: 0.034\n",
      "[ 7/100] (train) acc: 98.24%, loss: 0.059 | (test) acc: 99.33%, loss: 0.019\n",
      "[ 8/100] (train) acc: 98.34%, loss: 0.053 | (test) acc: 99.55%, loss: 0.012\n",
      "[ 9/100] (train) acc: 98.47%, loss: 0.050 | (test) acc: 99.52%, loss: 0.016\n",
      "[10/100] (train) acc: 98.57%, loss: 0.048 | (test) acc: 99.43%, loss: 0.016\n",
      "[11/100] (train) acc: 98.60%, loss: 0.045 | (test) acc: 99.44%, loss: 0.018\n",
      "[12/100] (train) acc: 98.61%, loss: 0.045 | (test) acc: 99.22%, loss: 0.024\n",
      "[13/100] (train) acc: 98.70%, loss: 0.042 | (test) acc: 99.22%, loss: 0.022\n",
      "[14/100] (train) acc: 98.70%, loss: 0.042 | (test) acc: 99.45%, loss: 0.020\n",
      "[15/100] (train) acc: 98.72%, loss: 0.040 | (test) acc: 99.42%, loss: 0.019\n",
      "[16/100] (train) acc: 98.85%, loss: 0.038 | (test) acc: 99.49%, loss: 0.015\n",
      "[17/100] (train) acc: 98.87%, loss: 0.036 | (test) acc: 99.44%, loss: 0.018\n",
      "[18/100] (train) acc: 98.92%, loss: 0.034 | (test) acc: 99.52%, loss: 0.014\n",
      "[19/100] (train) acc: 98.93%, loss: 0.034 | (test) acc: 99.57%, loss: 0.014\n",
      "[20/100] (train) acc: 99.04%, loss: 0.031 | (test) acc: 99.51%, loss: 0.015\n",
      "[21/100] (train) acc: 99.07%, loss: 0.031 | (test) acc: 99.58%, loss: 0.016\n",
      "[22/100] (train) acc: 99.09%, loss: 0.028 | (test) acc: 99.58%, loss: 0.013\n",
      "[23/100] (train) acc: 99.01%, loss: 0.030 | (test) acc: 99.51%, loss: 0.018\n",
      "[24/100] (train) acc: 99.07%, loss: 0.029 | (test) acc: 99.54%, loss: 0.017\n",
      "[25/100] (train) acc: 99.10%, loss: 0.028 | (test) acc: 99.58%, loss: 0.013\n",
      "[26/100] (train) acc: 99.10%, loss: 0.028 | (test) acc: 99.57%, loss: 0.014\n",
      "[27/100] (train) acc: 99.15%, loss: 0.027 | (test) acc: 99.58%, loss: 0.015\n",
      "[28/100] (train) acc: 99.19%, loss: 0.026 | (test) acc: 99.65%, loss: 0.013\n",
      "[29/100] (train) acc: 99.19%, loss: 0.026 | (test) acc: 99.55%, loss: 0.015\n",
      "[30/100] (train) acc: 99.17%, loss: 0.026 | (test) acc: 99.63%, loss: 0.012\n",
      "[31/100] (train) acc: 99.18%, loss: 0.026 | (test) acc: 99.46%, loss: 0.016\n",
      "[32/100] (train) acc: 99.15%, loss: 0.026 | (test) acc: 99.61%, loss: 0.013\n",
      "[33/100] (train) acc: 99.28%, loss: 0.023 | (test) acc: 99.62%, loss: 0.011\n",
      "[34/100] (train) acc: 99.25%, loss: 0.024 | (test) acc: 99.54%, loss: 0.013\n",
      "[35/100] (train) acc: 99.30%, loss: 0.023 | (test) acc: 99.61%, loss: 0.013\n",
      "[36/100] (train) acc: 99.23%, loss: 0.024 | (test) acc: 99.62%, loss: 0.012\n",
      "[37/100] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.58%, loss: 0.012\n",
      "[38/100] (train) acc: 99.28%, loss: 0.023 | (test) acc: 99.65%, loss: 0.012\n",
      "[39/100] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.67%, loss: 0.011\n",
      "[40/100] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.59%, loss: 0.013\n",
      "[41/100] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.58%, loss: 0.012\n",
      "[42/100] (train) acc: 99.30%, loss: 0.021 | (test) acc: 99.52%, loss: 0.014\n",
      "[43/100] (train) acc: 99.36%, loss: 0.021 | (test) acc: 99.64%, loss: 0.010\n",
      "[44/100] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.62%, loss: 0.011\n",
      "[45/100] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.63%, loss: 0.011\n",
      "[46/100] (train) acc: 99.36%, loss: 0.021 | (test) acc: 99.58%, loss: 0.013\n",
      "[47/100] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.60%, loss: 0.014\n",
      "[48/100] (train) acc: 99.38%, loss: 0.020 | (test) acc: 99.54%, loss: 0.016\n",
      "[49/100] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.60%, loss: 0.012\n",
      "[50/100] (train) acc: 99.41%, loss: 0.020 | (test) acc: 99.59%, loss: 0.012\n",
      "[51/100] (train) acc: 99.42%, loss: 0.018 | (test) acc: 99.58%, loss: 0.016\n",
      "[52/100] (train) acc: 99.35%, loss: 0.019 | (test) acc: 99.48%, loss: 0.016\n",
      "[53/100] (train) acc: 99.38%, loss: 0.018 | (test) acc: 99.59%, loss: 0.012\n",
      "[54/100] (train) acc: 99.42%, loss: 0.019 | (test) acc: 99.55%, loss: 0.012\n",
      "[55/100] (train) acc: 99.45%, loss: 0.018 | (test) acc: 99.57%, loss: 0.014\n",
      "[56/100] (train) acc: 99.40%, loss: 0.018 | (test) acc: 99.70%, loss: 0.010\n",
      "[57/100] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.67%, loss: 0.012\n",
      "[58/100] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.71%, loss: 0.010\n",
      "[59/100] (train) acc: 99.39%, loss: 0.018 | (test) acc: 99.60%, loss: 0.012\n",
      "[60/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.63%, loss: 0.013\n",
      "[61/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.61%, loss: 0.014\n",
      "[62/100] (train) acc: 99.42%, loss: 0.019 | (test) acc: 99.67%, loss: 0.011\n",
      "[63/100] (train) acc: 99.46%, loss: 0.018 | (test) acc: 99.67%, loss: 0.012\n",
      "[64/100] (train) acc: 99.48%, loss: 0.017 | (test) acc: 99.62%, loss: 0.014\n",
      "[65/100] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.63%, loss: 0.012\n",
      "[66/100] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.60%, loss: 0.010\n",
      "[67/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.69%, loss: 0.010\n",
      "[68/100] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.55%, loss: 0.015\n",
      "[69/100] (train) acc: 99.51%, loss: 0.016 | (test) acc: 99.65%, loss: 0.010\n",
      "[70/100] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.62%, loss: 0.011\n",
      "[71/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.60%, loss: 0.012\n",
      "[72/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.65%, loss: 0.011\n",
      "[73/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.69%, loss: 0.010\n",
      "[74/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.67%, loss: 0.011\n",
      "[75/100] (train) acc: 99.53%, loss: 0.016 | (test) acc: 99.61%, loss: 0.011\n",
      "[76/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.57%, loss: 0.012\n",
      "[77/100] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.65%, loss: 0.009\n",
      "[78/100] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.58%, loss: 0.012\n",
      "[79/100] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.64%, loss: 0.010\n",
      "[80/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.62%, loss: 0.010\n",
      "[81/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.67%, loss: 0.009\n",
      "[82/100] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.67%, loss: 0.012\n",
      "[83/100] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.61%, loss: 0.011\n",
      "[84/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.67%, loss: 0.010\n",
      "[85/100] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.61%, loss: 0.013\n",
      "[86/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.58%, loss: 0.012\n",
      "[87/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.67%, loss: 0.010\n",
      "[88/100] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.64%, loss: 0.009\n",
      "[89/100] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.59%, loss: 0.013\n",
      "[90/100] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.66%, loss: 0.010\n",
      "[91/100] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.67%, loss: 0.010\n",
      "[92/100] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.63%, loss: 0.012\n",
      "[93/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.63%, loss: 0.011\n",
      "[94/100] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.57%, loss: 0.012\n",
      "[95/100] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.61%, loss: 0.011\n",
      "[96/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.68%, loss: 0.011\n",
      "[97/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.60%, loss: 0.012\n",
      "[98/100] (train) acc: 99.55%, loss: 0.013 | (test) acc: 99.60%, loss: 0.013\n",
      "[99/100] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.61%, loss: 0.012\n",
      "[100/100] (train) acc: 99.52%, loss: 0.014 | (test) acc: 99.60%, loss: 0.013\n",
      "average of last 5 test acc: 99.62%\n"
     ]
    }
   ],
   "source": [
    "train_da(epoch_n=100, use_training_ph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(dense, n_class=10):\n",
    "    N = dense.shape[0]\n",
    "    ret = np.zeros([N, n_class])\n",
    "    ret[np.arange(N), dense] = 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BestModel(object):\n",
    "    def __init__(self, lr):\n",
    "        X = tf.placeholder(tf.float32, [None, 784])\n",
    "        y = tf.placeholder(tf.float32, [None, 10])\n",
    "        training = tf.placeholder(tf.bool)\n",
    "\n",
    "        net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "        n_filters = 32\n",
    "        bn_param = {'is_training': training, 'scale': True, 'decay': 0.99}\n",
    "        with slim.arg_scope([slim.conv2d], kernel_size=[3,3],\n",
    "                            normalizer_fn=slim.batch_norm, normalizer_params=bn_param):\n",
    "            for _ in range(3):\n",
    "                net = slim.conv2d(net, n_filters)\n",
    "                net = slim.conv2d(net, n_filters)\n",
    "                net = slim.max_pool2d(net, kernel_size=[2,2], padding='same')\n",
    "                net = slim.dropout(net, keep_prob=0.7, is_training=training)\n",
    "                n_filters *= 2\n",
    "\n",
    "        flat = slim.flatten(net)\n",
    "        logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "        prob = tf.nn.softmax(logits)\n",
    "\n",
    "        # add predict ops for majority voting ensemble\n",
    "        predict = tf.argmax(logits, axis=1)\n",
    "        correct = tf.equal(predict, tf.argmax(y, axis=1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "        \n",
    "        # for interaction\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.training = training\n",
    "        self.predict = predict\n",
    "        self.accuracy = accuracy\n",
    "        self.loss = loss\n",
    "        self.train_op = train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model0 ... {'lr': 0.0003, 'batch_size': 50, 'epoch_n': 80}\n",
      "[ 1/80] (train) acc: 84.67%, loss: 0.473 | (test) acc: 97.78%, loss: 0.065\n",
      "[ 2/80] (train) acc: 95.57%, loss: 0.139 | (test) acc: 98.92%, loss: 0.032\n",
      "[ 3/80] (train) acc: 96.72%, loss: 0.106 | (test) acc: 99.04%, loss: 0.029\n",
      "[ 4/80] (train) acc: 97.40%, loss: 0.083 | (test) acc: 99.32%, loss: 0.019\n",
      "[ 5/80] (train) acc: 97.75%, loss: 0.073 | (test) acc: 99.14%, loss: 0.026\n",
      "[ 6/80] (train) acc: 98.00%, loss: 0.066 | (test) acc: 99.35%, loss: 0.019\n",
      "[ 7/80] (train) acc: 98.14%, loss: 0.061 | (test) acc: 99.41%, loss: 0.018\n",
      "[ 8/80] (train) acc: 98.26%, loss: 0.056 | (test) acc: 99.43%, loss: 0.015\n",
      "[ 9/80] (train) acc: 98.32%, loss: 0.054 | (test) acc: 99.48%, loss: 0.015\n",
      "[10/80] (train) acc: 98.41%, loss: 0.051 | (test) acc: 99.56%, loss: 0.013\n",
      "[11/80] (train) acc: 98.44%, loss: 0.050 | (test) acc: 99.50%, loss: 0.017\n",
      "[12/80] (train) acc: 98.51%, loss: 0.047 | (test) acc: 99.36%, loss: 0.018\n",
      "[13/80] (train) acc: 98.62%, loss: 0.045 | (test) acc: 99.38%, loss: 0.018\n",
      "[14/80] (train) acc: 98.69%, loss: 0.042 | (test) acc: 99.42%, loss: 0.017\n",
      "[15/80] (train) acc: 98.74%, loss: 0.040 | (test) acc: 99.52%, loss: 0.016\n",
      "[16/80] (train) acc: 98.70%, loss: 0.042 | (test) acc: 99.39%, loss: 0.016\n",
      "[17/80] (train) acc: 98.85%, loss: 0.037 | (test) acc: 99.55%, loss: 0.013\n",
      "[18/80] (train) acc: 98.84%, loss: 0.037 | (test) acc: 99.47%, loss: 0.016\n",
      "[19/80] (train) acc: 98.88%, loss: 0.035 | (test) acc: 99.51%, loss: 0.013\n",
      "[20/80] (train) acc: 98.94%, loss: 0.035 | (test) acc: 99.51%, loss: 0.017\n",
      "[21/80] (train) acc: 98.88%, loss: 0.035 | (test) acc: 99.59%, loss: 0.013\n",
      "[22/80] (train) acc: 98.92%, loss: 0.034 | (test) acc: 99.50%, loss: 0.017\n",
      "[23/80] (train) acc: 98.96%, loss: 0.033 | (test) acc: 99.62%, loss: 0.012\n",
      "[24/80] (train) acc: 98.94%, loss: 0.033 | (test) acc: 99.44%, loss: 0.015\n",
      "[25/80] (train) acc: 98.96%, loss: 0.032 | (test) acc: 99.60%, loss: 0.013\n",
      "[26/80] (train) acc: 99.07%, loss: 0.030 | (test) acc: 99.51%, loss: 0.014\n",
      "[27/80] (train) acc: 99.05%, loss: 0.031 | (test) acc: 99.62%, loss: 0.013\n",
      "[28/80] (train) acc: 99.05%, loss: 0.029 | (test) acc: 99.53%, loss: 0.014\n",
      "[29/80] (train) acc: 99.04%, loss: 0.030 | (test) acc: 99.55%, loss: 0.014\n",
      "[30/80] (train) acc: 99.15%, loss: 0.027 | (test) acc: 99.59%, loss: 0.014\n",
      "[31/80] (train) acc: 99.08%, loss: 0.028 | (test) acc: 99.62%, loss: 0.012\n",
      "[32/80] (train) acc: 99.13%, loss: 0.028 | (test) acc: 99.61%, loss: 0.013\n",
      "[33/80] (train) acc: 99.16%, loss: 0.027 | (test) acc: 99.53%, loss: 0.014\n",
      "[34/80] (train) acc: 99.15%, loss: 0.028 | (test) acc: 99.60%, loss: 0.012\n",
      "[35/80] (train) acc: 99.19%, loss: 0.026 | (test) acc: 99.57%, loss: 0.014\n",
      "[36/80] (train) acc: 99.16%, loss: 0.027 | (test) acc: 99.54%, loss: 0.016\n",
      "[37/80] (train) acc: 99.22%, loss: 0.026 | (test) acc: 99.63%, loss: 0.011\n",
      "[38/80] (train) acc: 99.20%, loss: 0.026 | (test) acc: 99.57%, loss: 0.012\n",
      "[39/80] (train) acc: 99.19%, loss: 0.025 | (test) acc: 99.61%, loss: 0.013\n",
      "[40/80] (train) acc: 99.22%, loss: 0.024 | (test) acc: 99.55%, loss: 0.014\n",
      "[41/80] (train) acc: 99.25%, loss: 0.024 | (test) acc: 99.62%, loss: 0.012\n",
      "[42/80] (train) acc: 99.29%, loss: 0.023 | (test) acc: 99.57%, loss: 0.013\n",
      "[43/80] (train) acc: 99.22%, loss: 0.024 | (test) acc: 99.60%, loss: 0.013\n",
      "[44/80] (train) acc: 99.28%, loss: 0.022 | (test) acc: 99.62%, loss: 0.011\n",
      "[45/80] (train) acc: 99.25%, loss: 0.023 | (test) acc: 99.52%, loss: 0.015\n",
      "[46/80] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.58%, loss: 0.012\n",
      "[47/80] (train) acc: 99.21%, loss: 0.024 | (test) acc: 99.58%, loss: 0.012\n",
      "[48/80] (train) acc: 99.31%, loss: 0.021 | (test) acc: 99.65%, loss: 0.013\n",
      "[49/80] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.61%, loss: 0.012\n",
      "[50/80] (train) acc: 99.30%, loss: 0.022 | (test) acc: 99.62%, loss: 0.012\n",
      "[51/80] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.52%, loss: 0.012\n",
      "[52/80] (train) acc: 99.35%, loss: 0.020 | (test) acc: 99.57%, loss: 0.012\n",
      "[53/80] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.63%, loss: 0.011\n",
      "[54/80] (train) acc: 99.33%, loss: 0.020 | (test) acc: 99.45%, loss: 0.018\n",
      "[55/80] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.60%, loss: 0.012\n",
      "[56/80] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.67%, loss: 0.011\n",
      "[57/80] (train) acc: 99.35%, loss: 0.019 | (test) acc: 99.57%, loss: 0.015\n",
      "[58/80] (train) acc: 99.33%, loss: 0.020 | (test) acc: 99.65%, loss: 0.012\n",
      "[59/80] (train) acc: 99.38%, loss: 0.020 | (test) acc: 99.62%, loss: 0.012\n",
      "[60/80] (train) acc: 99.41%, loss: 0.017 | (test) acc: 99.61%, loss: 0.012\n",
      "[61/80] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.65%, loss: 0.010\n",
      "[62/80] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.65%, loss: 0.010\n",
      "[63/80] (train) acc: 99.40%, loss: 0.020 | (test) acc: 99.62%, loss: 0.013\n",
      "[64/80] (train) acc: 99.39%, loss: 0.019 | (test) acc: 99.65%, loss: 0.010\n",
      "[65/80] (train) acc: 99.39%, loss: 0.019 | (test) acc: 99.65%, loss: 0.012\n",
      "[66/80] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.62%, loss: 0.012\n",
      "[67/80] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.61%, loss: 0.012\n",
      "[68/80] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.66%, loss: 0.011\n",
      "[69/80] (train) acc: 99.42%, loss: 0.019 | (test) acc: 99.59%, loss: 0.014\n",
      "[70/80] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.64%, loss: 0.012\n",
      "[71/80] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.68%, loss: 0.011\n",
      "[72/80] (train) acc: 99.40%, loss: 0.020 | (test) acc: 99.62%, loss: 0.012\n",
      "[73/80] (train) acc: 99.44%, loss: 0.016 | (test) acc: 99.60%, loss: 0.013\n",
      "[74/80] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.63%, loss: 0.012\n",
      "[75/80] (train) acc: 99.48%, loss: 0.017 | (test) acc: 99.63%, loss: 0.010\n",
      "[76/80] (train) acc: 99.43%, loss: 0.017 | (test) acc: 99.58%, loss: 0.013\n",
      "[77/80] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.57%, loss: 0.013\n",
      "[78/80] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.70%, loss: 0.010\n",
      "[79/80] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.62%, loss: 0.011\n",
      "[80/80] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.62%, loss: 0.011\n",
      "\n",
      "Train model1 ... {'lr': 0.0007, 'batch_size': 100, 'epoch_n': 100}\n",
      "[ 1/100] (train) acc: 86.81%, loss: 0.415 | (test) acc: 96.89%, loss: 0.087\n",
      "[ 2/100] (train) acc: 96.33%, loss: 0.119 | (test) acc: 98.91%, loss: 0.032\n",
      "[ 3/100] (train) acc: 97.22%, loss: 0.090 | (test) acc: 99.33%, loss: 0.020\n",
      "[ 4/100] (train) acc: 97.61%, loss: 0.077 | (test) acc: 99.30%, loss: 0.020\n",
      "[ 5/100] (train) acc: 97.84%, loss: 0.068 | (test) acc: 99.35%, loss: 0.021\n",
      "[ 6/100] (train) acc: 97.97%, loss: 0.062 | (test) acc: 99.02%, loss: 0.030\n",
      "[ 7/100] (train) acc: 98.21%, loss: 0.057 | (test) acc: 99.16%, loss: 0.026\n",
      "[ 8/100] (train) acc: 98.28%, loss: 0.053 | (test) acc: 99.41%, loss: 0.020\n",
      "[ 9/100] (train) acc: 98.40%, loss: 0.054 | (test) acc: 99.46%, loss: 0.018\n",
      "[10/100] (train) acc: 98.54%, loss: 0.048 | (test) acc: 99.42%, loss: 0.017\n",
      "[11/100] (train) acc: 98.48%, loss: 0.049 | (test) acc: 99.33%, loss: 0.018\n",
      "[12/100] (train) acc: 98.60%, loss: 0.044 | (test) acc: 99.27%, loss: 0.022\n",
      "[13/100] (train) acc: 98.63%, loss: 0.044 | (test) acc: 99.38%, loss: 0.019\n",
      "[14/100] (train) acc: 98.70%, loss: 0.042 | (test) acc: 99.47%, loss: 0.018\n",
      "[15/100] (train) acc: 98.79%, loss: 0.039 | (test) acc: 99.41%, loss: 0.019\n",
      "[16/100] (train) acc: 98.71%, loss: 0.041 | (test) acc: 99.48%, loss: 0.015\n",
      "[17/100] (train) acc: 98.83%, loss: 0.039 | (test) acc: 99.39%, loss: 0.019\n",
      "[18/100] (train) acc: 98.92%, loss: 0.035 | (test) acc: 99.57%, loss: 0.014\n",
      "[19/100] (train) acc: 98.93%, loss: 0.034 | (test) acc: 99.44%, loss: 0.016\n",
      "[20/100] (train) acc: 98.93%, loss: 0.033 | (test) acc: 99.40%, loss: 0.018\n",
      "[21/100] (train) acc: 98.94%, loss: 0.033 | (test) acc: 99.45%, loss: 0.016\n",
      "[22/100] (train) acc: 99.06%, loss: 0.030 | (test) acc: 99.34%, loss: 0.021\n",
      "[23/100] (train) acc: 99.04%, loss: 0.030 | (test) acc: 99.56%, loss: 0.013\n",
      "[24/100] (train) acc: 98.96%, loss: 0.031 | (test) acc: 99.54%, loss: 0.013\n",
      "[25/100] (train) acc: 99.03%, loss: 0.031 | (test) acc: 99.53%, loss: 0.013\n",
      "[26/100] (train) acc: 99.09%, loss: 0.029 | (test) acc: 99.42%, loss: 0.019\n",
      "[27/100] (train) acc: 99.12%, loss: 0.028 | (test) acc: 99.55%, loss: 0.012\n",
      "[28/100] (train) acc: 99.17%, loss: 0.027 | (test) acc: 99.52%, loss: 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/100] (train) acc: 99.12%, loss: 0.028 | (test) acc: 99.61%, loss: 0.014\n",
      "[30/100] (train) acc: 99.15%, loss: 0.027 | (test) acc: 99.62%, loss: 0.011\n",
      "[31/100] (train) acc: 99.12%, loss: 0.027 | (test) acc: 99.62%, loss: 0.012\n",
      "[32/100] (train) acc: 99.23%, loss: 0.025 | (test) acc: 99.64%, loss: 0.010\n",
      "[33/100] (train) acc: 99.17%, loss: 0.025 | (test) acc: 99.59%, loss: 0.011\n",
      "[34/100] (train) acc: 99.15%, loss: 0.026 | (test) acc: 99.57%, loss: 0.014\n",
      "[35/100] (train) acc: 99.25%, loss: 0.024 | (test) acc: 99.64%, loss: 0.010\n",
      "[36/100] (train) acc: 99.23%, loss: 0.024 | (test) acc: 99.61%, loss: 0.012\n",
      "[37/100] (train) acc: 99.22%, loss: 0.024 | (test) acc: 99.63%, loss: 0.012\n",
      "[38/100] (train) acc: 99.25%, loss: 0.024 | (test) acc: 99.62%, loss: 0.012\n",
      "[39/100] (train) acc: 99.25%, loss: 0.023 | (test) acc: 99.58%, loss: 0.011\n",
      "[40/100] (train) acc: 99.27%, loss: 0.024 | (test) acc: 99.56%, loss: 0.012\n",
      "[41/100] (train) acc: 99.36%, loss: 0.022 | (test) acc: 99.68%, loss: 0.009\n",
      "[42/100] (train) acc: 99.27%, loss: 0.022 | (test) acc: 99.64%, loss: 0.012\n",
      "[43/100] (train) acc: 99.32%, loss: 0.022 | (test) acc: 99.60%, loss: 0.012\n",
      "[44/100] (train) acc: 99.31%, loss: 0.021 | (test) acc: 99.61%, loss: 0.013\n",
      "[45/100] (train) acc: 99.35%, loss: 0.020 | (test) acc: 99.61%, loss: 0.012\n",
      "[46/100] (train) acc: 99.30%, loss: 0.021 | (test) acc: 99.57%, loss: 0.013\n",
      "[47/100] (train) acc: 99.29%, loss: 0.022 | (test) acc: 99.66%, loss: 0.011\n",
      "[48/100] (train) acc: 99.32%, loss: 0.021 | (test) acc: 99.57%, loss: 0.012\n",
      "[49/100] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.60%, loss: 0.012\n",
      "[50/100] (train) acc: 99.38%, loss: 0.020 | (test) acc: 99.58%, loss: 0.012\n",
      "[51/100] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.56%, loss: 0.012\n",
      "[52/100] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.66%, loss: 0.012\n",
      "[53/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.68%, loss: 0.011\n",
      "[54/100] (train) acc: 99.39%, loss: 0.018 | (test) acc: 99.63%, loss: 0.011\n",
      "[55/100] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.60%, loss: 0.012\n",
      "[56/100] (train) acc: 99.39%, loss: 0.018 | (test) acc: 99.65%, loss: 0.012\n",
      "[57/100] (train) acc: 99.41%, loss: 0.019 | (test) acc: 99.60%, loss: 0.012\n",
      "[58/100] (train) acc: 99.41%, loss: 0.017 | (test) acc: 99.60%, loss: 0.011\n",
      "[59/100] (train) acc: 99.43%, loss: 0.019 | (test) acc: 99.59%, loss: 0.011\n",
      "[60/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.63%, loss: 0.012\n",
      "[61/100] (train) acc: 99.40%, loss: 0.018 | (test) acc: 99.72%, loss: 0.011\n",
      "[62/100] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.60%, loss: 0.011\n",
      "[63/100] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.59%, loss: 0.012\n",
      "[64/100] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.64%, loss: 0.010\n",
      "[65/100] (train) acc: 99.51%, loss: 0.016 | (test) acc: 99.67%, loss: 0.010\n",
      "[66/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.67%, loss: 0.013\n",
      "[67/100] (train) acc: 99.48%, loss: 0.017 | (test) acc: 99.66%, loss: 0.012\n",
      "[68/100] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.61%, loss: 0.011\n",
      "[69/100] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.67%, loss: 0.010\n",
      "[70/100] (train) acc: 99.51%, loss: 0.014 | (test) acc: 99.59%, loss: 0.013\n",
      "[71/100] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.69%, loss: 0.011\n",
      "[72/100] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.66%, loss: 0.011\n",
      "[73/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.67%, loss: 0.011\n",
      "[74/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.60%, loss: 0.012\n",
      "[75/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.69%, loss: 0.010\n",
      "[76/100] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.61%, loss: 0.011\n",
      "[77/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.64%, loss: 0.011\n",
      "[78/100] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.52%, loss: 0.015\n",
      "[79/100] (train) acc: 99.51%, loss: 0.016 | (test) acc: 99.65%, loss: 0.012\n",
      "[80/100] (train) acc: 99.48%, loss: 0.015 | (test) acc: 99.67%, loss: 0.011\n",
      "[81/100] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.64%, loss: 0.011\n",
      "[82/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.57%, loss: 0.012\n",
      "[83/100] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.50%, loss: 0.014\n",
      "[84/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.61%, loss: 0.013\n",
      "[85/100] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.62%, loss: 0.010\n",
      "[86/100] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.59%, loss: 0.011\n",
      "[87/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.60%, loss: 0.012\n",
      "[88/100] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.60%, loss: 0.010\n",
      "[89/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.49%, loss: 0.016\n",
      "[90/100] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.65%, loss: 0.013\n",
      "[91/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.62%, loss: 0.011\n",
      "[92/100] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.62%, loss: 0.011\n",
      "[93/100] (train) acc: 99.51%, loss: 0.014 | (test) acc: 99.62%, loss: 0.011\n",
      "[94/100] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.65%, loss: 0.010\n",
      "[95/100] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.64%, loss: 0.011\n",
      "[96/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.69%, loss: 0.011\n",
      "[97/100] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.60%, loss: 0.013\n",
      "[98/100] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.66%, loss: 0.010\n",
      "[99/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.76%, loss: 0.009\n",
      "[100/100] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.65%, loss: 0.010\n",
      "\n",
      "Train model2 ... {'lr': 0.001, 'batch_size': 100, 'epoch_n': 100}\n",
      "[ 1/100] (train) acc: 88.29%, loss: 0.368 | (test) acc: 95.10%, loss: 0.152\n",
      "[ 2/100] (train) acc: 96.48%, loss: 0.111 | (test) acc: 97.72%, loss: 0.070\n",
      "[ 3/100] (train) acc: 97.32%, loss: 0.085 | (test) acc: 99.22%, loss: 0.026\n",
      "[ 4/100] (train) acc: 97.60%, loss: 0.076 | (test) acc: 99.33%, loss: 0.020\n",
      "[ 5/100] (train) acc: 97.93%, loss: 0.068 | (test) acc: 99.23%, loss: 0.023\n",
      "[ 6/100] (train) acc: 98.14%, loss: 0.062 | (test) acc: 99.32%, loss: 0.020\n",
      "[ 7/100] (train) acc: 98.23%, loss: 0.057 | (test) acc: 98.79%, loss: 0.041\n",
      "[ 8/100] (train) acc: 98.27%, loss: 0.055 | (test) acc: 99.22%, loss: 0.022\n",
      "[ 9/100] (train) acc: 98.35%, loss: 0.052 | (test) acc: 99.50%, loss: 0.019\n",
      "[10/100] (train) acc: 98.50%, loss: 0.048 | (test) acc: 99.41%, loss: 0.019\n",
      "[11/100] (train) acc: 98.58%, loss: 0.046 | (test) acc: 99.43%, loss: 0.016\n",
      "[12/100] (train) acc: 98.64%, loss: 0.043 | (test) acc: 99.51%, loss: 0.014\n",
      "[13/100] (train) acc: 98.72%, loss: 0.041 | (test) acc: 99.57%, loss: 0.013\n",
      "[14/100] (train) acc: 98.77%, loss: 0.040 | (test) acc: 99.53%, loss: 0.014\n",
      "[15/100] (train) acc: 98.83%, loss: 0.038 | (test) acc: 99.35%, loss: 0.021\n",
      "[16/100] (train) acc: 98.92%, loss: 0.035 | (test) acc: 99.54%, loss: 0.013\n",
      "[17/100] (train) acc: 98.84%, loss: 0.036 | (test) acc: 99.42%, loss: 0.021\n",
      "[18/100] (train) acc: 98.92%, loss: 0.035 | (test) acc: 99.44%, loss: 0.017\n",
      "[19/100] (train) acc: 98.95%, loss: 0.034 | (test) acc: 99.55%, loss: 0.013\n",
      "[20/100] (train) acc: 99.00%, loss: 0.033 | (test) acc: 99.53%, loss: 0.013\n",
      "[21/100] (train) acc: 99.05%, loss: 0.032 | (test) acc: 99.55%, loss: 0.011\n",
      "[22/100] (train) acc: 99.10%, loss: 0.031 | (test) acc: 99.48%, loss: 0.013\n",
      "[23/100] (train) acc: 99.12%, loss: 0.027 | (test) acc: 99.45%, loss: 0.017\n",
      "[24/100] (train) acc: 99.11%, loss: 0.029 | (test) acc: 99.54%, loss: 0.012\n",
      "[25/100] (train) acc: 99.09%, loss: 0.029 | (test) acc: 99.57%, loss: 0.011\n",
      "[26/100] (train) acc: 99.15%, loss: 0.027 | (test) acc: 99.53%, loss: 0.013\n",
      "[27/100] (train) acc: 99.15%, loss: 0.028 | (test) acc: 99.42%, loss: 0.017\n",
      "[28/100] (train) acc: 99.21%, loss: 0.026 | (test) acc: 99.50%, loss: 0.014\n",
      "[29/100] (train) acc: 99.18%, loss: 0.028 | (test) acc: 99.46%, loss: 0.014\n",
      "[30/100] (train) acc: 99.19%, loss: 0.026 | (test) acc: 99.54%, loss: 0.014\n",
      "[31/100] (train) acc: 99.23%, loss: 0.024 | (test) acc: 99.60%, loss: 0.011\n",
      "[32/100] (train) acc: 99.24%, loss: 0.025 | (test) acc: 99.45%, loss: 0.017\n",
      "[33/100] (train) acc: 99.23%, loss: 0.025 | (test) acc: 99.51%, loss: 0.014\n",
      "[34/100] (train) acc: 99.26%, loss: 0.023 | (test) acc: 99.60%, loss: 0.012\n",
      "[35/100] (train) acc: 99.31%, loss: 0.023 | (test) acc: 99.55%, loss: 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/100] (train) acc: 99.26%, loss: 0.023 | (test) acc: 99.56%, loss: 0.014\n",
      "[37/100] (train) acc: 99.29%, loss: 0.023 | (test) acc: 99.60%, loss: 0.011\n",
      "[38/100] (train) acc: 99.31%, loss: 0.022 | (test) acc: 99.60%, loss: 0.011\n",
      "[39/100] (train) acc: 99.30%, loss: 0.023 | (test) acc: 99.61%, loss: 0.011\n",
      "[40/100] (train) acc: 99.32%, loss: 0.022 | (test) acc: 99.58%, loss: 0.014\n",
      "[41/100] (train) acc: 99.28%, loss: 0.022 | (test) acc: 99.49%, loss: 0.012\n",
      "[42/100] (train) acc: 99.35%, loss: 0.021 | (test) acc: 99.59%, loss: 0.012\n",
      "[43/100] (train) acc: 99.33%, loss: 0.022 | (test) acc: 99.62%, loss: 0.012\n",
      "[44/100] (train) acc: 99.32%, loss: 0.021 | (test) acc: 99.64%, loss: 0.012\n",
      "[45/100] (train) acc: 99.35%, loss: 0.020 | (test) acc: 99.70%, loss: 0.010\n",
      "[46/100] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.51%, loss: 0.014\n",
      "[47/100] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.57%, loss: 0.012\n",
      "[48/100] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.71%, loss: 0.008\n",
      "[49/100] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.57%, loss: 0.013\n",
      "[50/100] (train) acc: 99.39%, loss: 0.019 | (test) acc: 99.66%, loss: 0.011\n",
      "[51/100] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.53%, loss: 0.016\n",
      "[52/100] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.60%, loss: 0.013\n",
      "[53/100] (train) acc: 99.43%, loss: 0.019 | (test) acc: 99.72%, loss: 0.010\n",
      "[54/100] (train) acc: 99.42%, loss: 0.017 | (test) acc: 99.56%, loss: 0.015\n",
      "[55/100] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.53%, loss: 0.014\n",
      "[56/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.69%, loss: 0.010\n",
      "[57/100] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.56%, loss: 0.012\n",
      "[58/100] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.55%, loss: 0.013\n",
      "[59/100] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.54%, loss: 0.014\n",
      "[60/100] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.63%, loss: 0.012\n",
      "[61/100] (train) acc: 99.44%, loss: 0.016 | (test) acc: 99.57%, loss: 0.012\n",
      "[62/100] (train) acc: 99.45%, loss: 0.018 | (test) acc: 99.65%, loss: 0.010\n",
      "[63/100] (train) acc: 99.51%, loss: 0.016 | (test) acc: 99.63%, loss: 0.012\n",
      "[64/100] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.67%, loss: 0.012\n",
      "[65/100] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.68%, loss: 0.012\n",
      "[66/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.59%, loss: 0.014\n",
      "[67/100] (train) acc: 99.42%, loss: 0.017 | (test) acc: 99.66%, loss: 0.010\n",
      "[68/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.58%, loss: 0.014\n",
      "[69/100] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.63%, loss: 0.010\n",
      "[70/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.64%, loss: 0.010\n",
      "[71/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.72%, loss: 0.010\n",
      "[72/100] (train) acc: 99.51%, loss: 0.016 | (test) acc: 99.66%, loss: 0.009\n",
      "[73/100] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.59%, loss: 0.011\n",
      "[74/100] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.69%, loss: 0.010\n",
      "[75/100] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.64%, loss: 0.010\n",
      "[76/100] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.66%, loss: 0.010\n",
      "[77/100] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.61%, loss: 0.011\n",
      "[78/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.64%, loss: 0.010\n",
      "[79/100] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.63%, loss: 0.012\n",
      "[80/100] (train) acc: 99.48%, loss: 0.015 | (test) acc: 99.61%, loss: 0.011\n",
      "[81/100] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.67%, loss: 0.011\n",
      "[82/100] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.69%, loss: 0.010\n",
      "[83/100] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.68%, loss: 0.009\n",
      "[84/100] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.64%, loss: 0.010\n",
      "[85/100] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.67%, loss: 0.011\n",
      "[86/100] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.64%, loss: 0.012\n",
      "[87/100] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.68%, loss: 0.011\n",
      "[88/100] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.70%, loss: 0.010\n",
      "[89/100] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.58%, loss: 0.013\n",
      "[90/100] (train) acc: 99.52%, loss: 0.014 | (test) acc: 99.64%, loss: 0.011\n",
      "[91/100] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.63%, loss: 0.010\n",
      "[92/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.66%, loss: 0.011\n",
      "[93/100] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.60%, loss: 0.011\n",
      "[94/100] (train) acc: 99.55%, loss: 0.013 | (test) acc: 99.66%, loss: 0.011\n",
      "[95/100] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.71%, loss: 0.010\n",
      "[96/100] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.67%, loss: 0.010\n",
      "[97/100] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.69%, loss: 0.010\n",
      "[98/100] (train) acc: 99.55%, loss: 0.013 | (test) acc: 99.66%, loss: 0.012\n",
      "[99/100] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.57%, loss: 0.012\n",
      "[100/100] (train) acc: 99.55%, loss: 0.013 | (test) acc: 99.67%, loss: 0.012\n",
      "\n",
      "Train model3 ... {'lr': 0.002, 'batch_size': 200, 'epoch_n': 120}\n",
      "[ 1/120] (train) acc: 86.33%, loss: 0.473 | (test) acc: 9.90%, loss: 6.214\n",
      "[ 2/120] (train) acc: 96.57%, loss: 0.110 | (test) acc: 98.84%, loss: 0.035\n",
      "[ 3/120] (train) acc: 97.39%, loss: 0.084 | (test) acc: 98.86%, loss: 0.033\n",
      "[ 4/120] (train) acc: 97.81%, loss: 0.072 | (test) acc: 99.13%, loss: 0.024\n",
      "[ 5/120] (train) acc: 98.00%, loss: 0.066 | (test) acc: 99.07%, loss: 0.027\n",
      "[ 6/120] (train) acc: 98.13%, loss: 0.061 | (test) acc: 99.38%, loss: 0.019\n",
      "[ 7/120] (train) acc: 98.27%, loss: 0.055 | (test) acc: 99.25%, loss: 0.025\n",
      "[ 8/120] (train) acc: 98.39%, loss: 0.053 | (test) acc: 99.35%, loss: 0.021\n",
      "[ 9/120] (train) acc: 98.54%, loss: 0.048 | (test) acc: 99.45%, loss: 0.016\n",
      "[10/120] (train) acc: 98.59%, loss: 0.046 | (test) acc: 99.42%, loss: 0.020\n",
      "[11/120] (train) acc: 98.56%, loss: 0.046 | (test) acc: 99.50%, loss: 0.015\n",
      "[12/120] (train) acc: 98.67%, loss: 0.044 | (test) acc: 99.43%, loss: 0.015\n",
      "[13/120] (train) acc: 98.75%, loss: 0.041 | (test) acc: 99.53%, loss: 0.016\n",
      "[14/120] (train) acc: 98.83%, loss: 0.038 | (test) acc: 99.52%, loss: 0.016\n",
      "[15/120] (train) acc: 98.81%, loss: 0.038 | (test) acc: 99.25%, loss: 0.022\n",
      "[16/120] (train) acc: 98.83%, loss: 0.037 | (test) acc: 99.55%, loss: 0.014\n",
      "[17/120] (train) acc: 98.97%, loss: 0.035 | (test) acc: 99.55%, loss: 0.013\n",
      "[18/120] (train) acc: 98.88%, loss: 0.035 | (test) acc: 99.41%, loss: 0.019\n",
      "[19/120] (train) acc: 98.89%, loss: 0.035 | (test) acc: 99.38%, loss: 0.020\n",
      "[20/120] (train) acc: 98.99%, loss: 0.032 | (test) acc: 99.42%, loss: 0.018\n",
      "[21/120] (train) acc: 98.96%, loss: 0.034 | (test) acc: 99.48%, loss: 0.015\n",
      "[22/120] (train) acc: 99.04%, loss: 0.030 | (test) acc: 99.51%, loss: 0.014\n",
      "[23/120] (train) acc: 98.97%, loss: 0.032 | (test) acc: 99.23%, loss: 0.025\n",
      "[24/120] (train) acc: 99.10%, loss: 0.030 | (test) acc: 99.57%, loss: 0.015\n",
      "[25/120] (train) acc: 99.06%, loss: 0.030 | (test) acc: 99.36%, loss: 0.018\n",
      "[26/120] (train) acc: 99.04%, loss: 0.029 | (test) acc: 99.53%, loss: 0.017\n",
      "[27/120] (train) acc: 99.07%, loss: 0.030 | (test) acc: 99.48%, loss: 0.015\n",
      "[28/120] (train) acc: 99.15%, loss: 0.027 | (test) acc: 99.53%, loss: 0.014\n",
      "[29/120] (train) acc: 99.18%, loss: 0.026 | (test) acc: 99.44%, loss: 0.017\n",
      "[30/120] (train) acc: 99.19%, loss: 0.026 | (test) acc: 99.43%, loss: 0.018\n",
      "[31/120] (train) acc: 99.17%, loss: 0.026 | (test) acc: 99.67%, loss: 0.011\n",
      "[32/120] (train) acc: 99.22%, loss: 0.025 | (test) acc: 99.64%, loss: 0.011\n",
      "[33/120] (train) acc: 99.22%, loss: 0.024 | (test) acc: 99.65%, loss: 0.011\n",
      "[34/120] (train) acc: 99.25%, loss: 0.025 | (test) acc: 99.67%, loss: 0.011\n",
      "[35/120] (train) acc: 99.25%, loss: 0.023 | (test) acc: 99.50%, loss: 0.017\n",
      "[36/120] (train) acc: 99.22%, loss: 0.025 | (test) acc: 99.51%, loss: 0.015\n",
      "[37/120] (train) acc: 99.24%, loss: 0.024 | (test) acc: 99.50%, loss: 0.015\n",
      "[38/120] (train) acc: 99.25%, loss: 0.023 | (test) acc: 99.59%, loss: 0.014\n",
      "[39/120] (train) acc: 99.28%, loss: 0.023 | (test) acc: 99.61%, loss: 0.013\n",
      "[40/120] (train) acc: 99.25%, loss: 0.023 | (test) acc: 99.60%, loss: 0.014\n",
      "[41/120] (train) acc: 99.27%, loss: 0.023 | (test) acc: 99.67%, loss: 0.011\n",
      "[42/120] (train) acc: 99.31%, loss: 0.021 | (test) acc: 99.54%, loss: 0.013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43/120] (train) acc: 99.30%, loss: 0.024 | (test) acc: 99.60%, loss: 0.014\n",
      "[44/120] (train) acc: 99.34%, loss: 0.021 | (test) acc: 99.38%, loss: 0.018\n",
      "[45/120] (train) acc: 99.37%, loss: 0.021 | (test) acc: 99.57%, loss: 0.014\n",
      "[46/120] (train) acc: 99.25%, loss: 0.023 | (test) acc: 99.60%, loss: 0.013\n",
      "[47/120] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.55%, loss: 0.016\n",
      "[48/120] (train) acc: 99.28%, loss: 0.022 | (test) acc: 99.41%, loss: 0.020\n",
      "[49/120] (train) acc: 99.33%, loss: 0.022 | (test) acc: 99.71%, loss: 0.010\n",
      "[50/120] (train) acc: 99.37%, loss: 0.019 | (test) acc: 99.52%, loss: 0.018\n",
      "[51/120] (train) acc: 99.33%, loss: 0.020 | (test) acc: 99.54%, loss: 0.015\n",
      "[52/120] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.59%, loss: 0.015\n",
      "[53/120] (train) acc: 99.33%, loss: 0.021 | (test) acc: 99.56%, loss: 0.016\n",
      "[54/120] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.61%, loss: 0.012\n",
      "[55/120] (train) acc: 99.34%, loss: 0.020 | (test) acc: 99.67%, loss: 0.010\n",
      "[56/120] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.46%, loss: 0.015\n",
      "[57/120] (train) acc: 99.39%, loss: 0.019 | (test) acc: 99.53%, loss: 0.013\n",
      "[58/120] (train) acc: 99.38%, loss: 0.019 | (test) acc: 99.61%, loss: 0.012\n",
      "[59/120] (train) acc: 99.45%, loss: 0.018 | (test) acc: 99.51%, loss: 0.014\n",
      "[60/120] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.59%, loss: 0.014\n",
      "[61/120] (train) acc: 99.36%, loss: 0.019 | (test) acc: 99.60%, loss: 0.011\n",
      "[62/120] (train) acc: 99.45%, loss: 0.018 | (test) acc: 99.57%, loss: 0.012\n",
      "[63/120] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.61%, loss: 0.012\n",
      "[64/120] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.55%, loss: 0.015\n",
      "[65/120] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.62%, loss: 0.011\n",
      "[66/120] (train) acc: 99.42%, loss: 0.017 | (test) acc: 99.66%, loss: 0.011\n",
      "[67/120] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.63%, loss: 0.012\n",
      "[68/120] (train) acc: 99.40%, loss: 0.018 | (test) acc: 99.61%, loss: 0.013\n",
      "[69/120] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.58%, loss: 0.015\n",
      "[70/120] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.56%, loss: 0.015\n",
      "[71/120] (train) acc: 99.42%, loss: 0.019 | (test) acc: 99.62%, loss: 0.011\n",
      "[72/120] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.59%, loss: 0.010\n",
      "[73/120] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.58%, loss: 0.013\n",
      "[74/120] (train) acc: 99.42%, loss: 0.017 | (test) acc: 99.58%, loss: 0.011\n",
      "[75/120] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.62%, loss: 0.009\n",
      "[76/120] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.58%, loss: 0.014\n",
      "[77/120] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.61%, loss: 0.012\n",
      "[78/120] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.60%, loss: 0.011\n",
      "[79/120] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.60%, loss: 0.012\n",
      "[80/120] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.67%, loss: 0.012\n",
      "[81/120] (train) acc: 99.51%, loss: 0.014 | (test) acc: 99.65%, loss: 0.012\n",
      "[82/120] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.66%, loss: 0.010\n",
      "[83/120] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.65%, loss: 0.012\n",
      "[84/120] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.57%, loss: 0.011\n",
      "[85/120] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.66%, loss: 0.011\n",
      "[86/120] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.61%, loss: 0.012\n",
      "[87/120] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.53%, loss: 0.015\n",
      "[88/120] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.66%, loss: 0.013\n",
      "[89/120] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.77%, loss: 0.009\n",
      "[90/120] (train) acc: 99.53%, loss: 0.016 | (test) acc: 99.68%, loss: 0.011\n",
      "[91/120] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.68%, loss: 0.010\n",
      "[92/120] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.66%, loss: 0.011\n",
      "[93/120] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.67%, loss: 0.011\n",
      "[94/120] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.66%, loss: 0.010\n",
      "[95/120] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.66%, loss: 0.011\n",
      "[96/120] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.61%, loss: 0.010\n",
      "[97/120] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.61%, loss: 0.010\n",
      "[98/120] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.64%, loss: 0.009\n",
      "[99/120] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.66%, loss: 0.011\n",
      "[100/120] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.66%, loss: 0.013\n",
      "[101/120] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.67%, loss: 0.010\n",
      "[102/120] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.67%, loss: 0.011\n",
      "[103/120] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.63%, loss: 0.013\n",
      "[104/120] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.65%, loss: 0.013\n",
      "[105/120] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.66%, loss: 0.010\n",
      "[106/120] (train) acc: 99.55%, loss: 0.013 | (test) acc: 99.69%, loss: 0.010\n",
      "[107/120] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.64%, loss: 0.011\n",
      "[108/120] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.72%, loss: 0.010\n",
      "[109/120] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.73%, loss: 0.010\n",
      "[110/120] (train) acc: 99.59%, loss: 0.012 | (test) acc: 99.69%, loss: 0.010\n",
      "[111/120] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.65%, loss: 0.012\n",
      "[112/120] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.64%, loss: 0.013\n",
      "[113/120] (train) acc: 99.58%, loss: 0.012 | (test) acc: 99.62%, loss: 0.013\n",
      "[114/120] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.69%, loss: 0.009\n",
      "[115/120] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.64%, loss: 0.010\n",
      "[116/120] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.66%, loss: 0.010\n",
      "[117/120] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.66%, loss: 0.011\n",
      "[118/120] (train) acc: 99.60%, loss: 0.012 | (test) acc: 99.63%, loss: 0.011\n",
      "[119/120] (train) acc: 99.61%, loss: 0.012 | (test) acc: 99.65%, loss: 0.012\n",
      "[120/120] (train) acc: 99.62%, loss: 0.012 | (test) acc: 99.66%, loss: 0.010\n",
      "\n",
      "Train model4 ... {'lr': 0.003, 'batch_size': 300, 'epoch_n': 150}\n",
      "[ 1/150] (train) acc: 81.73%, loss: 0.654 | (test) acc: 11.35%, loss: 6.597\n",
      "[ 2/150] (train) acc: 96.02%, loss: 0.128 | (test) acc: 88.42%, loss: 0.362\n",
      "[ 3/150] (train) acc: 97.12%, loss: 0.093 | (test) acc: 98.61%, loss: 0.041\n",
      "[ 4/150] (train) acc: 97.54%, loss: 0.080 | (test) acc: 98.88%, loss: 0.031\n",
      "[ 5/150] (train) acc: 97.92%, loss: 0.069 | (test) acc: 98.94%, loss: 0.029\n",
      "[ 6/150] (train) acc: 98.08%, loss: 0.061 | (test) acc: 99.37%, loss: 0.018\n",
      "[ 7/150] (train) acc: 98.13%, loss: 0.061 | (test) acc: 99.20%, loss: 0.022\n",
      "[ 8/150] (train) acc: 98.36%, loss: 0.053 | (test) acc: 99.14%, loss: 0.024\n",
      "[ 9/150] (train) acc: 98.43%, loss: 0.051 | (test) acc: 99.50%, loss: 0.015\n",
      "[10/150] (train) acc: 98.54%, loss: 0.048 | (test) acc: 99.44%, loss: 0.017\n",
      "[11/150] (train) acc: 98.59%, loss: 0.045 | (test) acc: 99.51%, loss: 0.015\n",
      "[12/150] (train) acc: 98.64%, loss: 0.044 | (test) acc: 99.38%, loss: 0.017\n",
      "[13/150] (train) acc: 98.68%, loss: 0.042 | (test) acc: 99.46%, loss: 0.017\n",
      "[14/150] (train) acc: 98.71%, loss: 0.041 | (test) acc: 99.51%, loss: 0.015\n",
      "[15/150] (train) acc: 98.78%, loss: 0.039 | (test) acc: 99.51%, loss: 0.014\n",
      "[16/150] (train) acc: 98.78%, loss: 0.039 | (test) acc: 99.56%, loss: 0.014\n",
      "[17/150] (train) acc: 98.87%, loss: 0.034 | (test) acc: 99.31%, loss: 0.020\n",
      "[18/150] (train) acc: 98.88%, loss: 0.036 | (test) acc: 99.45%, loss: 0.015\n",
      "[19/150] (train) acc: 98.97%, loss: 0.032 | (test) acc: 99.35%, loss: 0.021\n",
      "[20/150] (train) acc: 98.98%, loss: 0.032 | (test) acc: 99.42%, loss: 0.017\n",
      "[21/150] (train) acc: 98.99%, loss: 0.034 | (test) acc: 99.60%, loss: 0.014\n",
      "[22/150] (train) acc: 98.97%, loss: 0.033 | (test) acc: 99.51%, loss: 0.014\n",
      "[23/150] (train) acc: 99.08%, loss: 0.031 | (test) acc: 99.53%, loss: 0.014\n",
      "[24/150] (train) acc: 99.07%, loss: 0.031 | (test) acc: 99.41%, loss: 0.019\n",
      "[25/150] (train) acc: 99.01%, loss: 0.030 | (test) acc: 99.52%, loss: 0.015\n",
      "[26/150] (train) acc: 99.03%, loss: 0.032 | (test) acc: 99.49%, loss: 0.017\n",
      "[27/150] (train) acc: 99.13%, loss: 0.028 | (test) acc: 99.46%, loss: 0.015\n",
      "[28/150] (train) acc: 99.03%, loss: 0.031 | (test) acc: 99.64%, loss: 0.011\n",
      "[29/150] (train) acc: 99.18%, loss: 0.028 | (test) acc: 99.50%, loss: 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/150] (train) acc: 99.10%, loss: 0.029 | (test) acc: 99.43%, loss: 0.016\n",
      "[31/150] (train) acc: 99.05%, loss: 0.029 | (test) acc: 99.57%, loss: 0.014\n",
      "[32/150] (train) acc: 99.13%, loss: 0.028 | (test) acc: 99.53%, loss: 0.016\n",
      "[33/150] (train) acc: 99.20%, loss: 0.025 | (test) acc: 99.55%, loss: 0.015\n",
      "[34/150] (train) acc: 99.22%, loss: 0.025 | (test) acc: 99.45%, loss: 0.021\n",
      "[35/150] (train) acc: 99.13%, loss: 0.027 | (test) acc: 99.59%, loss: 0.012\n",
      "[36/150] (train) acc: 99.13%, loss: 0.027 | (test) acc: 99.58%, loss: 0.013\n",
      "[37/150] (train) acc: 99.15%, loss: 0.026 | (test) acc: 99.57%, loss: 0.014\n",
      "[38/150] (train) acc: 99.14%, loss: 0.026 | (test) acc: 99.39%, loss: 0.021\n",
      "[39/150] (train) acc: 99.27%, loss: 0.024 | (test) acc: 99.61%, loss: 0.011\n",
      "[40/150] (train) acc: 99.27%, loss: 0.023 | (test) acc: 99.60%, loss: 0.015\n",
      "[41/150] (train) acc: 99.20%, loss: 0.025 | (test) acc: 99.59%, loss: 0.013\n",
      "[42/150] (train) acc: 99.27%, loss: 0.024 | (test) acc: 99.67%, loss: 0.011\n",
      "[43/150] (train) acc: 99.27%, loss: 0.023 | (test) acc: 99.58%, loss: 0.013\n",
      "[44/150] (train) acc: 99.23%, loss: 0.023 | (test) acc: 99.59%, loss: 0.013\n",
      "[45/150] (train) acc: 99.24%, loss: 0.024 | (test) acc: 99.59%, loss: 0.013\n",
      "[46/150] (train) acc: 99.24%, loss: 0.024 | (test) acc: 99.64%, loss: 0.012\n",
      "[47/150] (train) acc: 99.24%, loss: 0.024 | (test) acc: 99.65%, loss: 0.011\n",
      "[48/150] (train) acc: 99.28%, loss: 0.023 | (test) acc: 99.55%, loss: 0.015\n",
      "[49/150] (train) acc: 99.34%, loss: 0.021 | (test) acc: 99.61%, loss: 0.013\n",
      "[50/150] (train) acc: 99.28%, loss: 0.022 | (test) acc: 99.69%, loss: 0.012\n",
      "[51/150] (train) acc: 99.36%, loss: 0.021 | (test) acc: 99.63%, loss: 0.012\n",
      "[52/150] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.63%, loss: 0.014\n",
      "[53/150] (train) acc: 99.34%, loss: 0.022 | (test) acc: 99.62%, loss: 0.011\n",
      "[54/150] (train) acc: 99.31%, loss: 0.022 | (test) acc: 99.41%, loss: 0.018\n",
      "[55/150] (train) acc: 99.34%, loss: 0.020 | (test) acc: 99.65%, loss: 0.012\n",
      "[56/150] (train) acc: 99.32%, loss: 0.021 | (test) acc: 99.55%, loss: 0.013\n",
      "[57/150] (train) acc: 99.31%, loss: 0.021 | (test) acc: 99.67%, loss: 0.011\n",
      "[58/150] (train) acc: 99.32%, loss: 0.021 | (test) acc: 99.63%, loss: 0.015\n",
      "[59/150] (train) acc: 99.39%, loss: 0.019 | (test) acc: 99.62%, loss: 0.013\n",
      "[60/150] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.62%, loss: 0.011\n",
      "[61/150] (train) acc: 99.34%, loss: 0.020 | (test) acc: 99.62%, loss: 0.012\n",
      "[62/150] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.66%, loss: 0.011\n",
      "[63/150] (train) acc: 99.42%, loss: 0.019 | (test) acc: 99.65%, loss: 0.012\n",
      "[64/150] (train) acc: 99.37%, loss: 0.020 | (test) acc: 99.57%, loss: 0.013\n",
      "[65/150] (train) acc: 99.43%, loss: 0.019 | (test) acc: 99.66%, loss: 0.012\n",
      "[66/150] (train) acc: 99.42%, loss: 0.019 | (test) acc: 99.66%, loss: 0.012\n",
      "[67/150] (train) acc: 99.44%, loss: 0.018 | (test) acc: 99.63%, loss: 0.013\n",
      "[68/150] (train) acc: 99.41%, loss: 0.019 | (test) acc: 99.61%, loss: 0.014\n",
      "[69/150] (train) acc: 99.39%, loss: 0.018 | (test) acc: 99.67%, loss: 0.013\n",
      "[70/150] (train) acc: 99.40%, loss: 0.018 | (test) acc: 99.53%, loss: 0.014\n",
      "[71/150] (train) acc: 99.40%, loss: 0.018 | (test) acc: 99.57%, loss: 0.014\n",
      "[72/150] (train) acc: 99.34%, loss: 0.020 | (test) acc: 99.60%, loss: 0.015\n",
      "[73/150] (train) acc: 99.40%, loss: 0.018 | (test) acc: 99.53%, loss: 0.015\n",
      "[74/150] (train) acc: 99.44%, loss: 0.017 | (test) acc: 99.64%, loss: 0.013\n",
      "[75/150] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.66%, loss: 0.012\n",
      "[76/150] (train) acc: 99.52%, loss: 0.016 | (test) acc: 99.62%, loss: 0.014\n",
      "[77/150] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.70%, loss: 0.011\n",
      "[78/150] (train) acc: 99.43%, loss: 0.018 | (test) acc: 99.51%, loss: 0.015\n",
      "[79/150] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.47%, loss: 0.016\n",
      "[80/150] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.58%, loss: 0.014\n",
      "[81/150] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.53%, loss: 0.014\n",
      "[82/150] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.66%, loss: 0.013\n",
      "[83/150] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.56%, loss: 0.016\n",
      "[84/150] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.57%, loss: 0.014\n",
      "[85/150] (train) acc: 99.46%, loss: 0.017 | (test) acc: 99.63%, loss: 0.012\n",
      "[86/150] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.63%, loss: 0.012\n",
      "[87/150] (train) acc: 99.48%, loss: 0.017 | (test) acc: 99.62%, loss: 0.012\n",
      "[88/150] (train) acc: 99.51%, loss: 0.016 | (test) acc: 99.65%, loss: 0.012\n",
      "[89/150] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.60%, loss: 0.011\n",
      "[90/150] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.61%, loss: 0.012\n",
      "[91/150] (train) acc: 99.47%, loss: 0.016 | (test) acc: 99.63%, loss: 0.013\n",
      "[92/150] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.68%, loss: 0.011\n",
      "[93/150] (train) acc: 99.48%, loss: 0.016 | (test) acc: 99.60%, loss: 0.012\n",
      "[94/150] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.64%, loss: 0.010\n",
      "[95/150] (train) acc: 99.46%, loss: 0.016 | (test) acc: 99.67%, loss: 0.010\n",
      "[96/150] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.57%, loss: 0.015\n",
      "[97/150] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.69%, loss: 0.012\n",
      "[98/150] (train) acc: 99.50%, loss: 0.015 | (test) acc: 99.57%, loss: 0.014\n",
      "[99/150] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.68%, loss: 0.011\n",
      "[100/150] (train) acc: 99.49%, loss: 0.016 | (test) acc: 99.57%, loss: 0.013\n",
      "[101/150] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.69%, loss: 0.009\n",
      "[102/150] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.72%, loss: 0.010\n",
      "[103/150] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.71%, loss: 0.010\n",
      "[104/150] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.60%, loss: 0.013\n",
      "[105/150] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.69%, loss: 0.011\n",
      "[106/150] (train) acc: 99.49%, loss: 0.015 | (test) acc: 99.63%, loss: 0.013\n",
      "[107/150] (train) acc: 99.51%, loss: 0.015 | (test) acc: 99.72%, loss: 0.009\n",
      "[108/150] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.65%, loss: 0.012\n",
      "[109/150] (train) acc: 99.52%, loss: 0.014 | (test) acc: 99.70%, loss: 0.011\n",
      "[110/150] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.66%, loss: 0.010\n",
      "[111/150] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.58%, loss: 0.013\n",
      "[112/150] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.66%, loss: 0.011\n",
      "[113/150] (train) acc: 99.51%, loss: 0.014 | (test) acc: 99.62%, loss: 0.012\n",
      "[114/150] (train) acc: 99.58%, loss: 0.014 | (test) acc: 99.62%, loss: 0.014\n",
      "[115/150] (train) acc: 99.52%, loss: 0.014 | (test) acc: 99.63%, loss: 0.012\n",
      "[116/150] (train) acc: 99.52%, loss: 0.015 | (test) acc: 99.54%, loss: 0.016\n",
      "[117/150] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.64%, loss: 0.013\n",
      "[118/150] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.66%, loss: 0.012\n",
      "[119/150] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.60%, loss: 0.014\n",
      "[120/150] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.67%, loss: 0.012\n",
      "[121/150] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.69%, loss: 0.010\n",
      "[122/150] (train) acc: 99.58%, loss: 0.014 | (test) acc: 99.63%, loss: 0.010\n",
      "[123/150] (train) acc: 99.53%, loss: 0.014 | (test) acc: 99.61%, loss: 0.012\n",
      "[124/150] (train) acc: 99.59%, loss: 0.014 | (test) acc: 99.67%, loss: 0.011\n",
      "[125/150] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.59%, loss: 0.014\n",
      "[126/150] (train) acc: 99.54%, loss: 0.013 | (test) acc: 99.67%, loss: 0.011\n",
      "[127/150] (train) acc: 99.50%, loss: 0.016 | (test) acc: 99.60%, loss: 0.011\n",
      "[128/150] (train) acc: 99.53%, loss: 0.015 | (test) acc: 99.57%, loss: 0.012\n",
      "[129/150] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.60%, loss: 0.013\n",
      "[130/150] (train) acc: 99.58%, loss: 0.014 | (test) acc: 99.59%, loss: 0.015\n",
      "[131/150] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.59%, loss: 0.015\n",
      "[132/150] (train) acc: 99.54%, loss: 0.014 | (test) acc: 99.69%, loss: 0.013\n",
      "[133/150] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.63%, loss: 0.012\n",
      "[134/150] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.61%, loss: 0.014\n",
      "[135/150] (train) acc: 99.62%, loss: 0.012 | (test) acc: 99.62%, loss: 0.014\n",
      "[136/150] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.67%, loss: 0.012\n",
      "[137/150] (train) acc: 99.62%, loss: 0.012 | (test) acc: 99.70%, loss: 0.013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138/150] (train) acc: 99.64%, loss: 0.012 | (test) acc: 99.66%, loss: 0.013\n",
      "[139/150] (train) acc: 99.55%, loss: 0.013 | (test) acc: 99.57%, loss: 0.013\n",
      "[140/150] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.61%, loss: 0.012\n",
      "[141/150] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.63%, loss: 0.013\n",
      "[142/150] (train) acc: 99.57%, loss: 0.013 | (test) acc: 99.56%, loss: 0.015\n",
      "[143/150] (train) acc: 99.60%, loss: 0.013 | (test) acc: 99.65%, loss: 0.013\n",
      "[144/150] (train) acc: 99.55%, loss: 0.014 | (test) acc: 99.64%, loss: 0.013\n",
      "[145/150] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.64%, loss: 0.013\n",
      "[146/150] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.66%, loss: 0.013\n",
      "[147/150] (train) acc: 99.60%, loss: 0.013 | (test) acc: 99.73%, loss: 0.011\n",
      "[148/150] (train) acc: 99.60%, loss: 0.011 | (test) acc: 99.72%, loss: 0.011\n",
      "[149/150] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.63%, loss: 0.012\n",
      "[150/150] (train) acc: 99.56%, loss: 0.013 | (test) acc: 99.65%, loss: 0.012\n",
      "\n",
      "last test accs for each model\n",
      "model0: 99.62%\n",
      "model1: 99.65%\n",
      "model2: 99.67%\n",
      "model3: 99.66%\n",
      "model4: 99.65%\n",
      "ensemble: 99.71%\n"
     ]
    }
   ],
   "source": [
    "N = mnist.train.num_examples\n",
    "dq = collections.deque()\n",
    "datagen=AffineGenerator(mnist)\n",
    "\n",
    "lrs = [0.0003, 0.0007, 0.001, 0.002, 0.003]\n",
    "configs = [\n",
    "    {'lr': 0.0003, 'batch_size':  50, 'epoch_n':  80},\n",
    "    {'lr': 0.0007, 'batch_size': 100, 'epoch_n': 100},\n",
    "    {'lr':  0.001, 'batch_size': 100, 'epoch_n': 100},\n",
    "    {'lr':  0.002, 'batch_size': 200, 'epoch_n': 120},\n",
    "    {'lr':  0.003, 'batch_size': 300, 'epoch_n': 150}\n",
    "]\n",
    "\n",
    "pred_vote = np.zeros([mnist.test.num_examples, 10])\n",
    "for i, cfg in enumerate(configs):\n",
    "    tf.reset_default_graph()\n",
    "    model = BestModel(cfg['lr'])\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Train model{} ... {}\".format(i, cfg))\n",
    "        \n",
    "        for epoch in range(cfg['epoch_n']):\n",
    "            avg_loss = 0.\n",
    "            avg_acc = 0.\n",
    "\n",
    "            n_iter = 0\n",
    "            for batch_x, batch_y in datagen.generate(batch_size=cfg['batch_size']):\n",
    "                feed_dict = {model.X: batch_x, model.y: batch_y, model.training: True}\n",
    "                _, cur_acc, cur_loss = sess.run([model.train_op, model.accuracy, model.loss], feed_dict=feed_dict)\n",
    "                avg_acc += cur_acc\n",
    "                avg_loss += cur_loss\n",
    "                n_iter += 1\n",
    "\n",
    "            avg_acc /= n_iter\n",
    "            avg_loss /= n_iter\n",
    "\n",
    "            feed_dict = {model.X: mnist.test.images, model.y: mnist.test.labels, model.training: False}\n",
    "            test_acc, test_loss = sess.run([model.accuracy, model.loss], feed_dict=feed_dict)\n",
    "            \n",
    "            print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f}\".\n",
    "                  format(epoch+1, cfg['epoch_n'], avg_acc, avg_loss, test_acc, test_loss))\n",
    "    \n",
    "        # use last model for ensemble\n",
    "        feed_dict = {model.X: mnist.test.images, model.y: mnist.test.labels, model.training: False}\n",
    "        test_acc, test_pred = sess.run([model.accuracy, model.predict], feed_dict=feed_dict)\n",
    "        dq.append(test_acc)\n",
    "        pred_vote += one_hot(test_pred)\n",
    "        print\n",
    "\n",
    "print(\"last test accs for each model\")\n",
    "for i, acc in enumerate(dq):\n",
    "    print(\"model{}: {:.2%}\".format(i, acc))\n",
    "\n",
    "ensemble_acc = np.average(np.argmax(mnist.test.labels, axis=1) == np.argmax(pred_vote, axis=1))\n",
    "print(\"ensemble: {:.2%}\".format(ensemble_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check wrong predictions of ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(images, labels, predicted):\n",
    "    l = len(images)\n",
    "    height = (l+9) // 10\n",
    "    fig = plt.figure(figsize=(10,height+1))\n",
    "    gs = gridspec.GridSpec(height,10)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    corrects = np.argmax(labels, axis=1) # one-hot => single\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_title(\"{} ({})\".format(corrects[i], predicted[i]))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(image.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = np.argmax(pred_vote, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = np.argmax(mnist.test.labels, axis=1) == predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrongs = np.argwhere(correct == False).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAADqCAYAAAChtUGUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngyHDMAzDMBqa/wcSV3KxRbwcBwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb873c4f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(mnist.test.images[wrongs], mnist.test.labels[wrongs], predicted[wrongs]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  2.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,  0.],\n",
       "       [ 0.,  3.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  2.],\n",
       "       [ 0.,  0.,  4.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  4.,  0.,  0.,  0.],\n",
       "       [ 0.,  5.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  4.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  2.,  0.],\n",
       "       [ 0.,  0.,  2.,  0.,  0.,  0.,  0.,  3.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  5.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  3.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  4.],\n",
       "       [ 0.,  0.,  1.,  3.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  4.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  3.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  3.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  4.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  2.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  3.,  0.,  2.,  0.,  0.,  0.,  0.],\n",
       "       [ 5.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 4.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  2.,  3.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  4.,  0.,  0.],\n",
       "       [ 0.,  0.,  3.,  0.,  0.,  0.,  0.,  2.,  0.,  0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_vote[wrongs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - tf.latest",
   "language": "python",
   "name": "python2-tf-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
