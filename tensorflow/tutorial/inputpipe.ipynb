{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-grained TFRecords\n",
    "\n",
    "* tfrecords 를 처음에 하나로 만들었는데 그러면 안 됨.\n",
    "\n",
    "## Experiment\n",
    "\n",
    "* 클래스별로 하나씩 만들어보고, `batch_shuffle` 과 `batch_shuffle_join` 의 차이점을 확인해보자.\n",
    "* 클래스별로 하나씩 만들어도 충분한 지 확인하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry!\n",
    "\n",
    "* http://coolingoff.tistory.com/23\n",
    "* 이걸 참고해서 해 보자.\n",
    "* 그냥 reading data 번역인 거 같긴 한데... 그것도 참고하고.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concolusion\n",
    "\n",
    "Setting: 각 클래스별 tfrecords 파일 하나\n",
    "\n",
    "* train data\n",
    "    * 트레이닝 데이터는 랜덤하게 셔플해서 받을 수 있어야 함\n",
    "    * 그러나 shuffle_batch 는 tfrecords 파일 하나에서 받아오므로 제대로 셔플이 안 됨 (한 클래스에 대해서 읽어온 후 셔플)\n",
    "    * 따라서 shuffle_batch_join 을 써야 하고 batch_join 을 써도 다양한 클래스에서 읽어오기는 함\n",
    "        * 이렇게 되면 read_thread 수를 클래스 수 이상으로 해 줘야 할 듯\n",
    "        * ImageNet 같이 데이터/클래스 전부 엄청 많으면 어떻게 해야 하지?\n",
    "* test data\n",
    "    * 테스트 데이터는 5개의 tfrecords 를 통째로 읽어와야 함\n",
    "    * 생각해보면 num_epoch 을 사용하면 컨트롤 할 수 있을 것 같은데?\n",
    "    * => 실험결과 된다!\n",
    "    \n",
    "## 결론 of 결론\n",
    "    \n",
    "**`shuffle_batch_join` + num_epochs 써라!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/irteam/anaconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url, dest=None):\n",
    "    if not dest:\n",
    "        dest = 'data/' + url.split('/')[-1]\n",
    "    if sys.version_info[0] == 3:\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "    else:\n",
    "        urllib.urlretrieve(url, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TF Flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/flower_photos\"):\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.mkdir(\"data\")\n",
    "    print(\"Download flower dataset..\")\n",
    "    download_file(url)\n",
    "    print(\"Extracting dataset..\")\n",
    "    tarfile.open(\"data/flower_photos.tgz\", \"r:gz\").extractall(path=\"data/\")\n",
    "#     os.remove(\"data/flower_photos.tgz\") # 굳이..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "remake = False\n",
    "parent_dir = \"data/flower_photos\"\n",
    "train_dir = os.path.join(parent_dir, \"train\")\n",
    "test_dir = os.path.join(parent_dir, \"test\")\n",
    "\n",
    "if not os.path.exists(train_dir) or not os.path.exists(test_dir) or remake:\n",
    "    # make dirs\n",
    "    for label in LABELS:\n",
    "        # tf.gfile.MakeDirs make dir recursively & ignore exist dir\n",
    "        tf.gfile.MakeDirs(os.path.join(train_dir, label))\n",
    "        tf.gfile.MakeDirs(os.path.join(test_dir, label))\n",
    "\n",
    "    # copy files\n",
    "    for i, label in enumerate(LABELS):\n",
    "        dir_name = os.path.join(parent_dir, label)\n",
    "        paths = glob.glob(dir_name + \"/*.jpg\")\n",
    "        num_examples = len(paths)\n",
    "        for j, path in enumerate(paths):\n",
    "            fn = os.path.basename(path)\n",
    "            is_train = j < (num_examples * train_ratio)\n",
    "\n",
    "            if is_train:\n",
    "                to_path = os.path.join(train_dir, label, fn)\n",
    "            else:\n",
    "                to_path = os.path.join(test_dir, label, fn)\n",
    "            \n",
    "            tf.gfile.Copy(path, to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    364 test\r\n",
      "   3306 train\r\n"
     ]
    }
   ],
   "source": [
    "!find ./data/flower_photos/test ./data/flower_photos/train -type f | cut -d/ -f4 | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to `TFRecords` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bytes_features(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def _int64_features(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dir_to_tfrecords(dir_path, class_idx, tfrecords_path):\n",
    "    '''convert image-containing dir to tfrecords without exist check.\n",
    "    return: # of image files\n",
    "    '''\n",
    "    with tf.python_io.TFRecordWriter(tfrecords_path) as writer:\n",
    "        paths = glob.glob(dir_path + \"/*.jpg\")\n",
    "        num_examples = len(paths)\n",
    "        for path in paths:\n",
    "            im = scipy.misc.imread(path)\n",
    "            im = scipy.misc.imresize(im, [64, 64])\n",
    "            im_raw = im.tostring()\n",
    "            features = {\n",
    "                \"shape\": _int64_features(im.shape),\n",
    "                \"image\": _bytes_features([im_raw]),\n",
    "                \"label\": _int64_features([class_idx])\n",
    "            }\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            writer.write(example.SerializeToString())\n",
    "            \n",
    "        return num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert dataset to fine-grained TFRecord files ..\n",
      "3306 364\n"
     ]
    }
   ],
   "source": [
    "print(\"Convert dataset to fine-grained TFRecord files ..\")\n",
    "\n",
    "tfrecords_format = \"data/flower_photos_{}_{}.tfrecords\"\n",
    "\n",
    "num_train = 0\n",
    "num_test = 0\n",
    "\n",
    "remake_tfrecords = False\n",
    "tfrecords_path_list = [tfrecords_format.format(top, label) for top in ['train' , 'test'] for label in LABELS]\n",
    "\n",
    "if all(map(tf.gfile.Exists, tfrecords_path_list)) and remake_tfrecords == False:\n",
    "    # already exists\n",
    "    num_train = 3306\n",
    "    num_test = 364\n",
    "else:\n",
    "    # make tfrecords files\n",
    "#     num_train = dir_to_tfrecords('data/flower_photos/train/', tfrecords_train_fn)\n",
    "#     num_test = dir_to_tfrecords('data/flower_photos/test/', tfrecords_test_fn)\n",
    "    num = {'train': 0, 'test': 0}\n",
    "    for top in ['train', 'test']:\n",
    "        for i, label in enumerate(LABELS):\n",
    "            dir_path = os.path.join('data/flower_photos/', top, label)\n",
    "            tfrecords_path = tfrecords_format.format(top, label)\n",
    "            num_cur = dir_to_tfrecords(dir_path, i, tfrecords_path)\n",
    "            num[top] += num_cur\n",
    "            print('# of {}/{}: {}'.format(top, label, num_cur))\n",
    "    num_train = num['train']\n",
    "    num_test = num['test']\n",
    "\n",
    "    \n",
    "# how to get num_examples from tfrecords file?\n",
    "print(num_train, num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_root_dir = './summary/fine-grained/'\n",
    "summary_train_dir = os.path.join(summary_root_dir, 'train')\n",
    "summary_test_dir = os.path.join(summary_root_dir, 'test')\n",
    "model_name = 'tfrecords-fine-grained'\n",
    "tfrecords_path_list_train = [path for path in tfrecords_path_list if 'train' in path]\n",
    "tfrecords_path_list_test = [path for path in tfrecords_path_list if 'test' in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/flower_photos_train_daisy.tfrecords',\n",
       " 'data/flower_photos_train_dandelion.tfrecords',\n",
       " 'data/flower_photos_train_roses.tfrecords',\n",
       " 'data/flower_photos_train_sunflowers.tfrecords',\n",
       " 'data/flower_photos_train_tulips.tfrecords']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrecords_path_list_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/flower_photos_test_daisy.tfrecords',\n",
       " 'data/flower_photos_test_dandelion.tfrecords',\n",
       " 'data/flower_photos_test_roses.tfrecords',\n",
       " 'data/flower_photos_test_sunflowers.tfrecords',\n",
       " 'data/flower_photos_test_tulips.tfrecords']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrecords_path_list_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check our batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename_queue):\n",
    "    with tf.variable_scope('read_data'):\n",
    "        reader = tf.TFRecordReader()\n",
    "        key, records = reader.read(filename_queue)\n",
    "        \n",
    "        # parse records\n",
    "        features = tf.parse_single_example(\n",
    "            records,\n",
    "            features={\n",
    "                \"shape\": tf.FixedLenFeature([3], tf.int64),\n",
    "                \"image\": tf.FixedLenFeature([], tf.string),\n",
    "                \"label\": tf.FixedLenFeature([], tf.int64)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        image = tf.decode_raw(features[\"image\"], tf.uint8)\n",
    "        shape = tf.cast(features[\"shape\"], tf.int32)\n",
    "        label = tf.cast(features[\"label\"], tf.int32)\n",
    "\n",
    "        # preproc\n",
    "        image = tf.reshape(image, [64, 64, 3])\n",
    "#         image = tf.image.resize_images(images=image, size=[64, 64])\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = image / 255.0\n",
    "\n",
    "        one_hot_label = tf.one_hot(label, depth=5)\n",
    "        \n",
    "        return image, one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/programmers_guide/reading_data\n",
    "\n",
    "def get_batch_join(tfrecords_path_list, batch_size, shuffle=False, \n",
    "                   read_thread=5, min_after_dequeue=500, num_epochs=None):\n",
    "    with tf.variable_scope(\"get_batch_join\"):\n",
    "        # make input pipeline\n",
    "        filename_queue = tf.train.string_input_producer(tfrecords_path_list, shuffle=shuffle, num_epochs=num_epochs)\n",
    "        # 5개의 reader 를 두고 각각 읽어오게 하자\n",
    "        example_list = [read_data(filename_queue) for _ in range(read_thread)]\n",
    "        \n",
    "        # train case (shuffle)\n",
    "        capacity = min_after_dequeue + 3*batch_size\n",
    "        if shuffle:\n",
    "            images, labels = tf.train.shuffle_batch_join(tensors_list=example_list, batch_size=batch_size,\n",
    "                                                         capacity=capacity, min_after_dequeue=min_after_dequeue,\n",
    "                                                         allow_smaller_final_batch=True)\n",
    "        else:\n",
    "            images, labels = tf.train.batch_join(example_list, batch_size, capacity=capacity, \n",
    "                                                 allow_smaller_final_batch=True)\n",
    "            \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(tfrecords_path_list, batch_size, shuffle=False, \n",
    "              read_thread=5, min_after_dequeue=500, num_epochs=None):\n",
    "    with tf.variable_scope(\"get_batch\"):\n",
    "        filename_queue = tf.train.string_input_producer(tfrecords_path_list, shuffle=shuffle, num_epochs=num_epochs)\n",
    "        image, label = read_data(filename_queue)\n",
    "        \n",
    "        capacity = min_after_dequeue + 3*batch_size\n",
    "        if shuffle:\n",
    "            images, labels = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=capacity, \n",
    "                                                    min_after_dequeue=min_after_dequeue, num_threads=read_thread,\n",
    "                                                    allow_smaller_final_batch=True)\n",
    "        else:\n",
    "            images, labels = tf.train.batch([image, label], batch_size, capacity=capacity, num_threads=read_thread,\n",
    "                                            allow_smaller_final_batch=True)\n",
    "        \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3306, 364)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train, num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# default min_after_dequeue = 500\n",
    "X, y = get_batch(tfrecords_path_list_train, batch_size=128, shuffle=True, num_epochs=None)\n",
    "\n",
    "# sess = tf.Session()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    cur_X, cur_y = sess.run([X, y])\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.,   0., 126.,   0.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cur_y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for all cases\n",
    "\n",
    "`batch`, `batch_join`, `shuffle_batch`, `shuffle_batch_join` 4개의 케이스에 대해 테스트해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3306 364\n",
      "===== train =====\n",
      "[batch]\n",
      "[512.   0.   0.   0.   0.] 1\n",
      "[ 58. 454.   0.   0.   0.] 1\n",
      "[  0. 355. 157.   0.   0.] 1\n",
      "[  0.   0. 420.  92.   0.] 1\n",
      "[  0.   0.   0. 512.   0.] 1\n",
      "[  0.   0.   0.  26. 486.] 1\n",
      "[278.   0.   0.   0. 234.] 2\n",
      "[292. 220.   0.   0.   0.] 2\n",
      "[  0. 512.   0.   0.   0.] 2\n",
      "[  0.  77. 435.   0.   0.] 2\n",
      "[  0.   0. 142. 370.   0.] 2\n",
      "[  0.   0.   0. 260. 252.] 2\n",
      "[ 44.   0.   0.   0. 468.] 3\n",
      "[512.   0.   0.   0.   0.] 3\n",
      "[ 14. 498.   0.   0.   0.] 3\n",
      "[  0. 311. 201.   0.   0.] 3\n",
      "[  0.   0. 376. 136.   0.] 3\n",
      "[  0.   0.   0. 494.  18.] 3\n",
      "[  0.   0.   0.   0. 512.] 3\n",
      "[322.   0.   0.   0. 190.] 4\n",
      "[248. 264.   0.   0.   0.] 4\n",
      "[  0. 512.   0.   0.   0.] 4\n",
      "[  0.  33. 479.   0.   0.] 4\n",
      "[  0.   0.  98. 414.   0.] 4\n",
      "[  0.   0.   0. 216. 296.] 4\n",
      "[ 88.   0.   0.   0. 424.] 5\n",
      "[482.  30.   0.   0.   0.] 5\n",
      "[  0. 512.   0.   0.   0.] 5\n",
      "[  0. 267. 245.   0.   0.] 5\n",
      "[  0.   0. 332. 180.   0.] 5\n",
      "[  0.   0.   0. 450.  62.] 5\n",
      "[  0.   0.   0.   0. 512.] 5\n",
      "[  0.   0.   0.   0. 146.] 6\n",
      "Done -- epoch limit reached\n",
      "[2850. 4045. 2885. 3150. 3600.] 16530\n",
      "elapsed time: 2.20s\n",
      "\n",
      "[batch_join]\n",
      "[105. 103. 100. 104. 100.] 1\n",
      "[101.  99. 107. 103. 102.] 1\n",
      "[100. 104. 102. 105. 101.] 1\n",
      "[103. 101. 102. 103. 103.] 1\n",
      "[103.  99. 104. 106. 100.] 1\n",
      "[102. 102. 104. 101. 103.] 1\n",
      "[100. 101. 100. 105. 106.] 2\n",
      "[103. 100. 103. 102. 104.] 2\n",
      "[103. 102. 103. 103. 101.] 2\n",
      "[103. 103. 100. 103. 103.] 2\n",
      "[103. 103. 100. 106. 100.] 2\n",
      "[101. 101. 103. 106. 101.] 2\n",
      "[103. 103. 104. 100. 102.] 3\n",
      "[100. 104. 100. 106. 102.] 3\n",
      "[106. 103. 102. 101. 100.] 3\n",
      "[104. 102. 102. 101. 103.] 3\n",
      "[105. 102. 104. 100. 101.] 3\n",
      "[101. 100. 105. 102. 104.] 3\n",
      "[101. 104. 103. 101. 103.] 3\n",
      "[104. 100. 103. 102. 103.] 4\n",
      "[103. 106. 102. 101. 100.] 4\n",
      "[102. 101. 102. 104. 103.] 4\n",
      "[ 58. 132.  73. 122. 127.] 4\n",
      "[ 30. 150.  32. 152. 148.] 4\n",
      "[ 40. 189.  39.  63. 181.] 4\n",
      "[ 38. 197.  38.  39. 200.] 5\n",
      "[ 45. 229.  46.  46. 146.] 5\n",
      "[ 57. 283.  57.  58.  57.] 5\n",
      "[ 95. 136.  95.  93.  93.] 5\n",
      "[102. 103. 101. 103. 103.] 5\n",
      "[102. 101. 105. 102. 102.] 5\n",
      "[ 27. 166.  44. 107. 168.] 5\n",
      "[  0. 116.   0.   0.  30.] 6\n",
      "Done -- epoch limit reached\n",
      "[2850. 4045. 2885. 3150. 3600.] 16530\n",
      "elapsed time: 2.54s\n",
      "\n",
      "[shuffle_batch]\n",
      "[  0.   0. 117. 395.   0.] 1\n",
      "[ 89.   0. 258. 165.   0.] 1\n",
      "[274.   0. 133.  45.  60.] 1\n",
      "[127.   5.  48.  15. 317.] 1\n",
      "[ 50. 221.  13.   6. 222.] 1\n",
      "[ 11. 354.  65.   4.  78.] 1\n",
      "[ 11. 173. 299.   0.  29.] 2\n",
      "[  6. 348. 148.   0.  10.] 2\n",
      "[  2. 325.  51. 131.   3.] 2\n",
      "[  0. 110.  16. 316.  70.] 2\n",
      "[ 11.  53.   6. 108. 334.] 2\n",
      "[242.  19.   0.  48. 203.] 2\n",
      "[204. 217.   0.  18.  73.] 3\n",
      "[ 70. 356.   0.   6.  80.] 3\n",
      "[ 27. 160.   0.   1. 324.] 3\n",
      "[223.  57.   0.   2. 230.] 3\n",
      "[241.  19. 172.   0.  80.] 3\n",
      "[ 80.   8. 273. 121.  30.] 3\n",
      "[109.   2.  87. 303.  11.] 3\n",
      "[288.   0.  34. 134.  56.] 4\n",
      "[140.   2.   7.  49. 314.] 4\n",
      "[ 56. 217.   2.  13. 224.] 4\n",
      "[ 11. 353.  54.   8.  86.] 4\n",
      "[  6. 154. 282.  40.  30.] 4\n",
      "[  1.  48. 156. 297.  10.] 4\n",
      "[  0.  22. 293. 192.   5.] 5\n",
      "[  1.   8. 236.  76. 191.] 5\n",
      "[ 79.   3.  93.  17. 320.] 5\n",
      "[286.  56.  28.   7. 135.] 5\n",
      "[139. 316.  11.   1.  45.] 5\n",
      "[ 40. 284.   2. 166.  20.] 5\n",
      "[ 22. 131.   1. 350.   8.] 5\n",
      "[  4.  24.   0. 116.   2.] 6\n",
      "Done -- epoch limit reached\n",
      "[2850. 4045. 2885. 3150. 3600.] 16530\n",
      "elapsed time: 2.20s\n",
      "\n",
      "[shuffle_batch_join]\n",
      "[104. 100. 101.  96. 111.] 1\n",
      "[105. 109. 107.  95.  96.] 1\n",
      "[101. 101. 100. 102. 108.] 1\n",
      "[ 96. 101. 109. 100. 106.] 1\n",
      "[ 97.  96. 105. 105. 109.] 1\n",
      "[109. 101. 102. 108.  92.] 1\n",
      "[107. 105. 102. 101.  97.] 2\n",
      "[106. 106.  94.  98. 108.] 2\n",
      "[ 98.  99. 107. 108. 100.] 2\n",
      "[ 94. 103.  95. 106. 114.] 2\n",
      "[116. 106. 105.  97.  88.] 2\n",
      "[ 97. 108.  90. 108. 109.] 2\n",
      "[109.  99. 104. 105.  95.] 3\n",
      "[100.  99. 104.  98. 111.] 3\n",
      "[102. 113.  99. 106.  92.] 3\n",
      "[ 91.  88. 106. 105. 122.] 3\n",
      "[104. 100. 110. 103.  95.] 3\n",
      "[105. 106. 101. 103.  97.] 3\n",
      "[114. 105.  97.  90. 106.] 3\n",
      "[106. 102.  99. 109.  96.] 4\n",
      "[ 91. 100. 108. 102. 111.] 4\n",
      "[ 93. 111. 100.  94. 114.] 4\n",
      "[ 53. 126.  56. 144. 133.] 4\n",
      "[ 42. 145.  47. 129. 149.] 4\n",
      "[ 29. 195.  40.  69. 179.] 4\n",
      "[ 45. 197.  38.  53. 179.] 5\n",
      "[ 50. 256.  49.  50. 107.] 5\n",
      "[ 67. 216.  66.  72.  91.] 5\n",
      "[ 84. 152.  99.  89.  88.] 5\n",
      "[ 96. 112.  95. 106. 103.] 5\n",
      "[ 91. 119.  90. 100. 112.] 5\n",
      "[ 36. 206.  46.  80. 144.] 5\n",
      "[12. 63. 14. 19. 38.] 6\n",
      "Done -- epoch limit reached\n",
      "[2850. 4045. 2885. 3150. 3600.] 16530\n",
      "elapsed time: 2.52s\n",
      "\n",
      "===== test =====\n",
      "[batch]\n",
      "[126. 174.  64.  69.  79.] 2\n",
      "[ 63.  93. 128. 138.  90.] 3\n",
      "[126. 106.  64.  69. 147.] 5\n",
      "[ 0. 72. 64. 69. 79.] 6\n",
      "Done -- epoch limit reached\n",
      "[315. 445. 320. 345. 395.] 1820\n",
      "elapsed time: 0.30s\n",
      "\n",
      "[batch_join]\n",
      "[100.  99. 104. 103. 106.] 2\n",
      "[102. 105. 104. 102.  99.] 3\n",
      "[ 72. 147.  70.  92. 131.] 5\n",
      "[41. 94. 42. 48. 59.] 6\n",
      "Done -- epoch limit reached\n",
      "[315. 445. 320. 345. 395.] 1820\n",
      "elapsed time: 0.55s\n",
      "\n",
      "[shuffle_batch]\n",
      "[ 82. 118. 106.  90. 116.] 2\n",
      "[ 94. 142.  87. 100.  89.] 3\n",
      "[ 92. 108.  91.  96. 125.] 5\n",
      "[47. 77. 36. 59. 65.] 6\n",
      "Done -- epoch limit reached\n",
      "[315. 445. 320. 345. 395.] 1820\n",
      "elapsed time: 0.32s\n",
      "\n",
      "[shuffle_batch_join]\n",
      "[102. 104. 109. 105.  92.] 2\n",
      "[ 89. 110.  94. 100. 119.] 3\n",
      "[ 72. 156.  72.  95. 117.] 5\n",
      "[52. 75. 45. 45. 67.] 6\n",
      "Done -- epoch limit reached\n",
      "[315. 445. 320. 345. 395.] 1820\n",
      "elapsed time: 0.57s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(num_train, num_test)\n",
    "for data_type in ['train', 'test']:\n",
    "    n_examples = num_train if data_type == 'train' else num_test\n",
    "    kargs = {\n",
    "        'min_after_dequeue': 500,\n",
    "        'num_epochs': 5,\n",
    "        'read_thread': 20,\n",
    "        'batch_size': 512\n",
    "    }\n",
    "    if data_type == 'train':\n",
    "        kargs['tfrecords_path_list'] = tfrecords_path_list_train\n",
    "    else:\n",
    "        kargs['tfrecords_path_list'] = tfrecords_path_list_test\n",
    "    \n",
    "    print(\"===== {} =====\".format(data_type))\n",
    "    for func_type in ['batch', 'batch_join', 'shuffle_batch', 'shuffle_batch_join']:\n",
    "        tf.reset_default_graph()\n",
    "        kargs['shuffle'] = 'shuffle' in func_type\n",
    "\n",
    "        if 'join' in func_type:\n",
    "            X, y = get_batch_join(**kargs)\n",
    "        else:\n",
    "            X, y = get_batch(**kargs)\n",
    "\n",
    "        print(\"[{}]\".format(func_type))\n",
    "\n",
    "        # sess = tf.Session()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # epochs 를 쓰면 local_variables_init 도 해줘야 함.\n",
    "            # 위에서 지정한 num_epochs 가 local_variable 로 그래프에 박히는 듯\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "            \n",
    "            st = time.time()\n",
    "\n",
    "            # num_epochs 를 지정해주면 이러한 방식으로 해야 함\n",
    "            n_iter = None\n",
    "            step_cnt = 0\n",
    "            data_cnt = np.zeros([5])\n",
    "            try:\n",
    "                while not coord.should_stop():\n",
    "                    cur_X, cur_y = sess.run([X, y])\n",
    "                    print(np.sum(cur_y, axis=0), end=\" \")\n",
    "                    data_cnt += np.sum(cur_y, axis=0)\n",
    "                    step_cnt += 1\n",
    "                    epoch = np.ceil(float(step_cnt * kargs['batch_size']) / n_examples)\n",
    "                    print(int(epoch))\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Done -- epoch limit reached')\n",
    "                print(data_cnt, np.sum(data_cnt, dtype=np.int32))\n",
    "                print('elapsed time: {:.2f}s'.format(time.time() - st))\n",
    "            finally:\n",
    "                coord.request_stop()\n",
    "\n",
    "            print(\"\")\n",
    "#             coord.request_stop()\n",
    "            coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
