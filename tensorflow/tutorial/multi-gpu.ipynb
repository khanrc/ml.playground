{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi GPU training\n",
    "\n",
    "Two versions:\n",
    "\n",
    "1. optimizer 의 `colocate_gradients_with_ops=True` 옵션을 사용\n",
    "2. 위 방법을 가속하기 위해 NCCL 라이브러리 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GPU MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/irteam/anaconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow.contrib.slim as slim\n",
    "import pandas as pd\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_graph():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(777)\n",
    "    np.random.seed(777)\n",
    "\n",
    "    # inputs\n",
    "    X = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(X, y):\n",
    "    x = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "    x = slim.conv2d(x, 128, kernel_size=[5,5])\n",
    "    x = slim.conv2d(x, 128, kernel_size=[5,5])\n",
    "    x = slim.max_pool2d(x, kernel_size=[2,2])\n",
    "\n",
    "    x = slim.conv2d(x, 128, kernel_size=[3,3])\n",
    "    x = slim.conv2d(x, 128, kernel_size=[3,3])\n",
    "    x = slim.max_pool2d(x, kernel_size=[2,2])\n",
    "    \n",
    "    x = slim.conv2d(x, 128, kernel_size=[3,3])\n",
    "    x = slim.conv2d(x, 128, kernel_size=[3,3])\n",
    "    x = slim.max_pool2d(x, kernel_size=[2,2])\n",
    "\n",
    "    flat = slim.flatten(x)\n",
    "    logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "    prob = tf.nn.softmax(logits)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits)\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, train_op, loss, accuracy, n_epoch, batch_size, test_batch_size, sync_op=None):\n",
    "    # multi-gpu 환경에서는 test batch size 를 더 키워도 됨\n",
    "    config = tf.ConfigProto(log_device_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    # config.log_device_placement = True # 안 나오네...\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # NCCL 의 경우에는 이게 필요함\n",
    "    if sync_op:\n",
    "        sess.run(sync_op)\n",
    "\n",
    "    N = mnist.train.num_examples\n",
    "    n_iter = N // batch_size\n",
    "    \n",
    "    total_train_time = 0.\n",
    "    total_test_time = 0.\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        st = time.time()\n",
    "\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "        for _ in range(n_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, cur_acc, cur_loss = sess.run([train_op, accuracy, loss], {X: batch_x, y: batch_y})\n",
    "            avg_acc += cur_acc\n",
    "            avg_loss += cur_loss\n",
    "\n",
    "        avg_acc /= n_iter\n",
    "        avg_loss /= n_iter\n",
    "\n",
    "        train_elapsed = time.time() - st\n",
    "        st = time.time()\n",
    "\n",
    "        test_acc = 0.\n",
    "        test_loss = 0.\n",
    "        for _ in range(mnist.test.num_examples // test_batch_size):\n",
    "            batch_x, batch_y = mnist.test.next_batch(test_batch_size)\n",
    "            cur_acc, cur_loss = sess.run([accuracy, loss], {X: batch_x, y: batch_y})\n",
    "            test_acc += cur_acc\n",
    "            test_loss += cur_loss\n",
    "        test_acc /= (mnist.test.num_examples // test_batch_size)\n",
    "        test_loss /= (mnist.test.num_examples // test_batch_size)\n",
    "\n",
    "        test_elapsed = time.time() - st\n",
    "\n",
    "        # skip the warm-up epoch: 0\n",
    "        if epoch > 0:\n",
    "            total_train_time += train_elapsed\n",
    "            total_test_time += test_elapsed\n",
    "        print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f} | {:.2f}s, {:.2f}s\".\n",
    "              format(epoch+1, n_epoch, avg_acc, avg_loss, test_acc, test_loss, train_elapsed, test_elapsed))\n",
    "    \n",
    "    total_train_time /= (n_epoch-1)\n",
    "    total_test_time /= (n_epoch-1)\n",
    "    print(\"Average time\\t TRAIN {:.3f}\\t TEST {:.3f}\".format(total_train_time, total_test_time))\n",
    "    \n",
    "    return total_train_time, total_test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "\"\"\" [!] batch_size 는 n_gpu 의 배수인 경우만 고려함. \"\"\"\n",
    "n_epoch = 10\n",
    "train_batch_size = 1024\n",
    "test_batch_size = 1024\n",
    "\n",
    "stats = pd.DataFrame(columns=[\"#GPUs\", \"train\", \"test\"])\n",
    "stats = stats.set_index(\"#GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home1/irteam/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:731: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "[ 1/10] (train) acc: 80.23%, loss: 0.638 | (test) acc: 97.09%, loss: 0.095 | 9.31s, 0.46s\n",
      "[ 2/10] (train) acc: 97.63%, loss: 0.076 | (test) acc: 98.80%, loss: 0.037 | 7.18s, 0.41s\n",
      "[ 3/10] (train) acc: 98.59%, loss: 0.043 | (test) acc: 98.83%, loss: 0.037 | 7.21s, 0.42s\n",
      "[ 4/10] (train) acc: 98.97%, loss: 0.032 | (test) acc: 99.08%, loss: 0.028 | 7.22s, 0.41s\n",
      "[ 5/10] (train) acc: 99.24%, loss: 0.024 | (test) acc: 98.96%, loss: 0.034 | 7.24s, 0.41s\n",
      "[ 6/10] (train) acc: 99.46%, loss: 0.017 | (test) acc: 98.93%, loss: 0.035 | 7.24s, 0.41s\n",
      "[ 7/10] (train) acc: 99.47%, loss: 0.017 | (test) acc: 99.22%, loss: 0.023 | 7.25s, 0.41s\n",
      "[ 8/10] (train) acc: 99.57%, loss: 0.014 | (test) acc: 99.15%, loss: 0.031 | 7.25s, 0.41s\n",
      "[ 9/10] (train) acc: 99.64%, loss: 0.012 | (test) acc: 98.95%, loss: 0.038 | 7.25s, 0.41s\n",
      "[10/10] (train) acc: 99.73%, loss: 0.009 | (test) acc: 99.31%, loss: 0.027 | 7.26s, 0.41s\n",
      "Average time\t TRAIN 7.234\t TEST 0.412\n"
     ]
    }
   ],
   "source": [
    "# no parallel\n",
    "X, y = prepare_graph()\n",
    "loss, accuracy = build_graph(X, y)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "avg_train, avg_test = train(X, y, train_op, loss, accuracy, n_epoch, train_batch_size, test_batch_size)\n",
    "stats.loc[1] = [avg_train, avg_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi GPUs MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_parallel(model_builder, num_gpus, **kwargs):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        fn: model builder function\n",
    "        num_gpus\n",
    "        kwargs: input of model builder; e.g. X=X, y=y.\n",
    "        \n",
    "    Returns:\n",
    "        2d tensors: num_gpus * retrun_list_of_fn\n",
    "        e.g. \n",
    "        [[loss, acc, train_op],\n",
    "         [loss, acc, train_op],\n",
    "        ...\n",
    "        ] \n",
    "    \"\"\"\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "    out_split = []\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=i > 0):\n",
    "                inputs = {k : v[i] for k, v in in_splits.items()}\n",
    "                ret = model_builder(**inputs) # return loss, accuracy\n",
    "                out_split.append(ret)\n",
    "\n",
    "    return tf.convert_to_tensor(out_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance_test(n_gpu):\n",
    "    X, y = prepare_graph()\n",
    "    parallel_tensors = make_parallel(build_graph, n_gpu, X=X, y=y) # loss, acc\n",
    "    integrated_tensors = tf.reduce_mean(parallel_tensors, axis=0)\n",
    "    loss = integrated_tensors[0]\n",
    "    accuracy = integrated_tensors[1]\n",
    "    '''\n",
    "    [!] colocate_gradients_with_ops 옵션을 켜 줘야 gradient 를 계산할 때 original ops 와 같은 디바이스에서 계산함.\n",
    "    만약 이걸 키지 않으면 gradient 는 전부 default device 인 gpu:0 에서 하게 되어서 속도가 안 빨라짐 (오히려 더 느려짐;)\n",
    "    '''\n",
    "    train_op = tf.train.AdamOptimizer().minimize(loss, colocate_gradients_with_ops=True)\n",
    "\n",
    "    avg_train, avg_test = train(X, y, train_op, loss, accuracy, n_epoch, train_batch_size, test_batch_size)\n",
    "    stats.loc[n_gpu] = [avg_train, avg_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance test with 2/4/8 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10] (train) acc: 83.26%, loss: 0.514 | (test) acc: 97.54%, loss: 0.082 | 5.48s, 0.29s\n",
      "[ 2/10] (train) acc: 97.84%, loss: 0.069 | (test) acc: 98.56%, loss: 0.048 | 3.87s, 0.24s\n",
      "[ 3/10] (train) acc: 98.75%, loss: 0.040 | (test) acc: 98.67%, loss: 0.040 | 3.87s, 0.21s\n",
      "[ 4/10] (train) acc: 99.16%, loss: 0.028 | (test) acc: 98.74%, loss: 0.040 | 3.87s, 0.22s\n",
      "[ 5/10] (train) acc: 99.28%, loss: 0.023 | (test) acc: 98.77%, loss: 0.038 | 3.93s, 0.25s\n",
      "[ 6/10] (train) acc: 99.41%, loss: 0.018 | (test) acc: 99.08%, loss: 0.028 | 3.92s, 0.23s\n",
      "[ 7/10] (train) acc: 99.52%, loss: 0.015 | (test) acc: 98.91%, loss: 0.034 | 3.90s, 0.23s\n",
      "[ 8/10] (train) acc: 99.61%, loss: 0.012 | (test) acc: 99.23%, loss: 0.030 | 3.93s, 0.23s\n",
      "[ 9/10] (train) acc: 99.63%, loss: 0.011 | (test) acc: 99.26%, loss: 0.027 | 3.93s, 0.23s\n",
      "[10/10] (train) acc: 99.69%, loss: 0.010 | (test) acc: 99.18%, loss: 0.035 | 3.89s, 0.23s\n",
      "Average time\t TRAIN 3.902\t TEST 0.231\n"
     ]
    }
   ],
   "source": [
    "performance_test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10] (train) acc: 81.60%, loss: 0.605 | (test) acc: 97.21%, loss: 0.087 | 4.61s, 0.34s\n",
      "[ 2/10] (train) acc: 97.99%, loss: 0.066 | (test) acc: 98.62%, loss: 0.042 | 2.32s, 0.25s\n",
      "[ 3/10] (train) acc: 98.66%, loss: 0.043 | (test) acc: 99.02%, loss: 0.030 | 2.31s, 0.25s\n",
      "[ 4/10] (train) acc: 99.11%, loss: 0.028 | (test) acc: 99.01%, loss: 0.029 | 2.33s, 0.25s\n",
      "[ 5/10] (train) acc: 99.18%, loss: 0.024 | (test) acc: 99.02%, loss: 0.034 | 2.30s, 0.25s\n",
      "[ 6/10] (train) acc: 99.43%, loss: 0.019 | (test) acc: 99.16%, loss: 0.027 | 2.33s, 0.22s\n",
      "[ 7/10] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.28%, loss: 0.024 | 2.32s, 0.24s\n",
      "[ 8/10] (train) acc: 99.60%, loss: 0.013 | (test) acc: 98.98%, loss: 0.033 | 2.27s, 0.25s\n",
      "[ 9/10] (train) acc: 99.62%, loss: 0.012 | (test) acc: 99.02%, loss: 0.027 | 2.31s, 0.24s\n",
      "[10/10] (train) acc: 99.66%, loss: 0.010 | (test) acc: 99.11%, loss: 0.032 | 2.30s, 0.24s\n",
      "Average time\t TRAIN 2.311\t TEST 0.242\n"
     ]
    }
   ],
   "source": [
    "performance_test(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10] (train) acc: 82.07%, loss: 0.588 | (test) acc: 97.80%, loss: 0.076 | 5.89s, 0.35s\n",
      "[ 2/10] (train) acc: 97.99%, loss: 0.066 | (test) acc: 98.18%, loss: 0.054 | 2.18s, 0.18s\n",
      "[ 3/10] (train) acc: 98.62%, loss: 0.045 | (test) acc: 98.99%, loss: 0.032 | 2.18s, 0.16s\n",
      "[ 4/10] (train) acc: 98.95%, loss: 0.032 | (test) acc: 99.02%, loss: 0.033 | 2.19s, 0.17s\n",
      "[ 5/10] (train) acc: 99.33%, loss: 0.021 | (test) acc: 98.74%, loss: 0.041 | 2.17s, 0.16s\n",
      "[ 6/10] (train) acc: 99.40%, loss: 0.018 | (test) acc: 98.97%, loss: 0.034 | 2.16s, 0.17s\n",
      "[ 7/10] (train) acc: 99.56%, loss: 0.014 | (test) acc: 99.11%, loss: 0.031 | 2.17s, 0.15s\n",
      "[ 8/10] (train) acc: 99.57%, loss: 0.013 | (test) acc: 98.95%, loss: 0.036 | 2.16s, 0.17s\n",
      "[ 9/10] (train) acc: 99.59%, loss: 0.013 | (test) acc: 99.12%, loss: 0.031 | 2.17s, 0.14s\n",
      "[10/10] (train) acc: 99.79%, loss: 0.007 | (test) acc: 99.05%, loss: 0.032 | 2.18s, 0.16s\n",
      "Average time\t TRAIN 2.173\t TEST 0.162\n"
     ]
    }
   ],
   "source": [
    "performance_test(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current processing time\n",
    "\n",
    "average of 10 epochs\n",
    "\n",
    "- train/test batch size: 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_print(df):\n",
    "    df = df.copy()\n",
    "    # train time / test time 별 성능 향상 % 계산\n",
    "    df['train %'] = df['train'][1] / df['train']\n",
    "    df['test %'] = df['test'][1] / df['test']\n",
    "    \n",
    "    # 예쁘게 출력\n",
    "    return df.style.format({\n",
    "        'train': '{:.3f}',\n",
    "        'test': '{:.3f}',\n",
    "        'train %': '{:.0%}',\n",
    "        'test %': '{:.0%}'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >train</th> \n",
       "        <th class=\"col_heading level0 col1\" >test</th> \n",
       "        <th class=\"col_heading level0 col2\" >train %</th> \n",
       "        <th class=\"col_heading level0 col3\" >test %</th> \n",
       "    </tr>    <tr> \n",
       "        <th class=\"index_name level0\" >#GPUs</th> \n",
       "        <th class=\"blank\" ></th> \n",
       "        <th class=\"blank\" ></th> \n",
       "        <th class=\"blank\" ></th> \n",
       "        <th class=\"blank\" ></th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672level0_row0\" class=\"row_heading level0 row0\" >1</th> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row0_col0\" class=\"data row0 col0\" >7.234</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row0_col1\" class=\"data row0 col1\" >0.412</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row0_col2\" class=\"data row0 col2\" >100%</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row0_col3\" class=\"data row0 col3\" >100%</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672level0_row1\" class=\"row_heading level0 row1\" >2</th> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row1_col0\" class=\"data row1 col0\" >3.902</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row1_col1\" class=\"data row1 col1\" >0.231</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row1_col2\" class=\"data row1 col2\" >185%</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row1_col3\" class=\"data row1 col3\" >178%</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672level0_row2\" class=\"row_heading level0 row2\" >4</th> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row2_col0\" class=\"data row2 col0\" >2.311</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row2_col1\" class=\"data row2 col1\" >0.242</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row2_col2\" class=\"data row2 col2\" >313%</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row2_col3\" class=\"data row2 col3\" >170%</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672level0_row3\" class=\"row_heading level0 row3\" >8</th> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row3_col0\" class=\"data row3 col0\" >2.173</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row3_col1\" class=\"data row3 col1\" >0.162</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row3_col2\" class=\"data row3 col2\" >333%</td> \n",
       "        <td id=\"T_2e65e1f6_3259_11e8_92ba_ac1f6b26d672row3_col3\" class=\"data row3 col3\" >254%</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8bef6a8908>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선: NCCL\n",
    "\n",
    "현재 multi-gpu training 프로세스는 다음과 같음:\n",
    "\n",
    "1. 각 gpu 에 네트워크를 할당\n",
    "2. input data 를 각 gpu 별로 분배 (DataParallel)\n",
    "3. 각 gpu 마다 weight 를 gpu:0 에서 읽어옴\n",
    "    - `make_parallel` 코드를 보면 0 번 gpu 외에는 `reuse=True` 로 들어감\n",
    "    - 즉, 매 step 마다 gpu:0 에서 weight 를 읽어오게 됨\n",
    "4. 각 gpu마다 2번에서 분배받은 input data 에 대해 forward/backward(gradients) 계산 \n",
    "5. gradients 를 gpu:0 으로 모아서 평균내어 weight update\n",
    "    - 이렇게 업데이트한 weight 는 gpu:0 에만 저장되지만 다음 스텝에서 다른 gpu들은 3번 과정에서 다시 gpu:0 에서 읽어오므로 괜찮음\n",
    "\n",
    "여기서 문제가 되는건 결국 gpu:0 으로 gradients 를 모으고 다시 재배포하는 과정으로, NCCL 을 사용하면 이 과정을 가속할 수 있음\n",
    "\n",
    "1. gpu 마다 네트워크를 할당할 때 `reuse=False` 로 하여 각 gpu 마다 weight 를 독립적으로 유지\n",
    "2. NCCL 의 all_sum 함수를 이용하여 gradient 를 모아서 평균내고 각 gpu 로 재배포\n",
    "    - 결국 gradients 를 모으고 다시 재배포하는 것은 동일하나 이것을 NCCL 을 통해 함으로써 가속\n",
    "    - NCCL 의 all_sum 함수를 활용하기 위해 weight 를 업데이트 후 재배포하는 것이 아니라 gradient 만 모아서 재배포하고 각 gpu 에서 알아서 업데이트\n",
    "    \n",
    "### 참고\n",
    "\n",
    "- [Tensorpack과 Multigpu를 활용한 빠른 트레이닝 코드 작성하기](http://openresearch.ai/t/tensorpack-multigpu/45)\n",
    "- [NCCL을 이용한 Efficient한 Tensorflow MultiGPU Training 코드 작성하기](http://openresearch.ai/t/nccl-efficient-tensorflow-multigpu-training/159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `synchronize_gpus()` 함수로 대체\n",
    "# def get_post_init_ops():\n",
    "#     \"\"\"\n",
    "#     Copy values of variables on GPU 0 to other GPUs.\n",
    "#     \"\"\"\n",
    "#     # literally all variables, because it's better to sync optimizer-internal variables as well\n",
    "#     all_vars = tf.global_variables() + tf.local_variables()\n",
    "#     var_by_name = dict([(v.name, v) for v in all_vars])\n",
    "#     post_init_ops = []\n",
    "#     for v in all_vars:\n",
    "#         if not v.name.startswith('tower'):\n",
    "#             continue\n",
    "#         if v.name.startswith('tower0'):\n",
    "#             # no need for copy to tower0\n",
    "#             continue\n",
    "#         # in this trainer, the master name doesn't have the towerx/ prefix\n",
    "#         split_name = v.name.split('/')\n",
    "#         prefix = split_name[0]\n",
    "#         realname = '/'.join(split_name[1:])\n",
    "#         if prefix in realname:\n",
    "#             # logger.warning(\"variable {} has its prefix {} appears multiple times in its name!\".format(v.name, prefix))\n",
    "#             pass\n",
    "#         copy_from = var_by_name.get(v.name.replace(prefix, 'tower0'))\n",
    "#         if copy_from is not None:\n",
    "#             post_init_ops.append(v.assign(copy_from.read_value()))\n",
    "#         else:\n",
    "#             # logger.warning(\"Cannot find {} in the graph!\".format(realname))\n",
    "#             pass\n",
    "#     print(\"'sync_variables_from_main_tower' includes {} operations.\".format(len(post_init_ops)))\n",
    "#     return tf.group(*post_init_ops, name='sync_variables_from_main_tower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_parallel_without_weight_sharing(model_builder, optimizers, n_gpu, **kwargs):\n",
    "    \"\"\" 각 gpu 마다 네트워크를 생성 (단, weight sharing 하지 않음)\n",
    "    여기서 `optimizers[i].compute_gradients` 를 통해 gradients 도 구함\n",
    "    \n",
    "    Args:\n",
    "        kwargs: input of model builder; e.g. X=X, y=y.\n",
    "        \n",
    "    Returns:\n",
    "        out_list: [K, 2]; [(loss, accuracy) * K]\n",
    "        gv_list: [K, N, 2]\n",
    "    \"\"\"\n",
    "    # 레퍼런스에 따르면 이 split 과정이 무겁기 때문에 gpu 별로 input queue 를 만드는게 좋다고 함\n",
    "    # 본 실험에서 8gpu 성능이 원하는 만큼 나오지 않는 것도 이 때문일 수 있음\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, n_gpu)\n",
    "\n",
    "    out_list = []\n",
    "    gv_list = []\n",
    "    for i in range(n_gpu):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            scope = \"tower{}\".format(i)\n",
    "            # reuse=False 로 설정하여 각 gpu 의 weight 를 독립적으로 운용\n",
    "            with tf.variable_scope(scope, reuse=False):\n",
    "                inputs = {k : v[i] for k, v in in_splits.items()}\n",
    "                ret = build_graph(**inputs) # (loss, accuracy)\n",
    "                out_list.append(ret)\n",
    "\n",
    "                # 각 gpu 에서 loss 에 대해 gradients 를 계산\n",
    "                \n",
    "                # variable 을 지정해주지 않으면 전체 variable 에 대해서 다 계산함\n",
    "                variables = slim.get_variables(scope=scope)\n",
    "                grads_vars = optimizers[i].compute_gradients(ret[0], var_list=variables)\n",
    "\n",
    "                # 전체 variable 에 대해서 다 계산해준 후 골라내는 방법도 있음\n",
    "#                 grads_vars = optimizers[i].compute_gradients(ret[0])\n",
    "#                 grads_vars = [(grad, var) for grad, var in grads_vars if grad is not None]\n",
    "            \n",
    "                gv_list.append(grads_vars) # [K, N, 2]\n",
    "\n",
    "    # gv_list 를 각각 grad_list 와 var_list 로 분리\n",
    "    grad_list = []\n",
    "    var_list = []\n",
    "    for tower in gv_list:\n",
    "        grad_list.append([x[0] for x in tower])\n",
    "        var_list.append([x[1] for x in tower])\n",
    "        \n",
    "    return out_list, grad_list, var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import nccl\n",
    "\n",
    "def reduce_average_gradients(grad_list):\n",
    "    \"\"\" 각 gradients 를 모아서 평균내고 다시 각 gpu 로 보냄 - NCCL 을 이용하여 가속.\n",
    "    Args:\n",
    "        grad_list: [K, N]; K == n_gpu.\n",
    "    Returns:\n",
    "        new_grad_list: [K, N]; averaged gradients.\n",
    "    \"\"\"\n",
    "    n_gpu = len(grad_list)\n",
    "    if n_gpu == 1:\n",
    "        return grad_list\n",
    "\n",
    "    # 각 tower (gpu) 를 돌면서 동일한 gradient 들에 대해 sum 을 해줘야 하므로 grad_list[:, i] 를 구해야 한다.\n",
    "    # numpy 가 아니므로 이걸 구하기가 까다로우므로 zip(*grad_list) 로 [N, K] 로 만들어주자.\n",
    "    new_grad_list = []\n",
    "    for grads_per_var in zip(*grad_list):\n",
    "        assert len(grads_per_var) == n_gpu\n",
    "        # grads_per_var 는 어떤 variable 에 대한 모든 gradients 를 다 가지고 있는 리스트.\n",
    "        # nccl.all_sum 을 통해 이 gradients 를 전부 더해준 뒤 다시 각 gpu로 재배포했다.\n",
    "        summed = nccl.all_sum(grads_per_var)\n",
    "\n",
    "        # 이 때 각 tensor 들은 sum 이므로, average 로 바꿔주자\n",
    "        averaged = []\n",
    "        for tensor in summed:\n",
    "            with tf.device(tensor.device):\n",
    "                averaged.append(tensor / float(n_gpu))\n",
    "        new_grad_list.append(averaged)\n",
    "#         new_grad_list.append(summed)\n",
    "\n",
    "    # 다시 [N, K] 로 transpose\n",
    "    new_grad_list = list(zip(*new_grad_list))\n",
    "\n",
    "    # 이제 averaged gradients 를 얻었다!\n",
    "    return new_grad_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_gradients(grad_list, var_list, optimizers, n_gpu):\n",
    "    \"\"\" grad_list 와 var_list 를 받아서 weights 에 gradients 를 업데이트 \"\"\"\n",
    "    train_ops = []\n",
    "    for i in range(n_gpu):\n",
    "        grads_vars = zip(grad_list[i], var_list[i])\n",
    "\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            scope = \"tower{}\".format(i)\n",
    "            # BN 이 있으면 이걸 해줘야 함\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=scope)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                train_op = optimizers[i].apply_gradients(grads_vars)\n",
    "                train_ops.append(train_op)\n",
    "\n",
    "    train_op = tf.group(*train_ops)\n",
    "    \n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights 의 초기값 동기화가 필요함\n",
    "def synchronize_gpus(n_gpu):\n",
    "    \"\"\" tower0 의 weights 를 tower{1~7} 로 복사하는 오퍼레이션 생성 \"\"\"\n",
    "    # local variables 는 일단 무시; 뭐가 있는지 모르겠네\n",
    "    assert tf.local_variables() == []\n",
    "    \n",
    "    copy_ops = []\n",
    "    towers = [slim.get_variables('tower{}'.format(i)) for i in range(n_gpu)]\n",
    "    for i in range(1, n_gpu):\n",
    "        # tower[i] <= tower[0] \n",
    "        # print(\"copy from {} to {}\".format(towers[0][0].name.split('/')[0], towers[i][0].name.split('/')[0]))\n",
    "        copy_ops += [tf.assign(ref, value) for ref, value in zip(towers[i], towers[0])]\n",
    "    \n",
    "    print(\"# of copy_ops = {}\".format(len(copy_ops)))\n",
    "    return tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance_test(n_gpu):\n",
    "    X, y = prepare_graph()\n",
    "    # optimizer 를 하나로 사용할 경우 optimizer 내부 변수가 공유되어 문제가 생김.\n",
    "    # Adam 의 경우 beta1_power, beta2_power 가 있는데 apply_gradients 때마다 매번 이 변수가 업데이트됨.\n",
    "    # 이 문제를 해결하기 위해 각 gpu 마다 optimizer 를 따로 사용해주자.\n",
    "    # optimizer = tf.train.AdamOptimizer()\n",
    "    optimizers = [tf.train.AdamOptimizer() for _ in range(n_gpu)]\n",
    "    \n",
    "    # 각 gpu 마다 네트워크 생성 및 gradients operation 생성\n",
    "    out_list, grad_list, var_list = make_parallel_without_weight_sharing(\n",
    "        build_graph, optimizers, n_gpu, X=X, y=y)\n",
    "\n",
    "    # loss/accuracy operation\n",
    "    parallel_tensors = tf.convert_to_tensor(out_list)\n",
    "    integrated_tensors = tf.reduce_mean(parallel_tensors, axis=0)\n",
    "    loss = integrated_tensors[0]\n",
    "    accuracy = integrated_tensors[1]\n",
    "\n",
    "    # gpu 들에 흩어져있는 gradients 를 통합하고 적용\n",
    "    ## reduce-average gradients via NCCL\n",
    "    grad_list = reduce_average_gradients(grad_list)\n",
    "    ## apply averaged gradients\n",
    "    train_op = apply_gradients(grad_list, var_list, optimizers, n_gpu)\n",
    "\n",
    "    # synchronize operation\n",
    "    sync_op = synchronize_gpus(n_gpu)\n",
    "\n",
    "    # train\n",
    "    avg_train, avg_test = train(X, y, train_op, loss, accuracy, n_epoch, \n",
    "                                train_batch_size, test_batch_size, sync_op=sync_op)\n",
    "    \n",
    "    # write stats\n",
    "    stats_nccl.loc[n_gpu] = [avg_train, avg_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_nccl = pd.DataFrame(columns=[\"#GPUs\", \"train\", \"test\"])\n",
    "stats_nccl = stats_nccl.set_index(\"#GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of copy_ops = 0\n",
      "[ 1/10] (train) acc: 81.12%, loss: 0.629 | (test) acc: 97.41%, loss: 0.084 | 7.48s, 0.47s\n",
      "[ 2/10] (train) acc: 97.85%, loss: 0.069 | (test) acc: 98.67%, loss: 0.044 | 7.31s, 0.41s\n",
      "[ 3/10] (train) acc: 98.64%, loss: 0.045 | (test) acc: 98.86%, loss: 0.033 | 7.38s, 0.42s\n",
      "[ 4/10] (train) acc: 99.06%, loss: 0.030 | (test) acc: 99.11%, loss: 0.030 | 7.39s, 0.41s\n",
      "[ 5/10] (train) acc: 99.21%, loss: 0.024 | (test) acc: 99.10%, loss: 0.030 | 7.34s, 0.41s\n",
      "[ 6/10] (train) acc: 99.38%, loss: 0.020 | (test) acc: 98.90%, loss: 0.035 | 7.33s, 0.43s\n",
      "[ 7/10] (train) acc: 99.54%, loss: 0.015 | (test) acc: 99.18%, loss: 0.025 | 7.37s, 0.41s\n",
      "[ 8/10] (train) acc: 99.63%, loss: 0.012 | (test) acc: 99.12%, loss: 0.032 | 7.34s, 0.41s\n",
      "[ 9/10] (train) acc: 99.67%, loss: 0.010 | (test) acc: 99.21%, loss: 0.025 | 7.33s, 0.41s\n",
      "[10/10] (train) acc: 99.76%, loss: 0.008 | (test) acc: 99.12%, loss: 0.033 | 7.34s, 0.41s\n",
      "Average time\t TRAIN 7.348\t TEST 0.415\n"
     ]
    }
   ],
   "source": [
    "performance_test(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of copy_ops = 42\n",
      "[ 1/10] (train) acc: 80.57%, loss: 0.634 | (test) acc: 97.32%, loss: 0.085 | 4.18s, 0.29s\n",
      "[ 2/10] (train) acc: 97.72%, loss: 0.073 | (test) acc: 98.36%, loss: 0.048 | 3.88s, 0.22s\n",
      "[ 3/10] (train) acc: 98.74%, loss: 0.041 | (test) acc: 98.98%, loss: 0.032 | 3.87s, 0.23s\n",
      "[ 4/10] (train) acc: 99.06%, loss: 0.031 | (test) acc: 98.99%, loss: 0.031 | 3.87s, 0.23s\n",
      "[ 5/10] (train) acc: 99.17%, loss: 0.027 | (test) acc: 98.98%, loss: 0.030 | 3.87s, 0.23s\n",
      "[ 6/10] (train) acc: 99.36%, loss: 0.020 | (test) acc: 99.12%, loss: 0.026 | 3.87s, 0.23s\n",
      "[ 7/10] (train) acc: 99.56%, loss: 0.014 | (test) acc: 98.97%, loss: 0.034 | 3.87s, 0.23s\n",
      "[ 8/10] (train) acc: 99.62%, loss: 0.013 | (test) acc: 99.15%, loss: 0.024 | 3.88s, 0.23s\n",
      "[ 9/10] (train) acc: 99.64%, loss: 0.011 | (test) acc: 99.06%, loss: 0.032 | 3.90s, 0.23s\n",
      "[10/10] (train) acc: 99.67%, loss: 0.010 | (test) acc: 99.27%, loss: 0.023 | 3.87s, 0.23s\n",
      "Average time\t TRAIN 3.877\t TEST 0.229\n"
     ]
    }
   ],
   "source": [
    "performance_test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of copy_ops = 126\n",
      "[ 1/10] (train) acc: 81.52%, loss: 0.591 | (test) acc: 97.59%, loss: 0.081 | 2.71s, 0.26s\n",
      "[ 2/10] (train) acc: 97.93%, loss: 0.068 | (test) acc: 98.82%, loss: 0.039 | 2.21s, 0.15s\n",
      "[ 3/10] (train) acc: 98.77%, loss: 0.042 | (test) acc: 98.71%, loss: 0.040 | 2.21s, 0.14s\n",
      "[ 4/10] (train) acc: 99.04%, loss: 0.030 | (test) acc: 98.88%, loss: 0.032 | 2.20s, 0.13s\n",
      "[ 5/10] (train) acc: 99.26%, loss: 0.024 | (test) acc: 99.07%, loss: 0.028 | 2.19s, 0.14s\n",
      "[ 6/10] (train) acc: 99.33%, loss: 0.021 | (test) acc: 98.78%, loss: 0.038 | 2.20s, 0.15s\n",
      "[ 7/10] (train) acc: 99.45%, loss: 0.017 | (test) acc: 99.33%, loss: 0.023 | 2.21s, 0.14s\n",
      "[ 8/10] (train) acc: 99.60%, loss: 0.013 | (test) acc: 98.76%, loss: 0.036 | 2.23s, 0.15s\n",
      "[ 9/10] (train) acc: 99.71%, loss: 0.009 | (test) acc: 99.12%, loss: 0.031 | 2.21s, 0.14s\n",
      "[10/10] (train) acc: 99.70%, loss: 0.010 | (test) acc: 99.07%, loss: 0.033 | 2.21s, 0.15s\n",
      "Average time\t TRAIN 2.208\t TEST 0.143\n"
     ]
    }
   ],
   "source": [
    "performance_test(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of copy_ops = 294\n",
      "[ 1/10] (train) acc: 81.01%, loss: 0.611 | (test) acc: 97.20%, loss: 0.086 | 2.47s, 0.32s\n",
      "[ 2/10] (train) acc: 97.76%, loss: 0.071 | (test) acc: 98.46%, loss: 0.049 | 1.46s, 0.10s\n",
      "[ 3/10] (train) acc: 98.75%, loss: 0.041 | (test) acc: 98.77%, loss: 0.034 | 1.47s, 0.09s\n",
      "[ 4/10] (train) acc: 99.13%, loss: 0.029 | (test) acc: 98.94%, loss: 0.034 | 1.46s, 0.10s\n",
      "[ 5/10] (train) acc: 99.24%, loss: 0.024 | (test) acc: 99.03%, loss: 0.027 | 1.44s, 0.09s\n",
      "[ 6/10] (train) acc: 99.40%, loss: 0.019 | (test) acc: 99.07%, loss: 0.032 | 1.40s, 0.10s\n",
      "[ 7/10] (train) acc: 99.61%, loss: 0.013 | (test) acc: 99.29%, loss: 0.023 | 1.44s, 0.08s\n",
      "[ 8/10] (train) acc: 99.59%, loss: 0.013 | (test) acc: 98.60%, loss: 0.046 | 1.46s, 0.10s\n",
      "[ 9/10] (train) acc: 99.58%, loss: 0.013 | (test) acc: 99.10%, loss: 0.033 | 1.46s, 0.09s\n",
      "[10/10] (train) acc: 99.71%, loss: 0.009 | (test) acc: 99.00%, loss: 0.034 | 1.45s, 0.10s\n",
      "Average time\t TRAIN 1.448\t TEST 0.095\n"
     ]
    }
   ],
   "source": [
    "performance_test(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >train</th> \n",
       "        <th class=\"col_heading level0 col1\" >test</th> \n",
       "        <th class=\"col_heading level0 col2\" >train %</th> \n",
       "        <th class=\"col_heading level0 col3\" >test %</th> \n",
       "    </tr>    <tr> \n",
       "        <th class=\"index_name level0\" >#GPUs</th> \n",
       "        <th class=\"blank\" ></th> \n",
       "        <th class=\"blank\" ></th> \n",
       "        <th class=\"blank\" ></th> \n",
       "        <th class=\"blank\" ></th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672level0_row0\" class=\"row_heading level0 row0\" >1</th> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row0_col0\" class=\"data row0 col0\" >7.234</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row0_col1\" class=\"data row0 col1\" >0.412</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row0_col2\" class=\"data row0 col2\" >100%</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row0_col3\" class=\"data row0 col3\" >100%</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672level0_row1\" class=\"row_heading level0 row1\" >2</th> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row1_col0\" class=\"data row1 col0\" >3.902</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row1_col1\" class=\"data row1 col1\" >0.231</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row1_col2\" class=\"data row1 col2\" >185%</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row1_col3\" class=\"data row1 col3\" >178%</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672level0_row2\" class=\"row_heading level0 row2\" >4</th> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row2_col0\" class=\"data row2 col0\" >2.311</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row2_col1\" class=\"data row2 col1\" >0.242</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row2_col2\" class=\"data row2 col2\" >313%</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row2_col3\" class=\"data row2 col3\" >170%</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672level0_row3\" class=\"row_heading level0 row3\" >8</th> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row3_col0\" class=\"data row3 col0\" >2.173</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row3_col1\" class=\"data row3 col1\" >0.162</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row3_col2\" class=\"data row3 col2\" >333%</td> \n",
       "        <td id=\"T_937cc37a_3259_11e8_92ba_ac1f6b26d672row3_col3\" class=\"data row3 col3\" >254%</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8be0282cf8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >train</th> \n",
       "        <th class=\"col_heading level0 col1\" >test</th> \n",
       "        <th class=\"col_heading level0 col2\" >train %</th> \n",
       "        <th class=\"col_heading level0 col3\" >test %</th> \n",
       "    </tr>    <tr> \n",
       "        <th class=\"index_name level0\" >#GPUs</th> \n",
       "        <th class=\"blank\" ></th> \n",
       "        <th class=\"blank\" ></th> \n",
       "        <th class=\"blank\" ></th> \n",
       "        <th class=\"blank\" ></th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672level0_row0\" class=\"row_heading level0 row0\" >1</th> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row0_col0\" class=\"data row0 col0\" >7.348</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row0_col1\" class=\"data row0 col1\" >0.415</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row0_col2\" class=\"data row0 col2\" >100%</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row0_col3\" class=\"data row0 col3\" >100%</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672level0_row1\" class=\"row_heading level0 row1\" >2</th> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row1_col0\" class=\"data row1 col0\" >3.877</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row1_col1\" class=\"data row1 col1\" >0.229</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row1_col2\" class=\"data row1 col2\" >190%</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row1_col3\" class=\"data row1 col3\" >181%</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672level0_row2\" class=\"row_heading level0 row2\" >4</th> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row2_col0\" class=\"data row2 col0\" >2.208</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row2_col1\" class=\"data row2 col1\" >0.143</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row2_col2\" class=\"data row2 col2\" >333%</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row2_col3\" class=\"data row2 col3\" >290%</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672level0_row3\" class=\"row_heading level0 row3\" >8</th> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row3_col0\" class=\"data row3 col0\" >1.448</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row3_col1\" class=\"data row3 col1\" >0.095</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row3_col2\" class=\"data row3 col2\" >507%</td> \n",
       "        <td id=\"T_937e695a_3259_11e8_92ba_ac1f6b26d672row3_col3\" class=\"data row3 col3\" >435%</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8be021d828>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_print(stats_nccl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance analysis\n",
    "\n",
    "- 유의미한 성능개선\n",
    "- 왜 여전히 507%?\n",
    "    1. 네트워크 크기\n",
    "    2. input data split\n",
    "    3. feed_dict\n",
    "- 위 세가지 수정해서 변화를 봐야 함\n",
    "    1. 네트워크 크기를 늘리면 유의미한 성능향상이 관측됨\n",
    "- 기타 개선가능사항\n",
    "    - gradient average 를 안 하고 LR 을 /n_gpu 해도 됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
