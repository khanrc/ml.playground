{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilling the Knowledge in a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Paper\n",
    "    * Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 (2015).\n",
    "* Problem\n",
    "    * 앙상블 모델은 강력하지만 무거움. 학습할 때 빡센거야 그렇다 치더라도 사용할 때 (인퍼런스 할 때) 는 가벼워야 실제로 쓸 수 있다. 하지만 앙상블 모델은 네트워크 자체를 여러개를 불러와야 하고, 다 돌려봐야 하므로 N개의 네트워크를 사용해서 앙상블을 하면 그대로 N배 무거워진다.\n",
    "* Method\n",
    "    * 따라서 이 논문에서는 앙상블 모델을 학습한 후 그 'knowledge' 를 single model 로 'distilling' 한다. 즉 모델 하나로 앙상블 모델의 knowledge 를 학습하는 것.\n",
    "    * 이를 위해 싱글 모델에서 앙상블 모델의 knowledge 에 해당하는 probability (softmax) 를 학습한다. \n",
    "    * 다만, softmax 결과는 너무 confident 한 경향이 있으므로 이를 해결하기 위해 soft target 을 사용한다. \n",
    "        * soft target: $q_i={exp(z_i/T) \\over \\sum_j exp(z_j/T)}$\n",
    "    * 이렇게 하면 probability 가 너무 confident 해지는걸 막고 soft 하게 만들어줄 수 있다.\n",
    "    * single model 은 이 soft target 학습. 학습할때도 마찬가지로 soft target 을 사용하고, inference 때에는 T=1 로 추정한다.\n",
    "        * inference 할때 T=1 로 하는게 의미가 있나? 어차피 classification 결과는 T와 상관없이 동일한데.\n",
    "        * 내가 이해를 잘못한건가?.?\n",
    "    * 이러한 soft target 만 가지고 학습을 하는 것은 아니고, original true target (original label) 도 같이 사용함.\n",
    "    * alpha 로 가중치를 두고 weighted sum 을 하는데, 논문에서는 0.5로 동등하게 사용하였음.\n",
    "* Details\n",
    "    * T=2~5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# This codes are based on DeepLearningZeroToAll 11-5.\n",
    "\n",
    "# Lab 11 MNIST and Deep learning CNN\n",
    "# https://www.tensorflow.org/tutorials/layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, sess, name, lr=0.001, drop_rate=0.7, init_fn=None):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self.lr = lr\n",
    "        self.drop_rate = drop_rate\n",
    "        self.init_fn = init_fn\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
    "            # for testing\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "            # img 28x28x1 (black/white), Input Layer\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "            # Convolutional Layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n",
    "                                     kernel_initializer=self.init_fn,\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            # Pooling Layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
    "                                         rate=self.drop_rate, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n",
    "                                     kernel_initializer=self.init_fn,\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
    "                                         rate=self.drop_rate, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #3 and Pooling Layer #3\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n",
    "                                     kernel_initializer=self.init_fn,\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
    "                                         rate=self.drop_rate, training=self.training)\n",
    "\n",
    "            # Dense Layer with Relu\n",
    "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
    "            dense4 = tf.layers.dense(inputs=flat, kernel_initializer=self.init_fn,\n",
    "                                     units=625, activation=tf.nn.relu)\n",
    "            dropout4 = tf.layers.dropout(inputs=dense4,\n",
    "                                         rate=self.drop_rate, training=self.training)\n",
    "\n",
    "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
    "            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n",
    "\n",
    "        # define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        return self.sess.run(self.logits,\n",
    "                             feed_dict={self.X: x_test, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy,\n",
    "                             feed_dict={self.X: x_test,\n",
    "                                        self.Y: y_test, self.training: training})\n",
    "\n",
    "    def train(self, x_data, y_data, training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data, self.training: training})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 50\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "models = []\n",
    "# num_models = 5\n",
    "models.append(Model(sess, \"model1\", lr=0.001, drop_rate=0.5, init_fn=None))\n",
    "models.append(Model(sess, \"model2\", lr=0.001, drop_rate=0.5, init_fn=tf.contrib.layers.xavier_initializer()))\n",
    "models.append(Model(sess, \"model3\", lr=0.001, drop_rate=0.3, init_fn=tf.contrib.layers.xavier_initializer()))\n",
    "models.append(Model(sess, \"model4\", lr=0.0007, drop_rate=0.3, init_fn=tf.contrib.layers.xavier_initializer()))\n",
    "models.append(Model(sess, \"model5\", lr=0.0007, drop_rate=0.3, init_fn=tf.contrib.layers.variance_scaling_initializer()))\n",
    "# for m in range(num_models):\n",
    "#     models.append(Model(sess, \"model\" + str(m)))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Started!\n",
      "('Epoch:', '0001', 'cost =', array([ 0.42743961,  0.409541  ,  0.25891181,  0.25897675,  0.47106397]))\n",
      "('Epoch:', '0002', 'cost =', array([ 0.14201703,  0.13348314,  0.08126428,  0.07938832,  0.11699478]))\n",
      "('Epoch:', '0003', 'cost =', array([ 0.10888712,  0.10356844,  0.05945898,  0.06044351,  0.08238382]))\n",
      "('Epoch:', '0004', 'cost =', array([ 0.09428023,  0.09103809,  0.05287405,  0.05060412,  0.07044538]))\n",
      "('Epoch:', '0005', 'cost =', array([ 0.08647937,  0.08050445,  0.04370158,  0.04372451,  0.0595039 ]))\n",
      "('Epoch:', '0006', 'cost =', array([ 0.0781533 ,  0.07459385,  0.04158848,  0.0391683 ,  0.05317622]))\n",
      "('Epoch:', '0007', 'cost =', array([ 0.07741929,  0.06749518,  0.03583228,  0.03475736,  0.04706765]))\n",
      "('Epoch:', '0008', 'cost =', array([ 0.06977531,  0.06747254,  0.0329352 ,  0.03268618,  0.04120996]))\n",
      "('Epoch:', '0009', 'cost =', array([ 0.06703566,  0.06423782,  0.0320037 ,  0.03037934,  0.03977831]))\n",
      "('Epoch:', '0010', 'cost =', array([ 0.06361486,  0.06003331,  0.03046537,  0.02938873,  0.03986276]))\n",
      "test acc: [0.9920001, 0.9922002, 0.99140012, 0.99440014, 0.99300015]\n",
      "('Epoch:', '0011', 'cost =', array([ 0.06443227,  0.06120394,  0.02802884,  0.02717826,  0.0343718 ]))\n",
      "('Epoch:', '0012', 'cost =', array([ 0.0594551 ,  0.05916766,  0.02673037,  0.02628188,  0.03421778]))\n",
      "('Epoch:', '0013', 'cost =', array([ 0.06068203,  0.05699556,  0.02433492,  0.02554404,  0.03186266]))\n",
      "('Epoch:', '0014', 'cost =', array([ 0.0586173 ,  0.05385433,  0.0242713 ,  0.02237498,  0.03134871]))\n",
      "('Epoch:', '0015', 'cost =', array([ 0.0578081 ,  0.05690427,  0.02290435,  0.02227108,  0.03014113]))\n",
      "('Epoch:', '0016', 'cost =', array([ 0.05439357,  0.05428303,  0.02057556,  0.02170113,  0.02759973]))\n",
      "('Epoch:', '0017', 'cost =', array([ 0.05273723,  0.05047418,  0.02197145,  0.02092083,  0.02484838]))\n",
      "('Epoch:', '0018', 'cost =', array([ 0.05290258,  0.05092086,  0.02172169,  0.01888361,  0.02590371]))\n",
      "('Epoch:', '0019', 'cost =', array([ 0.05127216,  0.05244644,  0.01870837,  0.01916676,  0.02426973]))\n",
      "('Epoch:', '0020', 'cost =', array([ 0.05306031,  0.04907098,  0.01863123,  0.01830071,  0.02365705]))\n",
      "test acc: [0.99410015, 0.99420011, 0.99370009, 0.99470013, 0.99460012]\n",
      "('Epoch:', '0021', 'cost =', array([ 0.05206843,  0.04833094,  0.01865631,  0.01883126,  0.02173768]))\n",
      "('Epoch:', '0022', 'cost =', array([ 0.05272117,  0.04855557,  0.01863025,  0.01824787,  0.02256474]))\n",
      "('Epoch:', '0023', 'cost =', array([ 0.05024505,  0.04805015,  0.01954564,  0.01769143,  0.02115823]))\n",
      "('Epoch:', '0024', 'cost =', array([ 0.05035518,  0.04810045,  0.01825573,  0.01606212,  0.02153582]))\n",
      "('Epoch:', '0025', 'cost =', array([ 0.04961735,  0.04793854,  0.01764939,  0.0176619 ,  0.01996983]))\n",
      "('Epoch:', '0026', 'cost =', array([ 0.04990832,  0.04647798,  0.01580079,  0.01609507,  0.01933755]))\n",
      "('Epoch:', '0027', 'cost =', array([ 0.04880371,  0.04585653,  0.01713317,  0.01724074,  0.01996755]))\n",
      "('Epoch:', '0028', 'cost =', array([ 0.0463843 ,  0.0451649 ,  0.01496632,  0.01488981,  0.0212609 ]))\n",
      "('Epoch:', '0029', 'cost =', array([ 0.04788653,  0.04541442,  0.01734648,  0.01359289,  0.01804631]))\n",
      "('Epoch:', '0030', 'cost =', array([ 0.0457846 ,  0.04593569,  0.01596338,  0.01492712,  0.01749746]))\n",
      "test acc: [0.99340016, 0.99410015, 0.99410021, 0.99460018, 0.99390018]\n",
      "('Epoch:', '0031', 'cost =', array([ 0.0440922 ,  0.04497011,  0.01636769,  0.01270872,  0.01769131]))\n",
      "('Epoch:', '0032', 'cost =', array([ 0.04677463,  0.04534673,  0.01755083,  0.0141348 ,  0.0172455 ]))\n",
      "('Epoch:', '0033', 'cost =', array([ 0.0470585 ,  0.04467146,  0.01341717,  0.01349698,  0.01729736]))\n",
      "('Epoch:', '0034', 'cost =', array([ 0.04727656,  0.04579255,  0.01516281,  0.01580245,  0.01942113]))\n",
      "('Epoch:', '0035', 'cost =', array([ 0.04515491,  0.04398266,  0.01356433,  0.01503487,  0.01857442]))\n",
      "('Epoch:', '0036', 'cost =', array([ 0.04617831,  0.04588848,  0.01226757,  0.01519436,  0.01694543]))\n",
      "('Epoch:', '0037', 'cost =', array([ 0.04559183,  0.04286989,  0.01553429,  0.01290411,  0.01722446]))\n",
      "('Epoch:', '0038', 'cost =', array([ 0.04428078,  0.04764096,  0.01258042,  0.01435553,  0.0161237 ]))\n",
      "('Epoch:', '0039', 'cost =', array([ 0.04409582,  0.04038383,  0.01492923,  0.01258549,  0.01807003]))\n",
      "('Epoch:', '0040', 'cost =', array([ 0.04562036,  0.04174392,  0.01460057,  0.01402162,  0.01507494]))\n",
      "test acc: [0.99380016, 0.99410015, 0.99520016, 0.99350011, 0.99360013]\n",
      "('Epoch:', '0041', 'cost =', array([ 0.04480023,  0.04374539,  0.01245675,  0.01482401,  0.01551842]))\n",
      "('Epoch:', '0042', 'cost =', array([ 0.04163416,  0.04331816,  0.01409321,  0.01263448,  0.01776118]))\n",
      "('Epoch:', '0043', 'cost =', array([ 0.04328991,  0.04519729,  0.01451063,  0.01371008,  0.01456064]))\n",
      "('Epoch:', '0044', 'cost =', array([ 0.04518123,  0.04485223,  0.01276146,  0.01315024,  0.01561861]))\n",
      "('Epoch:', '0045', 'cost =', array([ 0.04504646,  0.0413971 ,  0.01362506,  0.01199184,  0.01483857]))\n",
      "('Epoch:', '0046', 'cost =', array([ 0.04225877,  0.04261815,  0.01322701,  0.01285502,  0.01512155]))\n",
      "('Epoch:', '0047', 'cost =', array([ 0.04343155,  0.04195322,  0.01472974,  0.01233714,  0.01554765]))\n",
      "('Epoch:', '0048', 'cost =', array([ 0.04331777,  0.04395222,  0.0133644 ,  0.01289228,  0.01423134]))\n",
      "('Epoch:', '0049', 'cost =', array([ 0.04097044,  0.04035153,  0.0124816 ,  0.01388904,  0.01505017]))\n",
      "('Epoch:', '0050', 'cost =', array([ 0.04353807,  0.04388178,  0.01192712,  0.01283847,  0.01638858]))\n",
      "test acc: [0.99380016, 0.9951002, 0.99410015, 0.99410015, 0.99430013]\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "print('Learning Started!')\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost_list = np.zeros(len(models))\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # train each model\n",
    "        for m_idx, m in enumerate(models):\n",
    "            c, _ = m.train(batch_xs, batch_ys)\n",
    "            avg_cost_list[m_idx] += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
    "    \n",
    "    if epoch%10 == 9:\n",
    "        test_accs = []\n",
    "        for m_idx, m in enumerate(models):\n",
    "            test_accs.append(m.get_accuracy(mnist.test.images, mnist.test.labels))\n",
    "        print('test acc: {}'.format(test_accs))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Accuracy:', 0.99380016)\n",
      "(1, 'Accuracy:', 0.9951002)\n",
      "(2, 'Accuracy:', 0.99410015)\n",
      "(3, 'Accuracy:', 0.99410015)\n",
      "(4, 'Accuracy:', 0.99430025)\n",
      "('Ensemble accuracy:', 0.9957)\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "test_size = len(mnist.test.labels)\n",
    "predictions = np.zeros(test_size * 10).reshape(test_size, 10)\n",
    "for m_idx, m in enumerate(models):\n",
    "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
    "        mnist.test.images, mnist.test.labels))\n",
    "    p = m.predict(mnist.test.images)\n",
    "    predictions += p\n",
    "\n",
    "ensemble_correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
    "ensemble_accuracy = tf.reduce_mean(tf.cast(ensemble_correct_prediction, tf.float32))\n",
    "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Distilling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# same network architecture with Model\n",
    "class DistillModel:\n",
    "    def __init__(self, sess, name, lr=0.001, drop_rate=0.7, init_fn=None, alpha=0.5):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self.lr = lr\n",
    "        self.drop_rate = drop_rate\n",
    "        self.init_fn = init_fn\n",
    "#         self.T = T # temperature\n",
    "        self.alpha = alpha\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
    "            # for testing\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            self.T = tf.placeholder(tf.float32) # temperature\n",
    "\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "            # img 28x28x1 (black/white), Input Layer\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.soft_target = tf.placeholder(tf.float32, [None, 10]) # soft target\n",
    "            self.true_target = tf.placeholder(tf.float32, [None, 10]) # true target\n",
    "\n",
    "            # Convolutional Layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n",
    "                                     kernel_initializer=self.init_fn,\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            # Pooling Layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
    "                                         rate=self.drop_rate, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n",
    "                                     kernel_initializer=self.init_fn,\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
    "                                         rate=self.drop_rate, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #3 and Pooling Layer #3\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n",
    "                                     kernel_initializer=self.init_fn,\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
    "                                         rate=self.drop_rate, training=self.training)\n",
    "\n",
    "            # Dense Layer with Relu\n",
    "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
    "            dense4 = tf.layers.dense(inputs=flat, kernel_initializer=self.init_fn,\n",
    "                                     units=625, activation=tf.nn.relu)\n",
    "            dropout4 = tf.layers.dropout(inputs=dense4,\n",
    "                                         rate=self.drop_rate, training=self.training)\n",
    "\n",
    "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
    "            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n",
    "            \n",
    "        # define cost/loss & optimizer\n",
    "        self.soft_logits = self.logits / self.T\n",
    "        soft_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=self.soft_logits, labels=self.soft_target))\n",
    "        true_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=self.soft_logits, labels=self.true_target))\n",
    "        self.cost = self.alpha*soft_cost + (1-self.alpha)*true_cost\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(self.soft_logits, 1), tf.argmax(self.true_target, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    def predict(self, x_test, training=False, T=1):\n",
    "        return self.sess.run(self.soft_logits,\n",
    "                             feed_dict={self.X: x_test, self.training: training, self.T: T})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False, T=1):\n",
    "        return self.sess.run(self.accuracy,\n",
    "                             feed_dict={self.X: x_test,\n",
    "                                        self.true_target: y_test, \n",
    "                                        self.training: training, \n",
    "                                        self.T: T})\n",
    "\n",
    "    def train(self, x_data, soft_target, true_target, training=True, T=3):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X: x_data, \n",
    "            self.soft_target: soft_target, \n",
    "            self.true_target: true_target,\n",
    "            self.training: training, \n",
    "            self.T: T\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_soft_targets(logits, T=3):\n",
    "    return softmax(logits/T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 먼저 train dataset 에 대해 ensemble model 의 평균 logits 을 구한다.\n",
    "* 그리고 그걸 soft_target 으로 변환. \n",
    "* 그러면 train_y 대신 soft_target 을 target 으로 하여 distilled model 을 학습!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get train data\n"
     ]
    }
   ],
   "source": [
    "print('Get train data')\n",
    "# mnist.train 을 사용할 수 없으므로, train dataset 을 먼저 확보한다\n",
    "\n",
    "train_images = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "N = len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make soft target\n"
     ]
    }
   ],
   "source": [
    "print('Make soft target')\n",
    "\n",
    "# avg_logits = np.array()\n",
    "batch_size = 1000\n",
    "logits = np.zeros([N, 10])\n",
    "\n",
    "for i, m in enumerate(models):\n",
    "    for j in range(0, N, batch_size):\n",
    "        batch_x = train_images[j:j+batch_size]\n",
    "        cur_logits = m.predict(batch_x)\n",
    "        logits[j:j+batch_size] += cur_logits\n",
    "\n",
    "# logit averaging is right?\n",
    "logits /= len(models)\n",
    "soft_targets = get_soft_targets(logits, T=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test soft_targets\n",
    "# shape\n",
    "assert soft_targets.shape == train_labels.shape\n",
    "# sum of softmax = 1\n",
    "v = np.sum(soft_targets, axis=1) - 1\n",
    "assert (v < 1e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distilling model\n",
    "# 0.9934 params\n",
    "distill_model = DistillModel(sess, \"distill-model\", lr=0.001, drop_rate=0.3, \n",
    "                             init_fn=tf.contrib.layers.xavier_initializer(), alpha=0.5)\n",
    "# 이름이 distill-model2 인 이유는... distill-model 을 만들었는데 잘못만듦. 그래서 지우고 싶었는데 \n",
    "# tf.Graph() 는 append-only structure 래. -.-\n",
    "\n",
    "# 그럼 만약 이걸 수정하고 싶다면 어떻게 해야 할까?\n",
    "# 천천히 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Started!\n",
      "('Epoch:', '0001', 'cost =', 0.30836233073337527)\n",
      "('Epoch:', '0002', 'cost =', 0.1139974741231312)\n",
      "('Epoch:', '0003', 'cost =', 0.093903585540300052)\n",
      "('Epoch:', '0004', 'cost =', 0.082794651741331299)\n",
      "('Epoch:', '0005', 'cost =', 0.077219824012030189)\n",
      "('Epoch:', '0006', 'cost =', 0.073890537683936705)\n",
      "('Epoch:', '0007', 'cost =', 0.070381064838306473)\n",
      "('Epoch:', '0008', 'cost =', 0.068570296476510528)\n",
      "('Epoch:', '0009', 'cost =', 0.065557966642081772)\n",
      "('Epoch:', '0010', 'cost =', 0.063093038777058769)\n",
      "Accuracy: 0.994400143623\n",
      "('Epoch:', '0011', 'cost =', 0.062747025828469943)\n",
      "('Epoch:', '0012', 'cost =', 0.061259193037721253)\n",
      "('Epoch:', '0013', 'cost =', 0.060055479197339535)\n",
      "('Epoch:', '0014', 'cost =', 0.059618993035771638)\n",
      "('Epoch:', '0015', 'cost =', 0.058081561845134605)\n",
      "('Epoch:', '0016', 'cost =', 0.058570654551413852)\n",
      "('Epoch:', '0017', 'cost =', 0.0565283328497952)\n",
      "('Epoch:', '0018', 'cost =', 0.056902495267039041)\n",
      "('Epoch:', '0019', 'cost =', 0.055820776640691516)\n",
      "('Epoch:', '0020', 'cost =', 0.056306415251032811)\n",
      "Accuracy: 0.994800150394\n",
      "('Epoch:', '0021', 'cost =', 0.056030313440344579)\n",
      "('Epoch:', '0022', 'cost =', 0.054584170651029475)\n",
      "('Epoch:', '0023', 'cost =', 0.054777775098654391)\n",
      "('Epoch:', '0024', 'cost =', 0.0538275500657883)\n",
      "('Epoch:', '0025', 'cost =', 0.05392855833199886)\n",
      "('Epoch:', '0026', 'cost =', 0.054040510498664616)\n",
      "('Epoch:', '0027', 'cost =', 0.054162985042414923)\n",
      "('Epoch:', '0028', 'cost =', 0.053230015256188132)\n",
      "('Epoch:', '0029', 'cost =', 0.053094808107072698)\n",
      "('Epoch:', '0030', 'cost =', 0.052782218578186893)\n",
      "Accuracy: 0.994800209999\n",
      "('Epoch:', '0031', 'cost =', 0.052478813251311165)\n",
      "('Epoch:', '0032', 'cost =', 0.052637803473255834)\n",
      "('Epoch:', '0033', 'cost =', 0.052425846755504622)\n",
      "('Epoch:', '0034', 'cost =', 0.052444977259094111)\n",
      "('Epoch:', '0035', 'cost =', 0.051709454289891439)\n",
      "('Epoch:', '0036', 'cost =', 0.052028576704588832)\n",
      "('Epoch:', '0037', 'cost =', 0.051652751165357509)\n",
      "('Epoch:', '0038', 'cost =', 0.052036745378916921)\n",
      "('Epoch:', '0039', 'cost =', 0.051664554602043243)\n",
      "('Epoch:', '0040', 'cost =', 0.051684090854092049)\n",
      "Accuracy: 0.995000123978\n",
      "('Epoch:', '0041', 'cost =', 0.051630059013312489)\n",
      "('Epoch:', '0042', 'cost =', 0.051141415461897828)\n",
      "('Epoch:', '0043', 'cost =', 0.051824788173491322)\n",
      "('Epoch:', '0044', 'cost =', 0.051456562107936928)\n",
      "('Epoch:', '0045', 'cost =', 0.050777713639492413)\n",
      "('Epoch:', '0046', 'cost =', 0.051582663655281059)\n",
      "('Epoch:', '0047', 'cost =', 0.051153575568036624)\n",
      "('Epoch:', '0048', 'cost =', 0.050968016596002988)\n",
      "('Epoch:', '0049', 'cost =', 0.050901702229272226)\n",
      "('Epoch:', '0050', 'cost =', 0.050750510411506369)\n",
      "Accuracy: 0.996000111103\n",
      "('Epoch:', '0051', 'cost =', 0.050418507822535222)\n",
      "('Epoch:', '0052', 'cost =', 0.050983344136991265)\n",
      "('Epoch:', '0053', 'cost =', 0.050676741335879717)\n",
      "('Epoch:', '0054', 'cost =', 0.050438054728914328)\n",
      "('Epoch:', '0055', 'cost =', 0.050759524970569417)\n",
      "('Epoch:', '0056', 'cost =', 0.049806876551698513)\n",
      "('Epoch:', '0057', 'cost =', 0.050369270447302966)\n",
      "('Epoch:', '0058', 'cost =', 0.049983533749526174)\n",
      "('Epoch:', '0059', 'cost =', 0.050392209176312805)\n",
      "('Epoch:', '0060', 'cost =', 0.050297921682623321)\n",
      "Accuracy: 0.995200157166\n",
      "('Epoch:', '0061', 'cost =', 0.050052808540111252)\n",
      "('Epoch:', '0062', 'cost =', 0.050071956830268598)\n",
      "('Epoch:', '0063', 'cost =', 0.050109391984614525)\n",
      "('Epoch:', '0064', 'cost =', 0.05009231174872681)\n",
      "('Epoch:', '0065', 'cost =', 0.050163012197749164)\n",
      "('Epoch:', '0066', 'cost =', 0.050292935913259326)\n",
      "('Epoch:', '0067', 'cost =', 0.04981273637915197)\n",
      "('Epoch:', '0068', 'cost =', 0.049397855560210542)\n",
      "('Epoch:', '0069', 'cost =', 0.049563027102161518)\n",
      "('Epoch:', '0070', 'cost =', 0.04974767276847903)\n",
      "Accuracy: 0.995100140572\n",
      "('Epoch:', '0071', 'cost =', 0.049989168955521149)\n",
      "('Epoch:', '0072', 'cost =', 0.049485841491682954)\n",
      "('Epoch:', '0073', 'cost =', 0.050102144649082948)\n",
      "('Epoch:', '0074', 'cost =', 0.04959547300569038)\n",
      "('Epoch:', '0075', 'cost =', 0.049668414870446403)\n",
      "('Epoch:', '0076', 'cost =', 0.049866507043215318)\n",
      "('Epoch:', '0077', 'cost =', 0.049384320476515746)\n",
      "('Epoch:', '0078', 'cost =', 0.049371279108930702)\n",
      "('Epoch:', '0079', 'cost =', 0.049743268168108042)\n",
      "('Epoch:', '0080', 'cost =', 0.049804958044128017)\n",
      "Accuracy: 0.995100140572\n",
      "('Epoch:', '0081', 'cost =', 0.049726562445813943)\n",
      "('Epoch:', '0082', 'cost =', 0.049665251943198145)\n",
      "('Epoch:', '0083', 'cost =', 0.049161500348286195)\n",
      "('Epoch:', '0084', 'cost =', 0.04890008641237565)\n",
      "('Epoch:', '0085', 'cost =', 0.049044041630219373)\n",
      "('Epoch:', '0086', 'cost =', 0.049079775762828876)\n",
      "('Epoch:', '0087', 'cost =', 0.049550726640630899)\n",
      "('Epoch:', '0088', 'cost =', 0.049512193199585781)\n",
      "('Epoch:', '0089', 'cost =', 0.049073321853171717)\n",
      "('Epoch:', '0090', 'cost =', 0.048885529728775641)\n",
      "Accuracy: 0.995300114155\n",
      "('Epoch:', '0091', 'cost =', 0.04889634329148311)\n",
      "('Epoch:', '0092', 'cost =', 0.049229127751155351)\n",
      "('Epoch:', '0093', 'cost =', 0.049347566484727677)\n",
      "('Epoch:', '0094', 'cost =', 0.049172124283557619)\n",
      "('Epoch:', '0095', 'cost =', 0.049601123299111018)\n",
      "('Epoch:', '0096', 'cost =', 0.049337816682051519)\n",
      "('Epoch:', '0097', 'cost =', 0.04876693725585942)\n",
      "('Epoch:', '0098', 'cost =', 0.048838545507328052)\n",
      "('Epoch:', '0099', 'cost =', 0.049125455092977402)\n",
      "('Epoch:', '0100', 'cost =', 0.048868119855496057)\n",
      "Accuracy: 0.995200097561\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "# 이런식으로도 초기화가 가능한가보다! => 안되는가보다!\n",
    "# var_distill = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"distill-model\")\n",
    "# sess.run(tf.variables_initializer(var_list=var_distill))\n",
    "\n",
    "batch_size = 100\n",
    "training_epochs = 100\n",
    "# 앙상블 모델은 각각 20번씩 학습하였으나 얘는 좀더 해도 되겠지.\n",
    "# 더 하면 좀 이상한것도 같다. 애초에 앙상블 모델을 최대한 많이 해서 수렴할때까지 해줘야 제대로 된 검증인 듯! 그리고 얘도 수렴할때까지 하고.\n",
    "# 더 하면 왜 이상하냐면 애초에 앙상블에서 사용한 모델들이 수렴을 안했으니 (실제로 20번만 하면 그러함) 더 학습시키면 당연히 어큐러시가 올라감\n",
    "\n",
    "print('Learning Started!')\n",
    "\n",
    "# train_data for data shuffling\n",
    "train_data = np.concatenate([train_images, train_labels, soft_targets], axis=1)\n",
    "assert train_data.shape == (N, 804) # (55000, 784+10+10)\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    np.random.shuffle(train_data)\n",
    "    \n",
    "    avg_cost = 0\n",
    "    for i in range(0, N, batch_size):\n",
    "        batch_xs = train_data[i:i+batch_size, :784]\n",
    "        batch_true_targets = train_data[i:i+batch_size, 784:794]\n",
    "        batch_soft_targets = train_data[i:i+batch_size, 794:804]\n",
    "        c, _ = distill_model.train(batch_xs, soft_target=batch_soft_targets, true_target=batch_true_targets, T=3)\n",
    "        avg_cost += c / (N // batch_size)\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost)\n",
    "    \n",
    "    if epoch % 10 == 9:\n",
    "        # check test accuracy\n",
    "        print('Accuracy: {}'.format(distill_model.get_accuracy(mnist.test.images, mnist.test.labels, T=1)))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - tf 1.1",
   "language": "python",
   "name": "python2-tf1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
