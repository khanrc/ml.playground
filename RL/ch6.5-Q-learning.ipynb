{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "왜 Q-learning 은 off-policy 인데 importance sampling 을 사용하지 않는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa\n",
    "\n",
    "![sarsa](sarsa.png)\n",
    "\n",
    "#### python pesudo code\n",
    "\n",
    "on-policy 이므로 target policy = behavior policy = policy 로 두자.\n",
    "\n",
    "```py\n",
    "policy = e_greedy() # e-soft policy\n",
    "for episode in episodes:\n",
    "    # states[0]: initial state\n",
    "    S = states[0]\n",
    "    # 초기상태 S0 에서 Q 에 따라 액션을 선택\n",
    "    A = policy(S, Q)\n",
    "    while not terminal(S):\n",
    "        # state S 에서 action A 를 선택하면 environment 에서 다음 상태 S' 과 reward R 을 리턴\n",
    "        R, S` = env(S, A)\n",
    "        A` = policy(S`, Q)\n",
    "        \n",
    "        # update (gamma: discount factor)\n",
    "        Q[S, A] = Q[S, A] + alpha*(R + gamma*Q[S`, A`] - Q[S, A])\n",
    "        \n",
    "        # go to the next step\n",
    "        S = S`\n",
    "        A = A`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "\n",
    "![q-learning](q-learning.png)\n",
    "\n",
    "#### python pseudo code\n",
    "\n",
    "```py\n",
    "target_policy = greedy()\n",
    "behavior_policy = e_greedy()\n",
    "for episode in episodes:\n",
    "    S = states[0]\n",
    "    while not terminal(S):\n",
    "        A = behavior_policy(S, Q) # e-greedy\n",
    "        R, S` = env(S, A)\n",
    "        A` = target_policy(S`, Q) # greedy\n",
    "        \n",
    "        # update\n",
    "        Q[S, A] = Q[S, A] + alpha*(R + gamma*Q[S`, A`] - Q[S, A])\n",
    "        \n",
    "        # go to the next state\n",
    "        S = S`\n",
    "```\n",
    "\n",
    "여기서 target_policy 가 greedy 이므로 그냥 식에 집어넣어 줄 수도 있다 (책 버전; in-place):\n",
    "\n",
    "```py\n",
    "behavior_policy = e_greedy()\n",
    "for episode in episodes:\n",
    "    S = states[0]\n",
    "    while not terminal(S):\n",
    "        A = behavior_policy(S, Q) # e-greedy\n",
    "        R, S` = env(S, A)\n",
    "        \n",
    "        # update\n",
    "        # max(Q[S`, :]) == greedy(S`, Q).\n",
    "        Q[S, A] = Q[S, A] + alpha*(R + gamma*max(Q[S`, :]) - Q[S, A])\n",
    "        \n",
    "        # go to the next state\n",
    "        S = S`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do not use importance sampling?\n",
    "\n",
    "왜 안쓸까?\n",
    "\n",
    "- 일단 왜 쓰는지를 생각해보자.\n",
    "- MC 에서 behavior policy 는 e-greedy 고, target policy 는 greedy 일 때, behavior policy 로 구한 return 으로 target policy 를 업데이트 해 주기 위해 사용\n",
    "    - importance sampling 을 통해 behavior policy 에 대한 return $G_b$ 를 target policy 에 대한 return $G_\\pi$ 로 바꿈!\n",
    "\n",
    "### Q-learning\n",
    "\n",
    "- Q-learning 에서는 일단 return G 를 직접적으로 다루지 않음. \n",
    "- 하지만 $v_\\pi(s)=\\mathbb E_\\pi [G_t] = \\mathbb E_\\pi[R_{t+1}+\\gamma v_\\pi(S_{t+1})]$.\n",
    "- 즉 max Q(S', a) 를 하게 되면 이게 target policy pi 에 대한 expected return 이므로, 여기다가 importance sampling ratio 를 곱해줘야 할 것 같은데.\n",
    "\n",
    "왜 안하지?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "깨달았다!\n",
    "\n",
    "- MC 에서는 업데이트를 return G 로 함. 따라서 이 G 가 정확해야 한다. \n",
    "    - 그래서 behavior policy 로 구한 $G_b$ 를 target policy 에 대한 return $G_\\pi$ 로 변환하는 작업을 importance sampling 으로 해줌.\n",
    "- 하지만 TD (Q-learning) 에서는 업데이트를 return G 가 아니라 R + Q(S', A') 으로 함.\n",
    "- 여기서 R 은 정확하기 때문에 Q(S', A') 만 정확하면 됨.\n",
    "    - G 와 마찬가지로 결국 이 Q 가 $Q_b$ 냐 $Q_\\pi$ 냐의 문제. \n",
    "    - 우리는 target policy pi 로 Q 를 업데이트하고 있으므로 학습되는 Q 는 $Q_\\pi$ 가 된다.\n",
    "    - 즉 Q 는 이미 $Q_\\pi$ 이므로 굳이 importance sampling 을 통해 변환해 줄 필요가 없는 것!\n",
    "\n",
    "Summary:\n",
    "\n",
    "- MC 에서는 $G_b$ 를 $G_\\pi$ 로 변환하기 위해 importance sampling 을 사용\n",
    "- TD 에서는 이 return G 를 쓰는 것이 아니라 Q 를 씀\n",
    "- 따라서 MC 와 마찬가지로 $Q_b$ 로 $Q_\\pi$ 를 업데이트 하고 싶다면 importance sampling 으로 변환해줘야 하나, Q-learning 에서는 처음부터 $Q_\\pi$ 를 학습하므로 변환해줄 필요가 없음\n",
    "\n",
    "사족:\n",
    "\n",
    "- sarsa 에서 behavior policy 로 A' 을 구한 다음에 Q 를 업데이트 해줄 때 여기서 importance sampling 을 적용해서 업데이트 해주는 방식으로 사용할수는 있을 듯!\n",
    "    - 다만 이렇게 하려면 $Q_b$ 와 $Q_\\pi$ 두 Q table 을 운용해야 할 듯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
