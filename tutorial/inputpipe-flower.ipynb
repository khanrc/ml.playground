{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고\n",
    "\n",
    "이거 `inputpipe.ipynb` 파일처럼 바꾸다가 귀찮아서 중간에 그만둠. 실행해보려면 마저 바꿔야함... 귀찮으면 그냥 `inputpipe.ipynb` 를 참고하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "* 행이 걸리는 경우가 있음\n",
    "    * tfrecords 를 안만들어도 될 때 writer_io 로 열고 그냥 넘어갔음\n",
    "    * 이렇게 되면 데이터가 다 지워짐\n",
    "    * 이상태에서 실행하면 행이 걸림!\n",
    "* TFRecords 파일을 하나로 만들면 shuffle 이 제대로 안됨\n",
    "    * 이걸 여러개로 만들어줘야하는듯\n",
    "    * https://stackoverflow.com/questions/35657015/tfrecords-and-record-shuffling\n",
    "        * sharding your input\n",
    "        * 전체를 다 로드하지 않으면 full-shuffle 은 안됨\n",
    "    * https://stackoverflow.com/questions/34258043/getting-good-mixing-with-many-input-datafiles-in-tensorflow\n",
    "        * `tf.train.shuffle_batch` 를 쓰게 되면 `min_after_dequeue` 를 높게 잡아줘야함. 그래야 셔플이 제대로 된다\n",
    "        * `tf.train.shuffle_batch_join` 를 쓰면 여러 파일에서 인풋을 받을 수 있다. 이걸 쓰는게 좋은가봉가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url, dest=None):\n",
    "    if not dest:\n",
    "        dest = 'data/' + url.split('/')[-1]\n",
    "    urllib.urlretrieve(url, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TF Flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/flower_photos\"):\n",
    "    print(\"Download flower dataset..\")\n",
    "    download_file(url)\n",
    "    print(\"Extracting dataset..\")\n",
    "    tarfile.open(\"data/flower_photos.tgz\", \"r:gz\").extractall(path=\"data/\")\n",
    "#     os.remove(\"data/flower_photos.tgz\") # 굳이..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "remake = False\n",
    "parent_dir = \"data/flower_photos\"\n",
    "train_dir = os.path.join(parent_dir, \"train\")\n",
    "test_dir = os.path.join(parent_dir, \"test\")\n",
    "\n",
    "if not os.path.exists(train_dir) or not os.path.exists(test_dir) or remake:\n",
    "    # make dirs\n",
    "    for label in LABELS:\n",
    "        # tf.gfile.MakeDirs make dir recursively & ignore exist dir\n",
    "        tf.gfile.MakeDirs(os.path.join(train_dir, label))\n",
    "        tf.gfile.MakeDirs(os.path.join(test_dir, label))\n",
    "\n",
    "    # copy files\n",
    "    for i, label in enumerate(LABELS):\n",
    "        dir_name = os.path.join(parent_dir, label)\n",
    "        paths = glob.glob(dir_name + \"/*.jpg\")\n",
    "        num_examples = len(paths)\n",
    "        for j, path in enumerate(paths):\n",
    "            fn = os.path.basename(path)\n",
    "            is_train = j < (num_examples * train_ratio)\n",
    "\n",
    "            if is_train:\n",
    "                to_path = os.path.join(train_dir, label, fn)\n",
    "            else:\n",
    "                to_path = os.path.join(test_dir, label, fn)\n",
    "            \n",
    "            tf.gfile.Copy(path, to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    364 test\r\n",
      "   3306 train\r\n"
     ]
    }
   ],
   "source": [
    "!find ./data/flower_photos/test ./data/flower_photos/train -type f | cut -d/ -f4 | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to `TFRecords` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bytes_features(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def _int64_features(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dir_to_tfrecords(dir_name, tfrecords_path):\n",
    "    '''convert image-containing dir to tfrecords without exist check.\n",
    "    return: # of image files\n",
    "    '''\n",
    "    num_files = 0\n",
    "    with tf.python_io.TFRecordWriter(tfrecords_path) as writer:\n",
    "        for i, label in enumerate(LABELS):\n",
    "            cur_dir = os.path.join(dir_name, label)\n",
    "            paths = glob.glob(cur_dir + \"/*.jpg\")\n",
    "            num_examples = len(paths)\n",
    "            for j, path in enumerate(paths):\n",
    "                im = scipy.misc.imread(path)\n",
    "                im = scipy.misc.imresize(im, [64, 64])\n",
    "\n",
    "                im_raw = im.tostring()\n",
    "                features = {\n",
    "                    \"shape\": _int64_features(im.shape),\n",
    "                    \"image\": _bytes_features([im_raw]),\n",
    "                    \"label\": _int64_features([i])\n",
    "                }\n",
    "\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "                is_train = j < (num_examples * train_ratio)\n",
    "\n",
    "                num_files += 1\n",
    "                writer.write(example.SerializeToString())\n",
    "    \n",
    "    return num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert dataset to TFRecord format..\n",
      "3306 364\n"
     ]
    }
   ],
   "source": [
    "print(\"Convert dataset to TFRecord format..\")\n",
    "\n",
    "tfrecords_train_fn = \"data/flower_photos_train.tfrecords\"\n",
    "tfrecords_test_fn = \"data/flower_photos_test.tfrecords\"\n",
    "\n",
    "num_train = 0\n",
    "num_test = 0\n",
    "\n",
    "remake_tfrecords = False\n",
    "\n",
    "if not (tf.gfile.Exists(tfrecords_train_fn) and tf.gfile.Exists(tfrecords_test_fn) and remake_tfrecords == False):\n",
    "    num_train = dir_to_tfrecords('data/flower_photos/train/', tfrecords_train_fn)\n",
    "    num_test = dir_to_tfrecords('data/flower_photos/test/', tfrecords_test_fn)\n",
    "else:\n",
    "    num_train = 3306\n",
    "    num_test = 364\n",
    "    \n",
    "# how to get num_examples from tfrecords file?\n",
    "print num_train, num_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename_queue):\n",
    "    with tf.variable_scope('read_data'):\n",
    "        reader = tf.TFRecordReader()\n",
    "        key, records = reader.read(filename_queue)\n",
    "        \n",
    "        # parse records\n",
    "        features = tf.parse_single_example(\n",
    "            records,\n",
    "            features={\n",
    "                \"shape\": tf.FixedLenFeature([3], tf.int64),\n",
    "                \"image\": tf.FixedLenFeature([], tf.string),\n",
    "                \"label\": tf.FixedLenFeature([], tf.int64)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        image = tf.decode_raw(features[\"image\"], tf.uint8)\n",
    "        shape = tf.cast(features[\"shape\"], tf.int32)\n",
    "        label = tf.cast(features[\"label\"], tf.int32)\n",
    "\n",
    "        # preproc\n",
    "        image = tf.reshape(image, [64, 64, 3])\n",
    "        image = tf.image.resize_images(images=image, size=[64, 64])\n",
    "        image = tf.cast(image, tf.float32)\n",
    "#         image = resized_image / 255.0\n",
    "        image = tf.image.per_image_standardization(image)\n",
    "\n",
    "        label = tf.one_hot(label, depth=5)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/programmers_guide/reading_data\n",
    "\n",
    "def get_batch_join(tfrecords_path_list, batch_size, shuffle=False, num_threads=5, num_epochs=None):\n",
    "    with tf.variable_scope(\"get_batch_join\"):\n",
    "        # make input pipeline\n",
    "        filename_queue = tf.train.string_input_producer(tfrecords_path_list, shuffle=shuffle, num_epochs=num_epochs)\n",
    "        example_list = [read_data(filename_queue) for _ in range(num_threads)]\n",
    "        \n",
    "        # train case (shuffle)\n",
    "        min_aftter_dequeue = batch_size*10\n",
    "        capacity = min_after_dequeue + 3*batch_size\n",
    "        if shuffle:\n",
    "            images, labels = tf.train.shuffle_batch_join(tensors_list=example_list, batch_size=batch_size,\n",
    "                                                         capacity=capacity, min_after_dequeue=min_after_dequeue,\n",
    "                                                         allow_smaller_final_batch=True)\n",
    "        else:\n",
    "            images, labels = tf.train.batch_join(example_list, batch_size, capacity=capacity, \n",
    "                                                 allow_smaller_final_batch=True)\n",
    "            \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-17-f6ee51bd68f0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-f6ee51bd68f0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def build_nets(name, tfrecords_fn, training, batch_size, batch_shuffle=True, X, y):\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def build_nets(name, X, y, training, batch_shuffle=True):\n",
    "    with tf.variable_scope(name):\n",
    "#         X, y = get_batch(tfrecords_fn, batch_size=batch_size, shuffle=batch_shuffle)\n",
    "\n",
    "        net = X\n",
    "        n_filters = 32\n",
    "        bn_param = {'is_training': training, 'scale': True, 'decay': 0.99}\n",
    "        with slim.arg_scope([slim.conv2d], kernel_size=[3,3],\n",
    "                            normalizer_fn=slim.batch_norm, normalizer_params=bn_param):\n",
    "            for _ in range(3):\n",
    "                net = slim.conv2d(net, n_filters)\n",
    "                net = slim.conv2d(net, n_filters)\n",
    "                net = slim.max_pool2d(net, kernel_size=[2,2], padding='same')\n",
    "                net = slim.dropout(net, 0.7, is_training=training)\n",
    "                n_filters *= 2\n",
    "\n",
    "        flat = slim.flatten(net)\n",
    "        logits = slim.fully_connected(flat, 5, activation_fn=None)\n",
    "        with tf.variable_scope('softmax'):\n",
    "            prob = tf.nn.softmax(logits)\n",
    "\n",
    "        with tf.variable_scope('accuracy'):\n",
    "            correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        with tf.variable_scope('loss'):\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # must do this even with slim\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        \n",
    "        # heavy operation ...\n",
    "#         for var in tf.trainable_variables():\n",
    "#             tf.summary.histogram(var.op.name, var)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        return accuracy, loss, train_op, summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_root_dir = './summary/comparison/'\n",
    "summary_train_dir = os.path.join(summary_root_dir, 'train')\n",
    "summary_test_dir = os.path.join(summary_root_dir, 'test')\n",
    "model_name = 'full-shuffle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# batch_size 를 여기서 정하는 게 이상한가...\n",
    "n_epoch = 60\n",
    "batch_size = 128\n",
    "X, y = get_batch_join(tfrecords_train_fn, batch_size=batch_size, shuffle=True, num_epochs=n_epoch)\n",
    "accuracy, loss, train_op, summary_op = build_nets(model_name, \n",
    "                                                  tfrecords_train_fn, \n",
    "                                                  training=True, \n",
    "                                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 39.97%, loss: 2.7265 - 2s\n",
      "epoch: 1, acc: 51.06%, loss: 1.3215 - 2s\n",
      "epoch: 2, acc: 56.12%, loss: 1.2440 - 2s\n",
      "epoch: 3, acc: 58.50%, loss: 1.1427 - 2s\n",
      "epoch: 4, acc: 60.62%, loss: 1.0805 - 2s\n",
      "epoch: 5, acc: 60.97%, loss: 1.0710 - 2s\n",
      "epoch: 6, acc: 64.00%, loss: 0.9791 - 2s\n",
      "epoch: 7, acc: 64.66%, loss: 0.9756 - 2s\n",
      "epoch: 8, acc: 67.69%, loss: 0.9193 - 2s\n",
      "epoch: 9, acc: 66.22%, loss: 0.9532 - 2s\n",
      "epoch: 10, acc: 69.88%, loss: 0.8479 - 2s\n",
      "epoch: 11, acc: 71.97%, loss: 0.7725 - 2s\n",
      "epoch: 12, acc: 72.91%, loss: 0.7360 - 2s\n",
      "epoch: 13, acc: 76.97%, loss: 0.6455 - 2s\n",
      "epoch: 14, acc: 76.19%, loss: 0.6664 - 2s\n",
      "epoch: 15, acc: 75.66%, loss: 0.6717 - 2s\n",
      "epoch: 16, acc: 77.56%, loss: 0.6189 - 2s\n",
      "epoch: 17, acc: 78.41%, loss: 0.5700 - 2s\n",
      "epoch: 18, acc: 77.75%, loss: 0.6266 - 2s\n",
      "epoch: 19, acc: 79.03%, loss: 0.5840 - 2s\n",
      "epoch: 20, acc: 80.78%, loss: 0.5279 - 2s\n",
      "epoch: 21, acc: 80.12%, loss: 0.5348 - 2s\n",
      "epoch: 22, acc: 79.47%, loss: 0.5476 - 2s\n",
      "epoch: 23, acc: 81.22%, loss: 0.5066 - 2s\n",
      "epoch: 24, acc: 82.06%, loss: 0.4997 - 2s\n",
      "epoch: 25, acc: 83.84%, loss: 0.4438 - 2s\n",
      "epoch: 26, acc: 84.66%, loss: 0.4398 - 2s\n",
      "epoch: 27, acc: 84.94%, loss: 0.4035 - 2s\n",
      "epoch: 28, acc: 86.38%, loss: 0.3849 - 2s\n",
      "epoch: 29, acc: 86.22%, loss: 0.3739 - 2s\n",
      "epoch: 30, acc: 87.34%, loss: 0.3468 - 2s\n",
      "epoch: 31, acc: 88.59%, loss: 0.3074 - 2s\n",
      "epoch: 32, acc: 89.75%, loss: 0.2787 - 2s\n",
      "epoch: 33, acc: 89.91%, loss: 0.2752 - 2s\n",
      "epoch: 34, acc: 89.94%, loss: 0.2798 - 2s\n",
      "epoch: 35, acc: 89.56%, loss: 0.2719 - 2s\n",
      "epoch: 36, acc: 90.75%, loss: 0.2658 - 2s\n",
      "epoch: 37, acc: 90.38%, loss: 0.2636 - 2s\n",
      "epoch: 38, acc: 93.38%, loss: 0.1992 - 2s\n",
      "epoch: 39, acc: 92.59%, loss: 0.2093 - 2s\n",
      "epoch: 40, acc: 92.38%, loss: 0.2070 - 2s\n",
      "epoch: 41, acc: 91.56%, loss: 0.2154 - 2s\n",
      "epoch: 42, acc: 92.84%, loss: 0.2011 - 2s\n",
      "epoch: 43, acc: 91.62%, loss: 0.2145 - 2s\n",
      "epoch: 44, acc: 93.28%, loss: 0.1752 - 2s\n",
      "epoch: 45, acc: 93.16%, loss: 0.1869 - 2s\n",
      "epoch: 46, acc: 93.97%, loss: 0.1639 - 2s\n",
      "epoch: 47, acc: 90.97%, loss: 0.2569 - 2s\n",
      "epoch: 48, acc: 93.84%, loss: 0.1653 - 2s\n",
      "epoch: 49, acc: 94.94%, loss: 0.1481 - 2s\n",
      "epoch: 50, acc: 95.81%, loss: 0.1269 - 2s\n",
      "epoch: 51, acc: 94.66%, loss: 0.1471 - 2s\n",
      "epoch: 52, acc: 95.19%, loss: 0.1375 - 2s\n",
      "epoch: 53, acc: 95.81%, loss: 0.1094 - 2s\n",
      "epoch: 54, acc: 96.03%, loss: 0.1060 - 2s\n",
      "epoch: 55, acc: 95.94%, loss: 0.1165 - 2s\n",
      "epoch: 56, acc: 96.41%, loss: 0.1024 - 2s\n",
      "epoch: 57, acc: 96.88%, loss: 0.0926 - 2s\n",
      "epoch: 58, acc: 96.47%, loss: 0.1030 - 2s\n",
      "epoch: 59, acc: 96.97%, loss: 0.0913 - 2s\n"
     ]
    }
   ],
   "source": [
    "# sess = tf.Session()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=100) # None 으로 하면 안 됨\n",
    "    summary_writer = tf.summary.FileWriter(logdir=summary_train_dir, graph=sess.graph, flush_secs=10)\n",
    "    \n",
    "    iter_per_epoch = num_train // batch_size\n",
    "    global_step = 0\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        avg_acc = 0.\n",
    "        avg_loss = 0.\n",
    "        st = time.time()\n",
    "        for _ in range(iter_per_epoch):\n",
    "            _, cur_summary, cur_acc, cur_loss = sess.run([train_op, summary_op, accuracy, loss])\n",
    "            avg_acc += cur_acc\n",
    "            avg_loss += cur_loss\n",
    "            summary_writer.add_summary(cur_summary, global_step=global_step)\n",
    "            global_step += 1\n",
    "        \n",
    "        avg_acc /= iter_per_epoch\n",
    "        avg_loss /= iter_per_epoch\n",
    "        print \"epoch: {}, acc: {:.2%}, loss: {:.4f} - {:.0f}s\".format(i, avg_acc, avg_loss, time.time()-st)\n",
    "        saver.save(sess, 'checkpoints/flower', global_step=global_step)\n",
    "\n",
    "    summary_writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build evaluation graph\n",
    "tf.reset_default_graph()\n",
    "accuracy, loss, train_op, summary_op = build_nets(model_name, tfrecords_test_fn, training=False, \n",
    "                                                  batch_size=num_test, batch_shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== checkpoints ===\n",
      "model_checkpoint_path: \"checkpoints/flower-1500\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-25\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-50\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-75\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-100\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-125\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-150\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-175\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-200\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-225\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-250\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-275\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-300\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-325\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-350\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-375\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-400\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-425\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-450\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-475\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-500\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-525\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-550\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-575\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-600\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-625\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-650\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-675\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-700\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-725\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-750\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-775\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-800\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-825\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-850\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-875\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-900\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-925\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-950\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-975\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1000\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1025\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1050\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1075\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1100\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1125\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1150\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1175\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1200\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1225\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1250\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1275\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1300\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1325\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1350\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1375\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1400\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1425\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1450\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1475\"\n",
      "all_model_checkpoint_paths: \"checkpoints/flower-1500\"\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-25\n",
      "25 0.260989 1.63365\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-50\n",
      "50 0.244505 1.96436\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-75\n",
      "75 0.244505 2.2277\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-100\n",
      "100 0.244505 2.41832\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-125\n",
      "125 0.244505 2.5258\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-150\n",
      "150 0.244505 3.26384\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-175\n",
      "175 0.244505 2.64891\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-200\n",
      "200 0.244505 3.33088\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-225\n",
      "225 0.247253 2.68086\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-250\n",
      "250 0.247253 3.30027\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-275\n",
      "275 0.260989 3.24457\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-300\n",
      "300 0.277473 2.50592\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-325\n",
      "325 0.266484 3.14996\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-350\n",
      "350 0.318681 3.08794\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-375\n",
      "375 0.370879 2.08841\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-400\n",
      "400 0.32967 2.99666\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-425\n",
      "425 0.414835 1.74789\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-450\n",
      "450 0.508242 1.3063\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-475\n",
      "475 0.565934 1.17752\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-500\n",
      "500 0.568681 1.33569\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-525\n",
      "525 0.626374 1.04527\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-550\n",
      "550 0.651099 1.04695\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-575\n",
      "575 0.714286 0.862946\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-600\n",
      "600 0.631868 1.00822\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-625\n",
      "625 0.664835 0.881258\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-650\n",
      "650 0.711538 0.812902\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-675\n",
      "675 0.711538 0.827689\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-700\n",
      "700 0.587912 1.13236\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-725\n",
      "725 0.714286 0.78411\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-750\n",
      "750 0.706044 0.879865\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-775\n",
      "775 0.673077 0.939947\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-800\n",
      "800 0.711539 0.788677\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-825\n",
      "825 0.728022 0.889105\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-850\n",
      "850 0.706044 0.89638\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-875\n",
      "875 0.733517 0.841523\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-900\n",
      "900 0.675824 1.30304\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-925\n",
      "925 0.659341 1.69818\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-950\n",
      "950 0.673077 1.46165\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-975\n",
      "975 0.708791 1.0054\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1000\n",
      "1000 0.656593 1.15223\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1025\n",
      "1025 0.659341 1.31615\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1050\n",
      "1050 0.744506 0.960995\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1075\n",
      "1075 0.706044 1.11799\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1100\n",
      "1100 0.736264 1.06147\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1125\n",
      "1125 0.681319 1.14751\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1150\n",
      "1150 0.697802 1.08872\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1175\n",
      "1175 0.637363 1.58935\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1200\n",
      "1200 0.662088 1.54726\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1225\n",
      "1225 0.728022 1.12381\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1250\n",
      "1250 0.711538 1.30315\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1275\n",
      "1275 0.733517 1.16644\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1300\n",
      "1300 0.642857 1.88835\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1325\n",
      "1325 0.733517 1.15235\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1350\n",
      "1350 0.736264 1.06342\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1375\n",
      "1375 0.728022 1.01866\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1400\n",
      "1400 0.744506 0.979089\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1425\n",
      "1425 0.760989 1.09258\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450 0.739011 1.04018\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1475\n",
      "1475 0.730769 1.17497\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/flower-1500\n",
      "1500 0.763736 1.03598\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver() \n",
    "    ckpt = tf.train.get_checkpoint_state(\"checkpoints/\")\n",
    "    print \"=== checkpoints ===\"\n",
    "    print ckpt\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    summary_writer = tf.summary.FileWriter(logdir=summary_test_dir, graph=sess.graph, flush_secs=10)\n",
    "    \n",
    "    for v in ckpt.all_model_checkpoint_paths:\n",
    "        saver.restore(sess, v)\n",
    "        global_step = v.split('/')[-1].split('-')[-1]\n",
    "\n",
    "        cur_summary, cur_acc, cur_loss = sess.run([summary_op, accuracy, loss])\n",
    "        summary_writer.add_summary(cur_summary, global_step=global_step)\n",
    "\n",
    "        print global_step, cur_acc, cur_loss\n",
    "    \n",
    "    summary_writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why TF version works worse than Keras version?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 먼저 데이터셋을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X, y = get_batch(tfrecords_train_fn, batch_size=128, shuffle=True)\n",
    "\n",
    "# sess = tf.Session()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    cur_X, cur_y = sess.run([X, y])\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_X.shape, cur_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.max(cur_X), np.min(cur_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cur_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - tf.latest",
   "language": "python",
   "name": "python2-tf-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
