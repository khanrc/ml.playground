{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: http://python.usyiyi.cn/documents/effective-tf/10.html\n",
    "\n",
    "- 레퍼런스는 많은데... 이게 제일 깔끔해 보여서 일단 이런 방식으로 시도.\n",
    "\n",
    "Check:\n",
    "\n",
    "- 학습이 잘 되는지 => OK\n",
    "- Single GPU 랑 속도 차이가 얼마나 나는지 => OK\n",
    "- TODO: tb logging 은 잘 되는지 => 기본적으론 잘 되겠지만 좀 처리를 해 줘야 할 수도 있음\n",
    "\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GPU MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow.contrib.slim as slim\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graph():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # inputs\n",
    "    X = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(X, y):\n",
    "    x = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "    x = slim.conv2d(x, 128, kernel_size=[5,5]) \n",
    "    x = slim.max_pool2d(x, kernel_size=[2,2])\n",
    "\n",
    "    x = slim.conv2d(x, 128, kernel_size=[3,3])\n",
    "    x = slim.max_pool2d(x, kernel_size=[2,2])\n",
    "    \n",
    "    x = slim.conv2d(x, 128, kernel_size=[3,3])\n",
    "    x = slim.max_pool2d(x, kernel_size=[2,2])\n",
    "\n",
    "    flat = slim.flatten(x)\n",
    "    logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "    prob = tf.nn.softmax(logits)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits)\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, train_op, loss, accuracy):\n",
    "    config = tf.ConfigProto(log_device_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    # config.log_device_placement = True # 안 나오네...\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    N = mnist.train.num_examples\n",
    "    n_iter = N // batch_size\n",
    "    # multi-gpu 환경에서는 test batch size 를 더 키워도 됨\n",
    "    test_batch_size = 1024\n",
    "\n",
    "    for epoch in range(epoch_n):\n",
    "        st = time.time()\n",
    "\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "        for _ in range(n_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, cur_acc, cur_loss = sess.run([train_op, accuracy, loss], {X: batch_x, y: batch_y})\n",
    "            avg_acc += cur_acc\n",
    "            avg_loss += cur_loss\n",
    "\n",
    "        avg_acc /= n_iter\n",
    "        avg_loss /= n_iter\n",
    "\n",
    "        train_elapsed = time.time() - st\n",
    "        st = time.time()\n",
    "\n",
    "        test_acc = 0.\n",
    "        test_loss = 0.\n",
    "        for _ in range(mnist.test.num_examples // test_batch_size):\n",
    "            batch_x, batch_y = mnist.test.next_batch(test_batch_size)\n",
    "            cur_acc, cur_loss = sess.run([accuracy, loss], {X: batch_x, y: batch_y})\n",
    "            test_acc += cur_acc\n",
    "            test_loss += cur_loss\n",
    "        test_acc /= (mnist.test.num_examples // test_batch_size)\n",
    "        test_loss /= (mnist.test.num_examples // test_batch_size)\n",
    "\n",
    "        test_elapsed = time.time() - st\n",
    "\n",
    "        print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f} | {:.2f}s, {:.2f}s\".\n",
    "              format(epoch+1, epoch_n, avg_acc, avg_loss, test_acc, test_loss, train_elapsed, test_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "''' [!] batch_size 는 n_gpu 의 배수인 경우만 고려함. '''\n",
    "epoch_n = 10\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/losses/losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "[ 1/10] (train) acc: 83.63%, loss: 0.607 | (test) acc: 95.88%, loss: 0.132 | 3.99s, 0.20s\n",
      "[ 2/10] (train) acc: 96.87%, loss: 0.105 | (test) acc: 98.05%, loss: 0.065 | 3.10s, 0.18s\n",
      "[ 3/10] (train) acc: 98.05%, loss: 0.064 | (test) acc: 98.61%, loss: 0.047 | 3.10s, 0.18s\n",
      "[ 4/10] (train) acc: 98.45%, loss: 0.051 | (test) acc: 98.71%, loss: 0.044 | 3.11s, 0.18s\n",
      "[ 5/10] (train) acc: 98.61%, loss: 0.044 | (test) acc: 98.95%, loss: 0.037 | 3.11s, 0.18s\n",
      "[ 6/10] (train) acc: 98.91%, loss: 0.034 | (test) acc: 98.91%, loss: 0.034 | 3.11s, 0.18s\n",
      "[ 7/10] (train) acc: 99.05%, loss: 0.030 | (test) acc: 98.89%, loss: 0.035 | 3.12s, 0.18s\n",
      "[ 8/10] (train) acc: 99.25%, loss: 0.024 | (test) acc: 99.13%, loss: 0.028 | 3.19s, 0.18s\n",
      "[ 9/10] (train) acc: 99.30%, loss: 0.022 | (test) acc: 99.02%, loss: 0.031 | 3.27s, 0.18s\n",
      "[10/10] (train) acc: 99.46%, loss: 0.018 | (test) acc: 99.05%, loss: 0.032 | 3.13s, 0.18s\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_graph()\n",
    "loss, accuracy = build_graph(X, y)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "train(X, y, train_op, loss, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi GPUs MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parallel(fn, num_gpus, **kwargs):\n",
    "    '''\n",
    "    Args:\n",
    "        fn: model builder function\n",
    "        num_gpus\n",
    "        kwargs: input of model builder; e.g. X=X, y=y.\n",
    "        \n",
    "    Returns:\n",
    "        2d tensors: num_gpus * retrun_list_of_fn\n",
    "        e.g. \n",
    "        [[loss, acc, train_op],\n",
    "         [loss, acc, train_op],\n",
    "        ...\n",
    "        ]\n",
    "    '''\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "    out_split = []\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=i > 0):\n",
    "                inputs = {k : v[i] for k, v in in_splits.items()}\n",
    "                ret = fn(**inputs)\n",
    "                out_split.append(ret)\n",
    "\n",
    "    return tf.convert_to_tensor(out_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10] (train) acc: 82.05%, loss: 0.633 | (test) acc: 95.81%, loss: 0.140 | 2.52s, 0.10s\n",
      "[ 2/10] (train) acc: 96.58%, loss: 0.111 | (test) acc: 98.07%, loss: 0.066 | 0.95s, 0.07s\n",
      "[ 3/10] (train) acc: 97.91%, loss: 0.066 | (test) acc: 98.29%, loss: 0.054 | 1.01s, 0.06s\n",
      "[ 4/10] (train) acc: 98.47%, loss: 0.050 | (test) acc: 98.70%, loss: 0.041 | 0.95s, 0.06s\n",
      "[ 5/10] (train) acc: 98.73%, loss: 0.042 | (test) acc: 98.78%, loss: 0.035 | 0.91s, 0.06s\n",
      "[ 6/10] (train) acc: 98.91%, loss: 0.035 | (test) acc: 99.00%, loss: 0.030 | 0.94s, 0.06s\n",
      "[ 7/10] (train) acc: 99.12%, loss: 0.029 | (test) acc: 98.87%, loss: 0.034 | 0.95s, 0.07s\n",
      "[ 8/10] (train) acc: 99.19%, loss: 0.027 | (test) acc: 99.14%, loss: 0.026 | 0.96s, 0.07s\n",
      "[ 9/10] (train) acc: 99.28%, loss: 0.023 | (test) acc: 99.05%, loss: 0.029 | 0.94s, 0.07s\n",
      "[10/10] (train) acc: 99.33%, loss: 0.022 | (test) acc: 98.94%, loss: 0.031 | 0.94s, 0.10s\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_graph()\n",
    "parallel_tensors = make_parallel(build_graph, 4, X=X, y=y) # loss, acc\n",
    "integrated_tensors = tf.reduce_mean(parallel_tensors, axis=0)\n",
    "loss = integrated_tensors[0]\n",
    "accuracy = integrated_tensors[1]\n",
    "'''\n",
    "[!] colocate_gradients_with_ops 옵션을 켜 줘야 gradient 를 계산할 때 original ops 와 같은 디바이스에서 계산함.\n",
    "만약 이걸 키지 않으면 gradient 는 전부 default device 인 gpu:0 에서 하게 되어서 속도가 안 빨라짐 (오히려 더 느려짐;)\n",
    "'''\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss, colocate_gradients_with_ops=True)\n",
    "\n",
    "train(X, y, train_op, loss, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance test with 2 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10] (train) acc: 82.50%, loss: 0.628 | (test) acc: 95.88%, loss: 0.138 | 1.87s, 0.14s\n",
      "[ 2/10] (train) acc: 96.72%, loss: 0.108 | (test) acc: 97.43%, loss: 0.076 | 1.64s, 0.11s\n",
      "[ 3/10] (train) acc: 97.94%, loss: 0.067 | (test) acc: 98.72%, loss: 0.046 | 1.69s, 0.11s\n",
      "[ 4/10] (train) acc: 98.46%, loss: 0.049 | (test) acc: 98.56%, loss: 0.047 | 1.74s, 0.11s\n",
      "[ 5/10] (train) acc: 98.67%, loss: 0.043 | (test) acc: 98.99%, loss: 0.035 | 1.68s, 0.11s\n",
      "[ 6/10] (train) acc: 98.93%, loss: 0.035 | (test) acc: 98.67%, loss: 0.042 | 1.70s, 0.10s\n",
      "[ 7/10] (train) acc: 99.12%, loss: 0.029 | (test) acc: 98.96%, loss: 0.029 | 1.72s, 0.10s\n",
      "[ 8/10] (train) acc: 99.25%, loss: 0.024 | (test) acc: 98.93%, loss: 0.033 | 1.75s, 0.11s\n",
      "[ 9/10] (train) acc: 99.20%, loss: 0.026 | (test) acc: 99.06%, loss: 0.028 | 1.75s, 0.10s\n",
      "[10/10] (train) acc: 99.44%, loss: 0.019 | (test) acc: 99.31%, loss: 0.025 | 1.67s, 0.11s\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_graph()\n",
    "parallel_tensors = make_parallel(build_graph, 2, X=X, y=y) # loss, acc\n",
    "integrated_tensors = tf.reduce_mean(parallel_tensors, axis=0)\n",
    "loss = integrated_tensors[0]\n",
    "accuracy = integrated_tensors[1]\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss, colocate_gradients_with_ops=True)\n",
    "\n",
    "train(X, y, train_op, loss, accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
