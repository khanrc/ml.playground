{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: http://python.usyiyi.cn/documents/effective-tf/10.html\n",
    "\n",
    "- 레퍼런스는 많은데... 이게 제일 깔끔해 보여서 일단 이런 방식으로 시도.\n",
    "\n",
    "Check:\n",
    "\n",
    "- 학습이 잘 되는지\n",
    "- Single GPU 랑 속도 차이가 얼마나 나는지\n",
    "- tb logging 은 잘 되는지\n",
    "\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GPU MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow.contrib.slim as slim\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graph():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # inputs\n",
    "    X = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(X, y):\n",
    "    x = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "    x = slim.conv2d(x, 128, kernel_size=[5,5]) \n",
    "    x = slim.max_pool2d(x, kernel_size=[2,2])\n",
    "\n",
    "    x = slim.conv2d(x, 128, kernel_size=[3,3])\n",
    "    x = slim.max_pool2d(x, kernel_size=[2,2])\n",
    "    \n",
    "    x = slim.conv2d(x, 128, kernel_size=[3,3])\n",
    "    x = slim.max_pool2d(x, kernel_size=[2,2])\n",
    "\n",
    "    flat = slim.flatten(x)\n",
    "    logits = slim.fully_connected(flat, 10, activation_fn=None)\n",
    "    prob = tf.nn.softmax(logits)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits)\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "''' [!] batch_size 는 n_gpu 의 배수인 경우만 고려함. '''\n",
    "epoch_n = 10\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/losses/losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "[ 1/10] (train) acc: 83.28%, loss: 0.610 | (test) acc: 96.22%, loss: 0.124 | 4.03s, 0.20s\n",
      "[ 2/10] (train) acc: 97.09%, loss: 0.098 | (test) acc: 98.14%, loss: 0.058 | 3.15s, 0.19s\n",
      "[ 3/10] (train) acc: 98.20%, loss: 0.061 | (test) acc: 98.78%, loss: 0.041 | 3.17s, 0.18s\n",
      "[ 4/10] (train) acc: 98.51%, loss: 0.049 | (test) acc: 98.67%, loss: 0.039 | 3.17s, 0.18s\n",
      "[ 5/10] (train) acc: 98.80%, loss: 0.039 | (test) acc: 98.94%, loss: 0.034 | 3.18s, 0.18s\n",
      "[ 6/10] (train) acc: 99.06%, loss: 0.031 | (test) acc: 98.61%, loss: 0.040 | 3.16s, 0.18s\n",
      "[ 7/10] (train) acc: 99.09%, loss: 0.030 | (test) acc: 99.18%, loss: 0.027 | 3.35s, 0.18s\n",
      "[ 8/10] (train) acc: 99.33%, loss: 0.022 | (test) acc: 99.03%, loss: 0.030 | 3.16s, 0.18s\n",
      "[ 9/10] (train) acc: 99.35%, loss: 0.021 | (test) acc: 99.14%, loss: 0.026 | 3.12s, 0.18s\n",
      "[10/10] (train) acc: 99.42%, loss: 0.019 | (test) acc: 98.96%, loss: 0.035 | 3.13s, 0.18s\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_graph()\n",
    "loss, accuracy = build_graph(X, y)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "N = mnist.train.num_examples\n",
    "n_iter = N // batch_size\n",
    "test_batch_size = 1024\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    st = time.time()\n",
    "    \n",
    "    avg_loss = 0.\n",
    "    avg_acc = 0.\n",
    "    for _ in range(n_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cur_acc, cur_loss = sess.run([train_op, accuracy, loss], {X: batch_x, y: batch_y})\n",
    "        avg_acc += cur_acc\n",
    "        avg_loss += cur_loss\n",
    "\n",
    "    avg_acc /= n_iter\n",
    "    avg_loss /= n_iter\n",
    "    \n",
    "    train_elapsed = time.time() - st\n",
    "    st = time.time()\n",
    "\n",
    "    # test acc/loss does not depend on batch size\n",
    "    # so we can use the large batch size.\n",
    "    test_acc = 0.\n",
    "    test_loss = 0.\n",
    "    for _ in range(mnist.test.num_examples // test_batch_size):\n",
    "        batch_x, batch_y = mnist.test.next_batch(test_batch_size)\n",
    "        cur_acc, cur_loss = sess.run([accuracy, loss], {X: batch_x, y: batch_y})\n",
    "        test_acc += cur_acc\n",
    "        test_loss += cur_loss\n",
    "    test_acc /= (mnist.test.num_examples // test_batch_size)\n",
    "    test_loss /= (mnist.test.num_examples // test_batch_size)\n",
    "\n",
    "    test_elapsed = time.time() - st\n",
    "    \n",
    "    print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f} | {:.2f}s, {:.2f}s\".\n",
    "          format(epoch+1, epoch_n, avg_acc, avg_loss, test_acc, test_loss, train_elapsed, test_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi GPUs MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parallel(fn, num_gpus, **kwargs):\n",
    "    '''\n",
    "    Args:\n",
    "        fn: model builder function\n",
    "        num_gpus\n",
    "        kwargs: input of model builder; e.g. X=X, y=y.\n",
    "        \n",
    "    Returns:\n",
    "        2d tensors: num_gpus * retrun_list_of_fn\n",
    "        e.g. \n",
    "        [[loss, acc, train_op],\n",
    "         [loss, acc, train_op],\n",
    "        ...\n",
    "        ]\n",
    "    '''\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "    out_split = []\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=i > 0):\n",
    "                inputs = {k : v[i] for k, v in in_splits.items()}\n",
    "                ret = fn(**inputs)\n",
    "                out_split.append(ret)\n",
    "\n",
    "    return tf.convert_to_tensor(out_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10] (train) acc: 83.18%, loss: 0.620 | (test) acc: 96.07%, loss: 0.135 | 2.49s, 0.34s\n",
      "[ 2/10] (train) acc: 96.71%, loss: 0.110 | (test) acc: 97.89%, loss: 0.067 | 0.91s, 0.06s\n",
      "[ 3/10] (train) acc: 97.90%, loss: 0.067 | (test) acc: 98.21%, loss: 0.062 | 1.26s, 0.06s\n",
      "[ 4/10] (train) acc: 98.31%, loss: 0.055 | (test) acc: 98.36%, loss: 0.048 | 0.96s, 0.06s\n",
      "[ 5/10] (train) acc: 98.74%, loss: 0.041 | (test) acc: 98.87%, loss: 0.033 | 0.93s, 0.06s\n",
      "[ 6/10] (train) acc: 98.85%, loss: 0.036 | (test) acc: 98.70%, loss: 0.037 | 0.97s, 0.06s\n",
      "[ 7/10] (train) acc: 99.14%, loss: 0.030 | (test) acc: 98.94%, loss: 0.030 | 0.93s, 0.06s\n",
      "[ 8/10] (train) acc: 99.09%, loss: 0.028 | (test) acc: 98.98%, loss: 0.030 | 1.00s, 0.06s\n",
      "[ 9/10] (train) acc: 99.30%, loss: 0.022 | (test) acc: 98.85%, loss: 0.032 | 0.98s, 0.06s\n",
      "[10/10] (train) acc: 99.44%, loss: 0.019 | (test) acc: 99.13%, loss: 0.026 | 0.93s, 0.06s\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_graph()\n",
    "# loss, accuracy, train_op = build_graph(X, y)\n",
    "parallel_tensors = make_parallel(build_graph, 4, X=X, y=y) # loss, acc\n",
    "integrated_tensors = tf.reduce_mean(parallel_tensors, axis=0)\n",
    "loss = integrated_tensors[0]\n",
    "accuracy = integrated_tensors[1]\n",
    "'''\n",
    "[!] colocate_gradients_with_ops 옵션을 켜 줘야 gradient 를 계산할 때 original ops 와 같은 디바이스에서 계산함.\n",
    "만약 이걸 키지 않으면 gradient 는 전부 default device 인 gpu:0 에서 하게 되어서 속도가 안 빨라짐 (오히려 더 느려짐;)\n",
    "'''\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss, colocate_gradients_with_ops=True)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True # 안 나오네...\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "N = mnist.train.num_examples\n",
    "n_iter = N // batch_size\n",
    "test_batch_size = mnist.test.num_examples\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    st = time.time()\n",
    "    \n",
    "    avg_loss = 0.\n",
    "    avg_acc = 0.\n",
    "    for _ in range(n_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cur_acc, cur_loss = sess.run([train_op, accuracy, loss], {X: batch_x, y: batch_y})\n",
    "        avg_acc += cur_acc\n",
    "        avg_loss += cur_loss\n",
    "\n",
    "    avg_acc /= n_iter\n",
    "    avg_loss /= n_iter\n",
    "    \n",
    "    train_elapsed = time.time() - st\n",
    "    st = time.time()\n",
    "\n",
    "    # test acc/loss does not depend on batch size\n",
    "    # so we can use the large batch size.\n",
    "    test_acc = 0.\n",
    "    test_loss = 0.\n",
    "    for _ in range(mnist.test.num_examples // test_batch_size):\n",
    "        batch_x, batch_y = mnist.test.next_batch(test_batch_size)\n",
    "        cur_acc, cur_loss = sess.run([accuracy, loss], {X: batch_x, y: batch_y})\n",
    "        test_acc += cur_acc\n",
    "        test_loss += cur_loss\n",
    "    test_acc /= (mnist.test.num_examples // test_batch_size)\n",
    "    test_loss /= (mnist.test.num_examples // test_batch_size)\n",
    "\n",
    "    test_elapsed = time.time() - st\n",
    "    \n",
    "    print(\"[{:2}/{}] (train) acc: {:.2%}, loss: {:.3f} | (test) acc: {:.2%}, loss: {:.3f} | {:.2f}s, {:.2f}s\".\n",
    "          format(epoch+1, epoch_n, avg_acc, avg_loss, test_acc, test_loss, train_elapsed, test_elapsed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
