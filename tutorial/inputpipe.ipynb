{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-grained TFRecords\n",
    "\n",
    "* tfrecords 를 처음에 하나로 만들었는데 그러면 안 됨.\n",
    "\n",
    "## Experiment\n",
    "\n",
    "* 클래스별로 하나씩 만들어보고, `batch_shuffle` 과 `batch_shuffle_join` 의 차이점을 확인해보자.\n",
    "* 클래스별로 하나씩 만들어도 충분한 지 확인하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry!\n",
    "\n",
    "* http://coolingoff.tistory.com/23\n",
    "* 이걸 참고해서 해 보자.\n",
    "* 그냥 reading data 번역인 거 같긴 한데... 그것도 참고하고.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concolusion\n",
    "\n",
    "Setting: 각 클래스별 tfrecords 파일 하나\n",
    "\n",
    "* train data\n",
    "    * 트레이닝 데이터는 랜덤하게 셔플해서 받을 수 있어야 함\n",
    "    * 그러나 shuffle_batch 는 tfrecords 파일 하나에서 받아오므로 제대로 셔플이 안 됨 (한 클래스에 대해서 읽어온 후 셔플)\n",
    "    * 따라서 shuffle_batch_join 을 써야 하고 batch_join 을 써도 다양한 클래스에서 읽어오기는 함\n",
    "        * 이렇게 되면 read_thread 수를 클래스 수 이상으로 해 줘야 할 듯\n",
    "        * ImageNet 같이 데이터/클래스 전부 엄청 많으면 어떻게 해야 하지?\n",
    "* test data\n",
    "    * 테스트 데이터는 5개의 tfrecords 를 통째로 읽어와야 함\n",
    "    * 생각해보면 num_epoch 을 사용하면 컨트롤 할 수 있을 것 같은데?\n",
    "    * => 실험결과 된다!\n",
    "    \n",
    "## 결론 of 결론\n",
    "    \n",
    "**`shuffle_batch_join` + num_epochs 써라!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url, dest=None):\n",
    "    if not dest:\n",
    "        dest = 'data/' + url.split('/')[-1]\n",
    "    urllib.urlretrieve(url, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TF Flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/flower_photos\"):\n",
    "    print(\"Download flower dataset..\")\n",
    "    download_file(url)\n",
    "    print(\"Extracting dataset..\")\n",
    "    tarfile.open(\"data/flower_photos.tgz\", \"r:gz\").extractall(path=\"data/\")\n",
    "#     os.remove(\"data/flower_photos.tgz\") # 굳이..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "remake = False\n",
    "parent_dir = \"data/flower_photos\"\n",
    "train_dir = os.path.join(parent_dir, \"train\")\n",
    "test_dir = os.path.join(parent_dir, \"test\")\n",
    "\n",
    "if not os.path.exists(train_dir) or not os.path.exists(test_dir) or remake:\n",
    "    # make dirs\n",
    "    for label in LABELS:\n",
    "        # tf.gfile.MakeDirs make dir recursively & ignore exist dir\n",
    "        tf.gfile.MakeDirs(os.path.join(train_dir, label))\n",
    "        tf.gfile.MakeDirs(os.path.join(test_dir, label))\n",
    "\n",
    "    # copy files\n",
    "    for i, label in enumerate(LABELS):\n",
    "        dir_name = os.path.join(parent_dir, label)\n",
    "        paths = glob.glob(dir_name + \"/*.jpg\")\n",
    "        num_examples = len(paths)\n",
    "        for j, path in enumerate(paths):\n",
    "            fn = os.path.basename(path)\n",
    "            is_train = j < (num_examples * train_ratio)\n",
    "\n",
    "            if is_train:\n",
    "                to_path = os.path.join(train_dir, label, fn)\n",
    "            else:\n",
    "                to_path = os.path.join(test_dir, label, fn)\n",
    "            \n",
    "            tf.gfile.Copy(path, to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    364 test\r\n",
      "   3306 train\r\n"
     ]
    }
   ],
   "source": [
    "!find ./data/flower_photos/test ./data/flower_photos/train -type f | cut -d/ -f4 | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to `TFRecords` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bytes_features(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def _int64_features(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dir_to_tfrecords(dir_path, class_idx, tfrecords_path):\n",
    "    '''convert image-containing dir to tfrecords without exist check.\n",
    "    return: # of image files\n",
    "    '''\n",
    "    with tf.python_io.TFRecordWriter(tfrecords_path) as writer:\n",
    "        paths = glob.glob(dir_path + \"/*.jpg\")\n",
    "        num_examples = len(paths)\n",
    "        for path in paths:\n",
    "            im = scipy.misc.imread(path)\n",
    "            im = scipy.misc.imresize(im, [64, 64])\n",
    "            im_raw = im.tostring()\n",
    "            features = {\n",
    "                \"shape\": _int64_features(im.shape),\n",
    "                \"image\": _bytes_features([im_raw]),\n",
    "                \"label\": _int64_features([class_idx])\n",
    "            }\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            writer.write(example.SerializeToString())\n",
    "            \n",
    "        return num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert dataset to fine-grained TFRecord files ..\n",
      "3306 364\n"
     ]
    }
   ],
   "source": [
    "print(\"Convert dataset to fine-grained TFRecord files ..\")\n",
    "\n",
    "tfrecords_format = \"data/flower_photos_{}_{}.tfrecords\"\n",
    "\n",
    "num_train = 0\n",
    "num_test = 0\n",
    "\n",
    "remake_tfrecords = False\n",
    "tfrecords_path_list = [tfrecords_format.format(top, label) for top in ['train' , 'test'] for label in LABELS]\n",
    "\n",
    "if all(map(tf.gfile.Exists, tfrecords_path_list)) and remake_tfrecords == False:\n",
    "    # already exists\n",
    "    num_train = 3306\n",
    "    num_test = 364\n",
    "else:\n",
    "    # make tfrecords files\n",
    "#     num_train = dir_to_tfrecords('data/flower_photos/train/', tfrecords_train_fn)\n",
    "#     num_test = dir_to_tfrecords('data/flower_photos/test/', tfrecords_test_fn)\n",
    "    num = {'train': 0, 'test': 0}\n",
    "    for top in ['train', 'test']:\n",
    "        for i, label in enumerate(LABELS):\n",
    "            dir_path = os.path.join('data/flower_photos/', top, label)\n",
    "            tfrecords_path = tfrecords_format.format(top, label)\n",
    "            num[top] += dir_to_tfrecords(dir_path, i, tfrecords_path)\n",
    "    num_train = num['train']\n",
    "    num_test = num['test']\n",
    "\n",
    "    \n",
    "# how to get num_examples from tfrecords file?\n",
    "print num_train, num_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_root_dir = './summary/fine-grained/'\n",
    "summary_train_dir = os.path.join(summary_root_dir, 'train')\n",
    "summary_test_dir = os.path.join(summary_root_dir, 'test')\n",
    "model_name = 'tfrecords-fine-grained'\n",
    "tfrecords_path_list_train = [path for path in tfrecords_path_list if 'train' in path]\n",
    "tfrecords_path_list_test = [path for path in tfrecords_path_list if 'test' in path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check our batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename_queue):\n",
    "    with tf.variable_scope('read_data'):\n",
    "        reader = tf.TFRecordReader()\n",
    "        key, records = reader.read(filename_queue)\n",
    "        \n",
    "        # parse records\n",
    "        features = tf.parse_single_example(\n",
    "            records,\n",
    "            features={\n",
    "                \"shape\": tf.FixedLenFeature([3], tf.int64),\n",
    "                \"image\": tf.FixedLenFeature([], tf.string),\n",
    "                \"label\": tf.FixedLenFeature([], tf.int64)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        image = tf.decode_raw(features[\"image\"], tf.uint8)\n",
    "        shape = tf.cast(features[\"shape\"], tf.int32)\n",
    "        label = tf.cast(features[\"label\"], tf.int32)\n",
    "\n",
    "        # preproc\n",
    "        image = tf.reshape(image, [64, 64, 3])\n",
    "        resized_image = tf.image.resize_images(images=image, size=[64, 64])\n",
    "        resized_image = tf.cast(resized_image, tf.float32)\n",
    "        resized_image = resized_image / 255.0\n",
    "\n",
    "        one_hot_label = tf.one_hot(label, depth=5)\n",
    "        \n",
    "        return resized_image, one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/programmers_guide/reading_data\n",
    "\n",
    "def get_batch_join(tfrecords_path_list, batch_size, shuffle=False, \n",
    "                   read_thread=5, min_after_dequeue=500, num_epochs=None):\n",
    "    with tf.variable_scope(\"get_batch_join\"):\n",
    "        # make input pipeline\n",
    "        filename_queue = tf.train.string_input_producer(tfrecords_path_list, shuffle=shuffle, num_epochs=num_epochs)\n",
    "        # 5개의 reader 를 두고 각각 읽어오게 하자\n",
    "        example_list = [read_data(filename_queue) for _ in range(read_thread)]\n",
    "        \n",
    "        # train case (shuffle)\n",
    "        capacity = min_after_dequeue + 3*batch_size\n",
    "        if shuffle:\n",
    "            images, labels = tf.train.shuffle_batch_join(tensors_list=example_list, batch_size=batch_size,\n",
    "                                                         capacity=capacity, min_after_dequeue=min_after_dequeue,\n",
    "                                                         allow_smaller_final_batch=True)\n",
    "        else:\n",
    "            images, labels = tf.train.batch_join(example_list, batch_size, capacity=capacity, \n",
    "                                                 allow_smaller_final_batch=True)\n",
    "            \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(tfrecords_path_list, batch_size, shuffle=False, \n",
    "              read_thread=5, min_after_dequeue=500, num_epochs=None):\n",
    "    with tf.variable_scope(\"get_batch\"):\n",
    "        filename_queue = tf.train.string_input_producer(tfrecords_path_list, shuffle=shuffle, num_epochs=num_epochs)\n",
    "        image, label = read_data(filename_queue)\n",
    "        \n",
    "        capacity = min_after_dequeue + 3*batch_size\n",
    "        if shuffle:\n",
    "            images, labels = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=capacity, \n",
    "                                                    min_after_dequeue=min_after_dequeue, num_threads=read_thread,\n",
    "                                                    allow_smaller_final_batch=True)\n",
    "        else:\n",
    "            images, labels = tf.train.batch([image, label], batch_size, capacity=capacity, num_threads=read_thread,\n",
    "                                            allow_smaller_final_batch=True)\n",
    "        \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3306, 364)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train, num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.FailedPreconditionError'>, Attempting to use uninitialized value get_batch/input_producer/limit_epochs/epochs\n",
      "\t [[Node: get_batch/input_producer/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@get_batch/input_producer/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](get_batch/input_producer/limit_epochs/epochs)]]\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "RandomShuffleQueue '_219_get_batch/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 128, current size 0)\n\t [[Node: get_batch/shuffle_batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](get_batch/shuffle_batch/random_shuffle_queue, get_batch/shuffle_batch/n)]]\n\nCaused by op u'get_batch/shuffle_batch', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/nhnent/tf-latest-env/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-44-5a6e4ffe258f>\", line 3, in <module>\n    X, y = get_batch(tfrecords_path_list_train, batch_size=128, shuffle=True, num_epochs=1)\n  File \"<ipython-input-41-ca37f1810389>\", line 9, in get_batch\n    min_after_dequeue=min_after_dequeue)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 1217, in shuffle_batch\n    name=name)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 788, in _shuffle_batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 457, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 946, in _queue_dequeue_many_v2\n    timeout_ms=timeout_ms, name=name)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): RandomShuffleQueue '_219_get_batch/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 128, current size 0)\n\t [[Node: get_batch/shuffle_batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](get_batch/shuffle_batch/random_shuffle_queue, get_batch/shuffle_batch/n)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-5a6e4ffe258f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mthreads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_queue_runners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcur_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: RandomShuffleQueue '_219_get_batch/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 128, current size 0)\n\t [[Node: get_batch/shuffle_batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](get_batch/shuffle_batch/random_shuffle_queue, get_batch/shuffle_batch/n)]]\n\nCaused by op u'get_batch/shuffle_batch', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/nhnent/tf-latest-env/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-44-5a6e4ffe258f>\", line 3, in <module>\n    X, y = get_batch(tfrecords_path_list_train, batch_size=128, shuffle=True, num_epochs=1)\n  File \"<ipython-input-41-ca37f1810389>\", line 9, in get_batch\n    min_after_dequeue=min_after_dequeue)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 1217, in shuffle_batch\n    name=name)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 788, in _shuffle_batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 457, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 946, in _queue_dequeue_many_v2\n    timeout_ms=timeout_ms, name=name)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/nhnent/tf-latest-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): RandomShuffleQueue '_219_get_batch/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 128, current size 0)\n\t [[Node: get_batch/shuffle_batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](get_batch/shuffle_batch/random_shuffle_queue, get_batch/shuffle_batch/n)]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# default min_after_dequeue = 500\n",
    "X, y = get_batch(tfrecords_path_list_train, batch_size=128, shuffle=True, num_epochs=1)\n",
    "\n",
    "# sess = tf.Session()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    cur_X, cur_y = sess.run([X, y])\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,    0.,    0.,    0.,  128.], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cur_y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for all cases\n",
    "\n",
    "`batch`, `batch_join`, `shuffle_batch`, `shuffle_batch_join` 4개의 케이스에 대해 테스트해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3306 364\n",
      "===== train =====\n",
      "[batch]\n",
      "[ 128.    0.    0.    0.    0.]\n",
      "[ 128.    0.    0.    0.    0.]\n",
      "[ 128.    0.    0.    0.    0.]\n",
      "[ 128.    0.    0.    0.    0.]\n",
      "[ 58.  70.   0.   0.   0.]\n",
      "[   0.  128.    0.    0.    0.]\n",
      "[   0.  128.    0.    0.    0.]\n",
      "[   0.  128.    0.    0.    0.]\n",
      "[   0.  128.    0.    0.    0.]\n",
      "[   0.  128.    0.    0.    0.]\n",
      "[  0.  99.  29.   0.   0.]\n",
      "[   0.    0.  128.    0.    0.]\n",
      "[   0.    0.  128.    0.    0.]\n",
      "[   0.    0.  128.    0.    0.]\n",
      "[   0.    0.  128.    0.    0.]\n",
      "[  0.   0.  36.  92.   0.]\n",
      "[   0.    0.    0.  128.    0.]\n",
      "[   0.    0.    0.  128.    0.]\n",
      "[   0.    0.    0.  128.    0.]\n",
      "[   0.    0.    0.  128.    0.]\n",
      "[   0.    0.    0.   26.  102.]\n",
      "[   0.    0.    0.    0.  128.]\n",
      "[   0.    0.    0.    0.  128.]\n",
      "[   0.    0.    0.    0.  128.]\n",
      "[   0.    0.    0.    0.  128.]\n",
      "[   0.    0.    0.    0.  106.]\n",
      "Done -- epoch limit reached\n",
      "[ 570.  809.  577.  630.  720.] 3306\n",
      "elapsed time: 0.4\n",
      "\n",
      "[batch_join]\n",
      "[ 26.  24.  27.  24.  27.]\n",
      "[ 27.  26.  25.  26.  24.]\n",
      "[ 27.  26.  26.  25.  24.]\n",
      "[ 26.  25.  24.  26.  27.]\n",
      "[ 24.  25.  27.  26.  26.]\n",
      "[ 24.  26.  28.  24.  26.]\n",
      "[ 26.  25.  26.  25.  26.]\n",
      "[ 23.  27.  27.  24.  27.]\n",
      "[ 26.  26.  25.  27.  24.]\n",
      "[ 26.  26.  24.  26.  26.]\n",
      "[ 27.  25.  25.  26.  25.]\n",
      "[ 30.  26.  26.  23.  23.]\n",
      "[ 25.  26.  25.  25.  27.]\n",
      "[ 26.  26.  25.  25.  26.]\n",
      "[ 27.  26.  27.  25.  23.]\n",
      "[ 25.  28.  25.  24.  26.]\n",
      "[ 24.  26.  25.  26.  27.]\n",
      "[ 24.  26.  28.  25.  25.]\n",
      "[ 26.  25.  26.  25.  26.]\n",
      "[ 25.  27.  24.  23.  29.]\n",
      "[ 27.  28.  26.  23.  24.]\n",
      "[ 24.  26.  27.  25.  26.]\n",
      "[  5.  39.   9.  37.  38.]\n",
      "[  0.  45.   0.  45.  38.]\n",
      "[  0.  64.   0.   0.  64.]\n",
      "[  0.  90.   0.   0.  16.]\n",
      "Done -- epoch limit reached\n",
      "[ 570.  809.  577.  630.  720.] 3306\n",
      "elapsed time: 0.4\n",
      "\n",
      "[shuffle_batch]\n",
      "[   0.  128.    0.    0.    0.]\n",
      "[   0.  128.    0.    0.    0.]\n",
      "[   0.  123.    0.    5.    0.]\n",
      "[  0.  96.   0.  32.   0.]\n",
      "[  0.  75.   0.  53.   0.]\n",
      "[  0.  55.   0.  73.   0.]\n",
      "[  0.  43.   0.  85.   0.]\n",
      "[  0.  36.   0.  87.   5.]\n",
      "[  0.  26.   0.  63.  39.]\n",
      "[  0.  15.   0.  56.  57.]\n",
      "[  0.  23.   0.  36.  69.]\n",
      "[  0.  17.   0.  23.  88.]\n",
      "[  0.  12.   1.  21.  94.]\n",
      "[  0.   6.  21.  20.  81.]\n",
      "[  0.   8.  37.  18.  65.]\n",
      "[  0.   4.  62.  13.  49.]\n",
      "[  0.   3.  75.  12.  38.]\n",
      "[  3.   1.  88.   8.  28.]\n",
      "[ 27.   4.  70.   5.  22.]\n",
      "[ 56.   2.  44.   6.  20.]\n",
      "[ 63.   1.  46.   4.  14.]\n",
      "[ 85.   2.  29.   4.   8.]\n",
      "[ 80.   0.  25.   2.  21.]\n",
      "[ 84.   0.  31.   4.   9.]\n",
      "[ 91.   0.  29.   0.   8.]\n",
      "[ 81.   1.  19.   0.   5.]\n",
      "Done -- epoch limit reached\n",
      "[ 570.  809.  577.  630.  720.] 3306\n",
      "elapsed time: 0.4\n",
      "\n",
      "[shuffle_batch_join]\n",
      "[ 29.  26.  24.  26.  23.]\n",
      "[ 19.  22.  33.  22.  32.]\n",
      "[ 28.  18.  20.  33.  29.]\n",
      "[ 26.  30.  20.  31.  21.]\n",
      "[ 25.  24.  27.  29.  23.]\n",
      "[ 28.  17.  31.  32.  20.]\n",
      "[ 35.  22.  24.  19.  28.]\n",
      "[ 31.  21.  19.  28.  29.]\n",
      "[ 20.  31.  22.  34.  21.]\n",
      "[ 23.  22.  29.  25.  29.]\n",
      "[ 29.  23.  32.  13.  31.]\n",
      "[ 23.  18.  41.  23.  23.]\n",
      "[ 29.  29.  24.  27.  19.]\n",
      "[ 26.  24.  27.  25.  26.]\n",
      "[ 27.  33.  21.  25.  22.]\n",
      "[ 24.  26.  22.  24.  32.]\n",
      "[ 20.  22.  25.  28.  33.]\n",
      "[ 29.  20.  25.  28.  26.]\n",
      "[ 23.  36.  22.  17.  30.]\n",
      "[ 13.  27.  19.  37.  32.]\n",
      "[ 16.  41.  12.  23.  36.]\n",
      "[ 10.  52.  13.  21.  32.]\n",
      "[  8.  59.  13.  15.  33.]\n",
      "[  9.  64.  13.  16.  26.]\n",
      "[  7.  53.   9.  18.  41.]\n",
      "[ 13.  49.  10.  11.  23.]\n",
      "Done -- epoch limit reached\n",
      "[ 570.  809.  577.  630.  720.] 3306\n",
      "elapsed time: 0.4\n",
      "\n",
      "===== test =====\n",
      "[batch]\n",
      "[ 63.  65.   0.   0.   0.]\n",
      "[  0.  24.  64.  40.   0.]\n",
      "[  0.   0.   0.  29.  79.]\n",
      "Done -- epoch limit reached\n",
      "[ 63.  89.  64.  69.  79.] 364\n",
      "elapsed time: 0.0\n",
      "\n",
      "[batch_join]\n",
      "[ 36.  25.  24.  21.  22.]\n",
      "[ 27.  25.  26.  25.  25.]\n",
      "[  0.  39.  14.  23.  32.]\n",
      "Done -- epoch limit reached\n",
      "[ 63.  89.  64.  69.  79.] 364\n",
      "elapsed time: 0.1\n",
      "\n",
      "[shuffle_batch]\n",
      "[ 23.  29.  20.  28.  28.]\n",
      "[ 19.  31.  26.  27.  25.]\n",
      "[ 21.  29.  18.  14.  26.]\n",
      "Done -- epoch limit reached\n",
      "[ 63.  89.  64.  69.  79.] 364\n",
      "elapsed time: 0.1\n",
      "\n",
      "[shuffle_batch_join]\n",
      "[ 23.  33.  20.  28.  24.]\n",
      "[ 27.  30.  18.  24.  29.]\n",
      "[ 13.  26.  26.  17.  26.]\n",
      "Done -- epoch limit reached\n",
      "[ 63.  89.  64.  69.  79.] 364\n",
      "elapsed time: 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print num_train, num_test\n",
    "for data_type in ['train', 'test']:\n",
    "    kargs = {\n",
    "        'min_after_dequeue': 500,\n",
    "        'num_epochs': 1,\n",
    "        'read_thread': 5,\n",
    "        'batch_size': 128\n",
    "    }\n",
    "    if data_type == 'train':\n",
    "        kargs['tfrecords_path_list'] = tfrecords_path_list_train\n",
    "#             'batch_size': 128,\n",
    "    else:\n",
    "        kargs['tfrecords_path_list'] = tfrecords_path_list_test\n",
    "#             'batch_size': 128\n",
    "    \n",
    "    print \"===== {} =====\".format(data_type)\n",
    "    for func_type in ['batch', 'batch_join', 'shuffle_batch', 'shuffle_batch_join']:\n",
    "        tf.reset_default_graph()\n",
    "        kargs['shuffle'] = 'shuffle' in func_type\n",
    "\n",
    "    #     X, y = get_batch(tfrecords_path_list_train, batch_size=128, shuffle=True)\n",
    "        if 'join' in func_type:\n",
    "            X, y = get_batch_join(**kargs)\n",
    "        else:\n",
    "            X, y = get_batch(**kargs)\n",
    "\n",
    "        print(\"[{}]\".format(func_type))\n",
    "\n",
    "        # sess = tf.Session()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # epochs 를 쓰면 local_variables_init 도 해줘야 함.\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "            \n",
    "            st = time.time()\n",
    "\n",
    "            # num_epochs 를 지정해주면 이러한 방식으로 해야 함\n",
    "            n_iter = None\n",
    "            cnt = 0\n",
    "            data_cnt = np.zeros([5])\n",
    "            try:\n",
    "                while not coord.should_stop():\n",
    "                    cur_X, cur_y = sess.run([X, y])\n",
    "                    print np.sum(cur_y, axis=0)\n",
    "                    data_cnt += np.sum(cur_y, axis=0)\n",
    "                    # 그리고 training_step 도 걸어주자\n",
    "                    cnt += 1\n",
    "                    if cnt == n_iter:\n",
    "                        break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Done -- epoch limit reached')\n",
    "                print data_cnt, np.sum(data_cnt, dtype=np.int32)\n",
    "                print('elapsed time: {:.1f}'.format(time.time() - st))\n",
    "            finally:\n",
    "                coord.request_stop()\n",
    "\n",
    "            print\n",
    "#             coord.request_stop()\n",
    "            coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle test\n",
    "\n",
    "* test dataset 을 통째로 다 읽어올 수 있는지 테스트\n",
    "* [63, 89, 64, 69, 79] 는 test dataset size\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "여기서 테스트한 4가지 방법중에서는 test dataset 을 통째로 읽어오려면 `tf.train.batch` 를 쓰는 수밖에 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is not shuffled!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X, y = get_batch(tfrecords_path_list_test, batch_size=num_test, shuffle=False)\n",
    "\n",
    "# sess = tf.Session()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for _ in range(5):\n",
    "        cur_X, cur_y = sess.run([X, y])\n",
    "        if not (np.sum(cur_y, axis=0) == [63., 89., 64., 69., 79.]).all():\n",
    "            print \"this is shuffled\"\n",
    "            break\n",
    "    else:\n",
    "        print \"this is not shuffled!\"\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - tf.latest",
   "language": "python",
   "name": "python2-tf-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
