{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-grained TFRecords\n",
    "\n",
    "* tfrecords 를 처음에 하나로 만들었는데 그러면 안 됨.\n",
    "\n",
    "## Experiment\n",
    "\n",
    "* 클래스별로 하나씩 만들어보고, `batch_shuffle` 과 `batch_shuffle_join` 의 차이점을 확인해보자.\n",
    "* 클래스별로 하나씩 만들어도 충분한 지 확인하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry!\n",
    "\n",
    "* http://coolingoff.tistory.com/23\n",
    "* 이걸 참고해서 해 보자.\n",
    "* 그냥 reading data 번역인 거 같긴 한데... 그것도 참고하고.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concolusion\n",
    "\n",
    "Setting: 각 클래스별 tfrecords 파일 하나\n",
    "\n",
    "* train data\n",
    "    * 트레이닝 데이터는 랜덤하게 셔플해서 받을 수 있어야 함\n",
    "    * 그러나 shuffle_batch 는 tfrecords 파일 하나에서 받아오므로 제대로 셔플이 안 됨 (한 클래스에 대해서 읽어온 후 셔플)\n",
    "    * 따라서 shuffle_batch_join 을 써야 하고 batch_join 을 써도 다양한 클래스에서 읽어오기는 함\n",
    "        * 이렇게 되면 read_thread 수를 클래스 수 이상으로 해 줘야 할 듯\n",
    "        * ImageNet 같이 데이터/클래스 전부 엄청 많으면 어떻게 해야 하지?\n",
    "* test data\n",
    "    * 테스트 데이터는 5개의 tfrecords 를 통째로 읽어와야 함\n",
    "    * 생각해보면 num_epoch 을 사용하면 컨트롤 할 수 있을 것 같은데?\n",
    "    * => 실험결과 된다!\n",
    "    \n",
    "## 결론 of 결론\n",
    "    \n",
    "**`shuffle_batch_join` + num_epochs 써라!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url, dest=None):\n",
    "    if not dest:\n",
    "        dest = 'data/' + url.split('/')[-1]\n",
    "    urllib.urlretrieve(url, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TF Flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/flower_photos\"):\n",
    "    print(\"Download flower dataset..\")\n",
    "    download_file(url)\n",
    "    print(\"Extracting dataset..\")\n",
    "    tarfile.open(\"data/flower_photos.tgz\", \"r:gz\").extractall(path=\"data/\")\n",
    "#     os.remove(\"data/flower_photos.tgz\") # 굳이..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "remake = False\n",
    "parent_dir = \"data/flower_photos\"\n",
    "train_dir = os.path.join(parent_dir, \"train\")\n",
    "test_dir = os.path.join(parent_dir, \"test\")\n",
    "\n",
    "if not os.path.exists(train_dir) or not os.path.exists(test_dir) or remake:\n",
    "    # make dirs\n",
    "    for label in LABELS:\n",
    "        # tf.gfile.MakeDirs make dir recursively & ignore exist dir\n",
    "        tf.gfile.MakeDirs(os.path.join(train_dir, label))\n",
    "        tf.gfile.MakeDirs(os.path.join(test_dir, label))\n",
    "\n",
    "    # copy files\n",
    "    for i, label in enumerate(LABELS):\n",
    "        dir_name = os.path.join(parent_dir, label)\n",
    "        paths = glob.glob(dir_name + \"/*.jpg\")\n",
    "        num_examples = len(paths)\n",
    "        for j, path in enumerate(paths):\n",
    "            fn = os.path.basename(path)\n",
    "            is_train = j < (num_examples * train_ratio)\n",
    "\n",
    "            if is_train:\n",
    "                to_path = os.path.join(train_dir, label, fn)\n",
    "            else:\n",
    "                to_path = os.path.join(test_dir, label, fn)\n",
    "            \n",
    "            tf.gfile.Copy(path, to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    364 test\r\n",
      "   3306 train\r\n"
     ]
    }
   ],
   "source": [
    "!find ./data/flower_photos/test ./data/flower_photos/train -type f | cut -d/ -f4 | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to `TFRecords` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bytes_features(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def _int64_features(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dir_to_tfrecords(dir_path, class_idx, tfrecords_path):\n",
    "    '''convert image-containing dir to tfrecords without exist check.\n",
    "    return: # of image files\n",
    "    '''\n",
    "    with tf.python_io.TFRecordWriter(tfrecords_path) as writer:\n",
    "        paths = glob.glob(dir_path + \"/*.jpg\")\n",
    "        num_examples = len(paths)\n",
    "        for path in paths:\n",
    "            im = scipy.misc.imread(path)\n",
    "            im = scipy.misc.imresize(im, [64, 64])\n",
    "            im_raw = im.tostring()\n",
    "            features = {\n",
    "                \"shape\": _int64_features(im.shape),\n",
    "                \"image\": _bytes_features([im_raw]),\n",
    "                \"label\": _int64_features([class_idx])\n",
    "            }\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            writer.write(example.SerializeToString())\n",
    "            \n",
    "        return num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert dataset to fine-grained TFRecord files ..\n",
      "# of train/daisy: 570\n",
      "# of train/dandelion: 809\n",
      "# of train/roses: 577\n",
      "# of train/sunflowers: 630\n",
      "# of train/tulips: 720\n",
      "# of test/daisy: 63\n",
      "# of test/dandelion: 89\n",
      "# of test/roses: 64\n",
      "# of test/sunflowers: 69\n",
      "# of test/tulips: 79\n",
      "3306 364\n"
     ]
    }
   ],
   "source": [
    "print(\"Convert dataset to fine-grained TFRecord files ..\")\n",
    "\n",
    "tfrecords_format = \"data/flower_photos_{}_{}.tfrecords\"\n",
    "\n",
    "num_train = 0\n",
    "num_test = 0\n",
    "\n",
    "remake_tfrecords = False\n",
    "tfrecords_path_list = [tfrecords_format.format(top, label) for top in ['train' , 'test'] for label in LABELS]\n",
    "\n",
    "if all(map(tf.gfile.Exists, tfrecords_path_list)) and remake_tfrecords == False:\n",
    "    # already exists\n",
    "    num_train = 3306\n",
    "    num_test = 364\n",
    "else:\n",
    "    # make tfrecords files\n",
    "#     num_train = dir_to_tfrecords('data/flower_photos/train/', tfrecords_train_fn)\n",
    "#     num_test = dir_to_tfrecords('data/flower_photos/test/', tfrecords_test_fn)\n",
    "    num = {'train': 0, 'test': 0}\n",
    "    for top in ['train', 'test']:\n",
    "        for i, label in enumerate(LABELS):\n",
    "            dir_path = os.path.join('data/flower_photos/', top, label)\n",
    "            tfrecords_path = tfrecords_format.format(top, label)\n",
    "            num_cur = dir_to_tfrecords(dir_path, i, tfrecords_path)\n",
    "            num[top] += num_cur\n",
    "            print '# of {}/{}: {}'.format(top, label, num_cur)\n",
    "    num_train = num['train']\n",
    "    num_test = num['test']\n",
    "\n",
    "    \n",
    "# how to get num_examples from tfrecords file?\n",
    "print num_train, num_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_root_dir = './summary/fine-grained/'\n",
    "summary_train_dir = os.path.join(summary_root_dir, 'train')\n",
    "summary_test_dir = os.path.join(summary_root_dir, 'test')\n",
    "model_name = 'tfrecords-fine-grained'\n",
    "tfrecords_path_list_train = [path for path in tfrecords_path_list if 'train' in path]\n",
    "tfrecords_path_list_test = [path for path in tfrecords_path_list if 'test' in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/flower_photos_train_daisy.tfrecords',\n",
       " 'data/flower_photos_train_dandelion.tfrecords',\n",
       " 'data/flower_photos_train_roses.tfrecords',\n",
       " 'data/flower_photos_train_sunflowers.tfrecords',\n",
       " 'data/flower_photos_train_tulips.tfrecords']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrecords_path_list_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/flower_photos_test_daisy.tfrecords',\n",
       " 'data/flower_photos_test_dandelion.tfrecords',\n",
       " 'data/flower_photos_test_roses.tfrecords',\n",
       " 'data/flower_photos_test_sunflowers.tfrecords',\n",
       " 'data/flower_photos_test_tulips.tfrecords']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrecords_path_list_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check our batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename_queue):\n",
    "    with tf.variable_scope('read_data'):\n",
    "        reader = tf.TFRecordReader()\n",
    "        key, records = reader.read(filename_queue)\n",
    "        \n",
    "        # parse records\n",
    "        features = tf.parse_single_example(\n",
    "            records,\n",
    "            features={\n",
    "                \"shape\": tf.FixedLenFeature([3], tf.int64),\n",
    "                \"image\": tf.FixedLenFeature([], tf.string),\n",
    "                \"label\": tf.FixedLenFeature([], tf.int64)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        image = tf.decode_raw(features[\"image\"], tf.uint8)\n",
    "        shape = tf.cast(features[\"shape\"], tf.int32)\n",
    "        label = tf.cast(features[\"label\"], tf.int32)\n",
    "\n",
    "        # preproc\n",
    "        image = tf.reshape(image, [64, 64, 3])\n",
    "#         image = tf.image.resize_images(images=image, size=[64, 64])\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = image / 255.0\n",
    "\n",
    "        one_hot_label = tf.one_hot(label, depth=5)\n",
    "        \n",
    "        return image, one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/programmers_guide/reading_data\n",
    "\n",
    "def get_batch_join(tfrecords_path_list, batch_size, shuffle=False, \n",
    "                   read_thread=5, min_after_dequeue=500, num_epochs=None):\n",
    "    with tf.variable_scope(\"get_batch_join\"):\n",
    "        # make input pipeline\n",
    "        filename_queue = tf.train.string_input_producer(tfrecords_path_list, shuffle=shuffle, num_epochs=num_epochs)\n",
    "        # 5개의 reader 를 두고 각각 읽어오게 하자\n",
    "        example_list = [read_data(filename_queue) for _ in range(read_thread)]\n",
    "        \n",
    "        # train case (shuffle)\n",
    "        capacity = min_after_dequeue + 3*batch_size\n",
    "        if shuffle:\n",
    "            images, labels = tf.train.shuffle_batch_join(tensors_list=example_list, batch_size=batch_size,\n",
    "                                                         capacity=capacity, min_after_dequeue=min_after_dequeue,\n",
    "                                                         allow_smaller_final_batch=True)\n",
    "        else:\n",
    "            images, labels = tf.train.batch_join(example_list, batch_size, capacity=capacity, \n",
    "                                                 allow_smaller_final_batch=True)\n",
    "            \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(tfrecords_path_list, batch_size, shuffle=False, \n",
    "              read_thread=5, min_after_dequeue=500, num_epochs=None):\n",
    "    with tf.variable_scope(\"get_batch\"):\n",
    "        filename_queue = tf.train.string_input_producer(tfrecords_path_list, shuffle=shuffle, num_epochs=num_epochs)\n",
    "        image, label = read_data(filename_queue)\n",
    "        \n",
    "        capacity = min_after_dequeue + 3*batch_size\n",
    "        if shuffle:\n",
    "            images, labels = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=capacity, \n",
    "                                                    min_after_dequeue=min_after_dequeue, num_threads=read_thread,\n",
    "                                                    allow_smaller_final_batch=True)\n",
    "        else:\n",
    "            images, labels = tf.train.batch([image, label], batch_size, capacity=capacity, num_threads=read_thread,\n",
    "                                            allow_smaller_final_batch=True)\n",
    "        \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3306, 364)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train, num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# default min_after_dequeue = 500\n",
    "X, y = get_batch(tfrecords_path_list_train, batch_size=128, shuffle=True, num_epochs=None)\n",
    "\n",
    "# sess = tf.Session()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    cur_X, cur_y = sess.run([X, y])\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,  128.,    0.,    0.,    0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cur_y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for all cases\n",
    "\n",
    "`batch`, `batch_join`, `shuffle_batch`, `shuffle_batch_join` 4개의 케이스에 대해 테스트해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3306 364\n",
      "===== train =====\n",
      "[batch]\n",
      "[ 128.    0.    0.    0.    0.] 1\n",
      "[ 128.    0.    0.    0.    0.] 1\n",
      "[ 128.    0.    0.    0.    0.] 1\n",
      "[ 128.    0.    0.    0.    0.] 1\n",
      "[ 58.  70.   0.   0.   0.] 1\n",
      "[   0.  128.    0.    0.    0.] 1\n",
      "[   0.  128.    0.    0.    0.] 1\n",
      "[   0.  128.    0.    0.    0.] 1\n",
      "[   0.  128.    0.    0.    0.] 1\n",
      "[   0.  128.    0.    0.    0.] 1\n",
      "[  0.  99.  29.   0.   0.] 1\n",
      "[   0.    0.  128.    0.    0.] 1\n",
      "[   0.    0.  128.    0.    0.] 1\n",
      "[   0.    0.  128.    0.    0.] 1\n",
      "[   0.    0.  128.    0.    0.] 1\n",
      "[  0.   0.  36.  92.   0.] 1\n",
      "[   0.    0.    0.  128.    0.] 1\n",
      "[   0.    0.    0.  128.    0.] 1\n",
      "[   0.    0.    0.  128.    0.] 1\n",
      "[   0.    0.    0.  128.    0.] 1\n",
      "[   0.    0.    0.   26.  102.] 1\n",
      "[   0.    0.    0.    0.  128.] 1\n",
      "[   0.    0.    0.    0.  128.] 1\n",
      "[   0.    0.    0.    0.  128.] 1\n",
      "[   0.    0.    0.    0.  128.] 1\n",
      "[  22.    0.    0.    0.  106.] 2\n",
      "[ 128.    0.    0.    0.    0.] 2\n",
      "[ 128.    0.    0.    0.    0.] 2\n",
      "[ 128.    0.    0.    0.    0.] 2\n",
      "[ 128.    0.    0.    0.    0.] 2\n",
      "[ 36.  92.   0.   0.   0.] 2\n",
      "[   0.  128.    0.    0.    0.] 2\n",
      "[   0.  128.    0.    0.    0.] 2\n",
      "[   0.  128.    0.    0.    0.] 2\n",
      "[   0.  128.    0.    0.    0.] 2\n",
      "[   0.  128.    0.    0.    0.] 2\n",
      "[  0.  77.  51.   0.   0.] 2\n",
      "[   0.    0.  128.    0.    0.] 2\n",
      "[   0.    0.  128.    0.    0.] 2\n",
      "[   0.    0.  128.    0.    0.] 2\n",
      "[   0.    0.  128.    0.    0.] 2\n",
      "[   0.    0.   14.  114.    0.] 2\n",
      "[   0.    0.    0.  128.    0.] 2\n",
      "[   0.    0.    0.  128.    0.] 2\n",
      "[   0.    0.    0.  128.    0.] 2\n",
      "[   0.    0.    0.  128.    0.] 2\n",
      "[   0.    0.    0.    4.  124.] 2\n",
      "[   0.    0.    0.    0.  128.] 2\n",
      "[   0.    0.    0.    0.  128.] 2\n",
      "[   0.    0.    0.    0.  128.] 2\n",
      "[   0.    0.    0.    0.  128.] 2\n",
      "[  0.   0.   0.   0.  84.] 3\n",
      "Done -- epoch limit reached\n",
      "[ 1140.  1618.  1154.  1260.  1440.] 6612\n",
      "elapsed time: 0.8\n",
      "\n",
      "[batch_join]\n",
      "[ 27.  27.  26.  26.  22.] 1\n",
      "[ 24.  26.  26.  25.  27.] 1\n",
      "[ 26.  26.  24.  27.  25.] 1\n",
      "[ 27.  26.  25.  25.  25.] 1\n",
      "[ 27.  25.  23.  26.  27.] 1\n",
      "[ 30.  23.  26.  24.  25.] 1\n",
      "[ 25.  25.  27.  25.  26.] 1\n",
      "[ 24.  26.  28.  23.  27.] 1\n",
      "[ 26.  26.  25.  26.  25.] 1\n",
      "[ 27.  26.  25.  25.  25.] 1\n",
      "[ 25.  25.  27.  24.  27.] 1\n",
      "[ 25.  27.  25.  25.  26.] 1\n",
      "[ 26.  26.  26.  26.  24.] 1\n",
      "[ 26.  26.  26.  25.  25.] 1\n",
      "[ 25.  26.  26.  26.  25.] 1\n",
      "[ 25.  25.  27.  25.  26.] 1\n",
      "[ 24.  27.  25.  26.  26.] 1\n",
      "[ 23.  26.  27.  26.  26.] 1\n",
      "[ 25.  25.  26.  27.  25.] 1\n",
      "[ 25.  27.  26.  26.  24.] 1\n",
      "[ 26.  25.  24.  25.  28.] 1\n",
      "[ 24.  27.  24.  27.  26.] 1\n",
      "[ 26.  37.  13.  26.  26.] 1\n",
      "[ 26.  49.   0.  27.  26.] 1\n",
      "[ 25.  51.   9.  17.  26.] 1\n",
      "[ 25.  51.  25.   0.  27.] 2\n",
      "[ 26.  52.  25.   0.  25.] 2\n",
      "[ 27.  53.  24.   0.  24.] 2\n",
      "[ 25.  52.  25.  22.   4.] 2\n",
      "[ 23.  53.  25.  27.   0.] 2\n",
      "[ 26.  51.  26.  25.   0.] 2\n",
      "[ 24.  32.  26.  26.  20.] 2\n",
      "[ 24.  26.  27.  26.  25.] 2\n",
      "[ 25.  25.  27.  25.  26.] 2\n",
      "[ 27.  27.  24.  26.  24.] 2\n",
      "[ 26.  26.  26.  25.  25.] 2\n",
      "[ 29.  24.  25.  25.  25.] 2\n",
      "[ 27.  26.  26.  25.  24.] 2\n",
      "[ 26.  26.  25.  27.  24.] 2\n",
      "[ 27.  24.  24.  29.  24.] 2\n",
      "[ 26.  25.  26.  27.  24.] 2\n",
      "[ 25.  25.  27.  25.  26.] 2\n",
      "[ 26.  26.  27.  25.  24.] 2\n",
      "[ 25.  27.  26.  23.  27.] 2\n",
      "[ 12.  28.  29.  28.  31.] 2\n",
      "[  0.  31.  32.  33.  32.] 2\n",
      "[  0.  35.  21.  37.  35.] 2\n",
      "[  0.  43.   0.  41.  44.] 2\n",
      "[  0.  44.   0.  41.  43.] 2\n",
      "[  0.  43.   0.  42.  43.] 2\n",
      "[  0.  38.   0.   0.  90.] 2\n",
      "[  0.   0.   0.   0.  84.] 3\n",
      "Done -- epoch limit reached\n",
      "[ 1140.  1618.  1154.  1260.  1440.] 6612\n",
      "elapsed time: 0.8\n",
      "\n",
      "[shuffle_batch]\n",
      "[   1.    0.  127.    0.    0.] 1\n",
      "[  28.    0.  100.    0.    0.] 1\n",
      "[ 57.   0.  71.   0.   0.] 1\n",
      "[ 62.   0.  66.   0.   0.] 1\n",
      "[ 80.   0.  48.   0.   0.] 1\n",
      "[ 80.   0.  38.   0.  10.] 1\n",
      "[ 55.   0.  27.   0.  46.] 1\n",
      "[ 55.   0.  23.   0.  50.] 1\n",
      "[ 38.   0.  19.   0.  71.] 1\n",
      "[ 24.   0.  12.   0.  92.] 1\n",
      "[ 24.   0.  10.   2.  92.] 1\n",
      "[ 13.   0.  10.  23.  82.] 1\n",
      "[ 15.   0.   3.  45.  65.] 1\n",
      "[  8.   0.   3.  60.  57.] 1\n",
      "[  6.   0.   2.  90.  30.] 1\n",
      "[  7.   2.   3.  87.  29.] 1\n",
      "[  3.  29.   2.  63.  31.] 1\n",
      "[  2.  47.   3.  63.  13.] 1\n",
      "[  0.  78.   1.  40.   9.] 1\n",
      "[  2.  79.   2.  32.  13.] 1\n",
      "[  1.  94.   1.  23.   9.] 1\n",
      "[  3.  93.   1.  29.   2.] 1\n",
      "[  2.  86.   0.  15.  25.] 1\n",
      "[  2.  70.   0.  13.  43.] 1\n",
      "[  0.  49.   0.  14.  65.] 1\n",
      "[  0.  49.   1.  11.  67.] 2\n",
      "[  0.  25.   2.   7.  94.] 2\n",
      "[  1.  23.   2.   4.  98.] 2\n",
      "[  0.  21.  26.   1.  80.] 2\n",
      "[  0.  13.  56.   4.  55.] 2\n",
      "[  1.  12.  64.   2.  49.] 2\n",
      "[  0.  12.  79.   0.  37.] 2\n",
      "[ 21.   7.  69.   0.  31.] 2\n",
      "[ 31.   8.  68.   0.  21.] 2\n",
      "[ 67.   0.  46.   0.  15.] 2\n",
      "[ 83.   2.  34.   0.   9.] 2\n",
      "[ 81.   9.  29.   1.   8.] 2\n",
      "[ 61.  33.  24.   1.   9.] 2\n",
      "[ 49.  54.  21.   0.   4.] 2\n",
      "[ 38.  71.  13.   0.   6.] 2\n",
      "[ 26.  90.   7.   0.   5.] 2\n",
      "[ 25.  89.  12.   0.   2.] 2\n",
      "[ 22.  93.   8.   0.   5.] 2\n",
      "[ 20.  86.   3.  18.   1.] 2\n",
      "[  5.  79.   1.  40.   3.] 2\n",
      "[ 16.  48.   3.  61.   0.] 2\n",
      "[  2.  39.   1.  86.   0.] 2\n",
      "[  7.  26.   2.  91.   2.] 2\n",
      "[  6.  36.   1.  85.   0.] 2\n",
      "[  2.  23.   4.  98.   1.] 2\n",
      "[  4.  28.   2.  92.   2.] 2\n",
      "[  4.  15.   4.  59.   2.] 3\n",
      "Done -- epoch limit reached\n",
      "[ 1140.  1618.  1154.  1260.  1440.] 6612\n",
      "elapsed time: 0.7\n",
      "\n",
      "[shuffle_batch_join]\n",
      "[ 23.  17.  27.  29.  32.] 1\n",
      "[ 22.  22.  34.  23.  27.] 1\n",
      "[ 24.  40.  27.  18.  19.] 1\n",
      "[ 28.  27.  22.  31.  20.] 1\n",
      "[ 30.  19.  29.  24.  26.] 1\n",
      "[ 24.  35.  32.  16.  21.] 1\n",
      "[ 19.  27.  29.  32.  21.] 1\n",
      "[ 22.  27.  22.  25.  32.] 1\n",
      "[ 22.  24.  29.  29.  24.] 1\n",
      "[ 29.  22.  26.  24.  27.] 1\n",
      "[ 23.  27.  22.  26.  30.] 1\n",
      "[ 25.  31.  21.  27.  24.] 1\n",
      "[ 23.  29.  13.  32.  31.] 1\n",
      "[ 31.  27.  27.  24.  19.] 1\n",
      "[ 28.  24.  29.  22.  25.] 1\n",
      "[ 22.  24.  23.  29.  30.] 1\n",
      "[ 28.  26.  19.  27.  28.] 1\n",
      "[ 25.  27.  26.  25.  25.] 1\n",
      "[ 32.  20.  29.  21.  26.] 1\n",
      "[ 32.  23.  13.  29.  31.] 1\n",
      "[ 27.  23.  17.  23.  38.] 1\n",
      "[ 31.  28.  13.  18.  38.] 1\n",
      "[ 24.  34.  11.  19.  40.] 1\n",
      "[ 24.  33.   7.  14.  50.] 1\n",
      "[ 21.  34.   7.  10.  56.] 1\n",
      "[ 25.  51.  10.   6.  36.] 2\n",
      "[ 25.  50.  14.   8.  31.] 2\n",
      "[ 24.  46.  21.   7.  30.] 2\n",
      "[ 23.  38.  16.  11.  40.] 2\n",
      "[ 29.  36.  22.  17.  24.] 2\n",
      "[ 24.  41.  21.  17.  25.] 2\n",
      "[ 29.  30.  17.  20.  32.] 2\n",
      "[ 38.  30.  26.  13.  21.] 2\n",
      "[ 25.  32.  27.  18.  26.] 2\n",
      "[ 16.  36.  31.  17.  28.] 2\n",
      "[ 29.  22.  27.  24.  26.] 2\n",
      "[ 21.  34.  20.  20.  33.] 2\n",
      "[ 30.  23.  27.  31.  17.] 2\n",
      "[ 20.  24.  28.  26.  30.] 2\n",
      "[ 24.  28.  27.  20.  29.] 2\n",
      "[ 32.  26.  19.  26.  25.] 2\n",
      "[ 19.  21.  31.  25.  32.] 2\n",
      "[ 21.  23.  25.  27.  32.] 2\n",
      "[ 14.  30.  33.  28.  23.] 2\n",
      "[  8.  28.  25.  29.  38.] 2\n",
      "[  6.  41.  24.  35.  22.] 2\n",
      "[  5.  41.  22.  36.  24.] 2\n",
      "[  5.  47.  20.  46.  10.] 2\n",
      "[  4.  41.  18.  49.  16.] 2\n",
      "[  3.  45.  16.  46.  18.] 2\n",
      "[  1.  49.  16.  43.  19.] 2\n",
      "[  1.  35.  17.  18.  13.] 3\n",
      "Done -- epoch limit reached\n",
      "[ 1140.  1618.  1154.  1260.  1440.] 6612\n",
      "elapsed time: 0.8\n",
      "\n",
      "===== test =====\n",
      "[batch]\n",
      "[ 63.  65.   0.   0.   0.] 1\n",
      "[  0.  24.  64.  40.   0.] 1\n",
      "[ 20.   0.   0.  29.  79.] 2\n",
      "[ 43.  85.   0.   0.   0.] 2\n",
      "[  0.   4.  64.  60.   0.] 2\n",
      "[  0.   0.   0.   9.  79.] 3\n",
      "Done -- epoch limit reached\n",
      "[ 126.  178.  128.  138.  158.] 728\n",
      "elapsed time: 0.1\n",
      "\n",
      "[batch_join]\n",
      "[ 26.  27.  27.  25.  23.] 1\n",
      "[ 27.  27.  24.  24.  26.] 1\n",
      "[ 27.  38.  18.  20.  25.] 2\n",
      "[ 26.  34.  25.  21.  22.] 2\n",
      "[ 20.  27.  28.  25.  28.] 2\n",
      "[  0.  25.   6.  23.  34.] 3\n",
      "Done -- epoch limit reached\n",
      "[ 126.  178.  128.  138.  158.] 728\n",
      "elapsed time: 0.1\n",
      "\n",
      "[shuffle_batch]\n",
      "[ 12.  43.  15.  24.  34.] 1\n",
      "[ 23.  30.  19.  23.  33.] 1\n",
      "[ 19.  30.  30.  26.  23.] 2\n",
      "[ 20.  31.  24.  33.  20.] 2\n",
      "[ 33.  25.  24.  18.  28.] 2\n",
      "[ 19.  19.  16.  14.  20.] 3\n",
      "Done -- epoch limit reached\n",
      "[ 126.  178.  128.  138.  158.] 728\n",
      "elapsed time: 0.1\n",
      "\n",
      "[shuffle_batch_join]\n",
      "[ 20.  24.  26.  22.  36.] 1\n",
      "[ 34.  20.  19.  28.  27.] 1\n",
      "[ 16.  38.  26.  22.  26.] 2\n",
      "[ 19.  41.  23.  19.  26.] 2\n",
      "[ 21.  28.  24.  28.  27.] 2\n",
      "[ 16.  27.  10.  19.  16.] 3\n",
      "Done -- epoch limit reached\n",
      "[ 126.  178.  128.  138.  158.] 728\n",
      "elapsed time: 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print num_train, num_test\n",
    "for data_type in ['train', 'test']:\n",
    "    n_examples = num_train if data_type == 'train' else num_test\n",
    "    kargs = {\n",
    "        'min_after_dequeue': 500,\n",
    "        'num_epochs': 2,\n",
    "        'read_thread': 5,\n",
    "        'batch_size': 128\n",
    "    }\n",
    "    if data_type == 'train':\n",
    "        kargs['tfrecords_path_list'] = tfrecords_path_list_train\n",
    "    else:\n",
    "        kargs['tfrecords_path_list'] = tfrecords_path_list_test\n",
    "    \n",
    "    print \"===== {} =====\".format(data_type)\n",
    "    for func_type in ['batch', 'batch_join', 'shuffle_batch', 'shuffle_batch_join']:\n",
    "        tf.reset_default_graph()\n",
    "        kargs['shuffle'] = 'shuffle' in func_type\n",
    "\n",
    "        if 'join' in func_type:\n",
    "            X, y = get_batch_join(**kargs)\n",
    "        else:\n",
    "            X, y = get_batch(**kargs)\n",
    "\n",
    "        print(\"[{}]\".format(func_type))\n",
    "\n",
    "        # sess = tf.Session()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # epochs 를 쓰면 local_variables_init 도 해줘야 함.\n",
    "            # 위에서 지정한 num_epochs 가 local_variable 로 그래프에 박히는 듯\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "            \n",
    "            st = time.time()\n",
    "\n",
    "            # num_epochs 를 지정해주면 이러한 방식으로 해야 함\n",
    "            n_iter = None\n",
    "            step_cnt = 0\n",
    "            data_cnt = np.zeros([5])\n",
    "            try:\n",
    "                while not coord.should_stop():\n",
    "                    cur_X, cur_y = sess.run([X, y])\n",
    "                    print np.sum(cur_y, axis=0),\n",
    "                    data_cnt += np.sum(cur_y, axis=0)\n",
    "                    step_cnt += 1\n",
    "                    epoch = np.ceil(float(step_cnt * kargs['batch_size']) / n_examples)\n",
    "                    print int(epoch)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Done -- epoch limit reached')\n",
    "                print data_cnt, np.sum(data_cnt, dtype=np.int32)\n",
    "                print('elapsed time: {:.1f}'.format(time.time() - st))\n",
    "            finally:\n",
    "                coord.request_stop()\n",
    "\n",
    "            print\n",
    "#             coord.request_stop()\n",
    "            coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle test\n",
    "\n",
    "* test dataset 을 통째로 다 읽어올 수 있는지 테스트\n",
    "* [63, 89, 64, 69, 79] 는 test dataset size\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "여기서 테스트한 4가지 방법중에서는 test dataset 을 통째로 읽어오려면 `tf.train.batch` 를 쓰는 수밖에 없음 - 그냥 `num_epoch` 을 쓰면 된다!\n",
    "\n",
    "### Issue\n",
    "\n",
    "* read_thread 가 5 이상이 되면 읽어오는 개수가 살짝 이상해짐\n",
    "    * [ 63.  89.  64.  69.  79.] => [ 64.  89.  64.  69.  78.]\n",
    "* 이유는 잘 모르겠음..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 64.  89.  64.  69.  78.]\n",
      "this is shuffled\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X, y = get_batch(tfrecords_path_list_test, batch_size=num_test, shuffle=False, read_thread=5)\n",
    "\n",
    "# sess = tf.Session()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for _ in range(5):\n",
    "        cur_X, cur_y = sess.run([X, y])\n",
    "        if not (np.sum(cur_y, axis=0) == [63., 89., 64., 69., 79.]).all():\n",
    "            print np.sum(cur_y, axis=0)\n",
    "            print \"this is shuffled\"\n",
    "            break\n",
    "    else:\n",
    "        print \"this is not shuffled!\"\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - tf.latest",
   "language": "python",
   "name": "python2-tf-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
